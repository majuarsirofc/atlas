Modul Loop Batching

1. all_data_backend.py:

import os
import sys
import uuid
import json
import datetime
import pandas as pd
import shutil

from fastapi import FastAPI, Request, Query, HTTPException, BackgroundTasks, status
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware

from loguru import logger
from rich.traceback import install

# === PATCH: Advanced Logging with JSON, Trace ID, Error Alert Sink, Custom Log Dir ===

LOG_DIR = os.path.join(r"C:\Users\ASUS\atlas_project\backend-python", "data_logging")
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, "fastapi_app.log")

logger.remove()
logger.add(sys.stdout, serialize=True)  # log ke terminal/cmd (JSON)
logger.add(LOG_FILE, rotation="10 MB", serialize=True)  # log ke file (JSON, rotasi)

def error_alert_sink(message):
    rec = message.record
    if rec["level"].name in ("ERROR", "CRITICAL"):
        # Di sini bisa diganti dengan send_alert_webhook/slack/email
        print(f"ALERT! [{rec['level'].name}] TraceID={rec['extra'].get('trace_id')}: {rec['message']}")

logger.add(error_alert_sink, level="ERROR")

install()

# --- BASE_DIR DAN DATA_DIR divisional_level ---
BASE_DIR = r"C:\Users\ASUS\atlas_project\backend-python\divisional_level"
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

from divisional_level.smart_file_loader import (
    load_all_parquet_tables,
    get_first_parquet_file_path,
)

from divisional_level.config import (
    START_BATCH_SIZE, MAX_BATCH_SIZE, MIN_BATCH_SIZE,
    TIME_FAST, TIME_SLOW, BATCH_SIZE, get_batch_config
)

from dotenv import load_dotenv
load_dotenv()

PER_FILE_MAX = START_BATCH_SIZE
TOTAL_MAX = MAX_BATCH_SIZE

GDRIVE_FOLDER_ID_OTHER = "1nDveh8erKuSFTghCHhlPEEZmu9SUo8jB"
SERVICE_ACCOUNT_JSON_PATH_OTHER = os.path.join(BASE_DIR, "gdrive_service_account.json")

META_OTHER_FILE = os.path.join(DATA_DIR, "other_gdrive_meta.json")
META_FILES = [META_OTHER_FILE]
EXCLUDE_FILES = {'file_progress.json', 'other_gdrive_meta.json'}
ALLOWED_STATUS_FILE = {"new file", "active", "change", "done", "deleted"}

from divisional_level.utils_gdrive import load_meta

def get_batch_limit_proxy():
    return get_batch_config()

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Middleware Trace ID untuk setiap request http
@app.middleware("http")
async def add_trace_id(request: Request, call_next):
    trace_id = str(uuid.uuid4())
    request.state.trace_id = trace_id
    with logger.contextualize(trace_id=trace_id):
        response = await call_next(request)
        response.headers['X-Trace-Id'] = trace_id
        return response

def try_include_router(module_name, router_name="router"):
    try:
        module = __import__(f"divisional_level.{module_name}", fromlist=[router_name])
        router = getattr(module, router_name)
        app.include_router(router)
        logger.info(f"[DEBUG] {module_name} router included")
    except Exception as e:
        logger.error(f"[ERROR] Failed to include {module_name} router: {e}")

try_include_router("scan_data_folder_summary")
try_include_router("upload_frontend_data")

try:
    from divisional_level.monitoring_process import router as monitoring_router
    app.include_router(monitoring_router)
    logger.info("[DEBUG] monitoring_process router included (explicit)")
except Exception as e:
    logger.error(f"[ERROR] Failed to explicitly include monitoring_process router: {e}")

def serialize_for_json(obj):
    if isinstance(obj, (datetime.date, datetime.datetime)):
        return obj.isoformat()
    if isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    return str(obj)

def get_valid_meta_files(meta_file=META_OTHER_FILE, status_file="active"):
    if not os.path.exists(meta_file):
        return []
    meta = load_meta(meta_file)
    if isinstance(meta, dict):
        meta = meta.get("files", [])
    return [
        m.get("saved_name") or m.get("name")
        for m in meta
        if (
            (m.get("saved_name") or m.get("name"))
            and (m.get("status_file", "") == status_file)
            and (m.get("saved_name") or m.get("name")).endswith(".parquet")
            and not (m.get("saved_name") or m.get("name")).endswith('.parquet.meta.json')
        )
    ]

def agentic_sync_meta_to_parquet():
    meta_file = META_OTHER_FILE
    data_dir = DATA_DIR
    if not os.path.exists(meta_file):
        with open(meta_file, "w", encoding="utf-8") as f:
            json.dump({"files": []}, f, indent=2, ensure_ascii=False)
    meta = load_meta(meta_file)
    meta_files = meta.get("files", meta) if isinstance(meta, dict) else meta
    meta_names = set((entry.get("name") or entry.get("saved_name") or "").strip() for entry in meta_files)
    files_on_disk = [f for f in os.listdir(data_dir)
                     if f.endswith(".parquet") and not f.endswith(".parquet.meta.json")]
    updated = False
    import uuid
    for fname in files_on_disk:
        if fname not in meta_names:
            id_new = str(uuid.uuid4())[:16]
            try:
                nrows = len(pd.read_parquet(os.path.join(data_dir, fname)))
            except Exception:
                nrows = 0
            entry = {
                "name": fname,
                "status_file": "new file",
                "id": id_new,
                "record_count": nrows
            }
            meta_files.append(entry)
            logger.info(f"[META][SYNC][ADD] File {fname} ditambahkan ke meta (id={id_new}, rows={nrows})")
            updated = True
    files_on_disk_set = set(files_on_disk)
    meta_new = []
    for entry in meta_files:
        fname = entry.get("name") or entry.get("saved_name")
        if fname in files_on_disk_set:
            meta_new.append(entry)
        else:
            logger.info(f"[META][SYNC][REMOVE] File {fname} di meta sudah tidak ada di disk, dihapus dari meta")
            updated = True
    if updated:
        with open(meta_file, "w", encoding="utf-8") as f:
            json.dump({"files": meta_new}, f, indent=2, ensure_ascii=False)
        logger.info(f"[META][SYNC] Meta master updated ({len(meta_new)} files)")
    return updated

@app.get("/config_limits")
def api_config_limits():
    return get_batch_limit_proxy()

@app.get("/list_active_files")
def list_active_files():
    return {"files": get_valid_meta_files()}

def ambil_data(file_name, offset, limit):
    fpath = os.path.join(DATA_DIR, file_name)
    if not os.path.exists(fpath):
        logger.error(f"File {file_name} not found")
        raise HTTPException(status_code=404, detail=f"File {file_name} not found")
    df = pd.read_parquet(fpath)
    return df.iloc[offset:offset+limit].to_dict(orient="records")

@app.get("/")
def root(request: Request):
    logger.info("Root called", extra={"trace_id": request.state.trace_id})
    return {"message": "FastAPI backend is running!"}

from divisional_level.utils_gdrive import (
    get_gdrive_file_list,
    scan_local_files,
    GDRIVE_FOLDER_ID as UTILS_GDRIVE_FOLDER_ID,
    SERVICE_ACCOUNT_JSON as UTILS_GDRIVE_SERVICE_ACCOUNT_JSON,
    DATA_DIR as UTILS_GDRIVE_DATA_DIR,
)

@app.get("/list_gdrive_files")
async def list_gdrive_files_endpoint(request: Request):
    try:
        files = get_gdrive_file_list(
            folder_id=UTILS_GDRIVE_FOLDER_ID,
            service_account_json_path=UTILS_GDRIVE_SERVICE_ACCOUNT_JSON
        )
        return {"status": "success", "count": len(files), "files": files}
    except Exception as e:
        logger.exception(f"[ERROR][ENDPOINT] /list_gdrive_files: {e}", extra={"trace_id": request.state.trace_id})
        return {"status": "error", "error": str(e), "files": []}

@app.get("/list_local_files")
async def list_local_files_endpoint(request: Request):
    try:
        files = scan_local_files(UTILS_GDRIVE_DATA_DIR)
        return {"status": "success", "count": len(files), "files": files}
    except Exception as e:
        logger.exception(f"[ERROR][ENDPOINT] /list_local_files: {e}", extra={"trace_id": request.state.trace_id})
        return {"status": "error", "error": str(e), "files": {}}

@app.post("/trigger_gdrive_sync")
async def trigger_gdrive_sync(request: Request):
    from divisional_level.utils_gdrive import trigger_gdrive_sync
    log = []
    try:
        _ = await request.json() if request.headers.get("content-type", "").startswith("application/json") else None
    except Exception:
        pass
    try:
        logger.info("[DEBUG] trigger_gdrive_sync: Syncing other folder", extra={"trace_id": request.state.trace_id})
        res_other = trigger_gdrive_sync(
            folder_id=GDRIVE_FOLDER_ID_OTHER,
            data_dir=DATA_DIR,
            service_account_json_path=SERVICE_ACCOUNT_JSON_PATH_OTHER,
            meta_prefix="other"
        )
        log.append(f"Synced other folder: {res_other.get('meta_file_main', '')}")
    except Exception as e:
        log.append(f"Failed to sync other: {e}")
        logger.error(f"[DEBUG] trigger_gdrive_sync: Failed to sync other: {e}", extra={"trace_id": request.state.trace_id})
    return JSONResponse({"status": "done", "log": log})

@app.post("/trigger_download_missing_files")
async def trigger_download_missing_files(request: Request):
    from divisional_level.download_gdrive_files import download_missing_files

    log = []
    results = []
    try:
        _ = await request.json() if request.headers.get("content-type", "").startswith("application/json") else None
    except Exception:
        pass

    folder_configs = [
        {
            "meta_prefix": "other",
            "data_dir": DATA_DIR,
            "service_account_json_path": SERVICE_ACCOUNT_JSON_PATH_OTHER,
        }
    ]
    for conf in folder_configs:
        try:
            meta_file_path = os.path.join(conf["data_dir"], "other_gdrive_meta.json")
            res = download_missing_files(
                data_dir=conf["data_dir"],
                meta_path=meta_file_path,
                service_account_json_path=conf["service_account_json_path"]
            )
            results.append(res)
            log.append(f"Downloaded for {conf['meta_prefix']}")
        except Exception as e:
            results.append({"error": str(e)})
            log.append(f"Failed to download for {conf['meta_prefix']}: {e}")
            logger.error(f"Failed to download for {conf['meta_prefix']}: {e}", extra={"trace_id": request.state.trace_id})
    return {"status": "done", "log": log, "results": results}

@app.post("/sync_meta_only")
def sync_meta_only_endpoint(request: Request):
    logger.info("[DEBUG] /sync_meta_only called", extra={"trace_id": request.state.trace_id})
    updated = agentic_sync_meta_to_parquet()
    return {"status": "meta_synced", "updated": updated}

from divisional_level.batch_agentic import ProgressManager, run_batch_agentic

@app.get("/progress")
def api_get_progress():
    pm = ProgressManager(DATA_DIR)
    return pm.get_all_progress()

@app.get("/progress/{file_id}")
def api_get_progress_file(file_id: str):
    pm = ProgressManager(DATA_DIR)
    prog = pm.get_file_progress(file_id)
    if not prog:
        logger.warning(f"Progress for file_id {file_id} not found")
        raise HTTPException(status_code=404, detail="Not found")
    return {file_id: prog}

@app.post("/batch_run_agentic")
def api_run_batch_agentic(background_tasks: BackgroundTasks, request: Request):
    def _run():
        logger.info("[DEBUG] /batch_run_agentic background batch start", extra={"trace_id": request.state.trace_id})
        run_batch_agentic()
        logger.info("[DEBUG] /batch_run_agentic background batch finished", extra={"trace_id": request.state.trace_id})
    background_tasks.add_task(_run)
    return {"status": "batch_agentic_started"}

@app.post("/progress/manual-reset")
def api_manual_reset(file_id: str = Query(..., description="Nama file untuk direset progress-nya"), request: Request = None):
    pm = ProgressManager(DATA_DIR)
    pm.reset_progress(file_id)
    logger.info(f"Progress for {file_id} reset", extra={"trace_id": request.state.trace_id if request else None})
    return {"status": "reset", "file": file_id}

@app.post("/progress/remove")
def api_remove_progress(file_id: str = Query(..., description="Nama file untuk dihapus progress-nya"), request: Request = None):
    pm = ProgressManager(DATA_DIR)
    pm.remove_file_progress(file_id)
    logger.info(f"Progress for {file_id} removed", extra={"trace_id": request.state.trace_id if request else None})
    return {"status": "removed", "file": file_id}

def _detect_file(tname, tdict, data_dir):
    filename = tdict.get('filename') or tdict.get('file_path') or tdict.get('saved_name') or None
    if filename and os.path.basename(filename):
        filename = os.path.basename(filename)
    else:
        candidates = []
        for f in get_valid_meta_files():
            fname, ext = os.path.splitext(f)
            if fname == tname or f == tname or f.startswith(tname):
                candidates.append(f)
        filename = candidates[0] if candidates else tname
    logger.info(f"[DEBUG] _detect_file: tname={tname}, detected filename={filename}")
    return filename

def collect_tabular_data(data_dir, only_table=None, include_progress=True, only_processed=True):
    logger.info(f"[DEBUG] collect_tabular_data: only_table={only_table}, only_processed={only_processed}")
    tables_parquet = load_all_parquet_tables(data_dir)
    logger.info(f"[DEBUG] collect_tabular_data: loaded tables_parquet={list(tables_parquet.keys())}")
    file_entries = []
    keys = [only_table] if only_table else list(tables_parquet.keys())
    for tname in keys:
        tdict = tables_parquet.get(tname)
        if not tdict:
            continue
        filename = _detect_file(tname, tdict, data_dir)
        if filename in EXCLUDE_FILES:
            logger.info(f"[DEBUG] collect_tabular_data: skipping excluded file {filename}")
            continue
        data = tdict.get('data', [])
        processed = None
        if only_processed:
            try:
                with open(os.path.join(DATA_DIR, "file_progress.json"), "r", encoding="utf-8") as f:
                    progress_map = json.load(f)
                processed = progress_map.get(filename, {}).get("processed", None)
            except Exception:
                processed = None
        if processed is not None and processed > 0:
            filtered_data = data[:processed]
        elif processed is not None and processed == 0:
            filtered_data = []
        else:
            filtered_data = data
        for row in filtered_data:
            row_with_file = dict(row)
            row_with_file['data_file'] = filename
            file_entries.append((tname, tdict, filename, len(filtered_data)))
    file_entries = sorted(file_entries, key=lambda x: x[3])
    merged = []
    for tname, tdict, filename, _ in file_entries:
        data = tdict.get('data', [])
        processed = None
        if only_processed:
            try:
                with open(os.path.join(DATA_DIR, "file_progress.json"), "r", encoding="utf-8") as f:
                    progress_map = json.load(f)
                processed = progress_map.get(filename, {}).get("processed", None)
            except Exception:
                processed = None
        if processed is not None and processed > 0:
            filtered_data = data[:processed]
        elif processed is not None and processed == 0:
            filtered_data = []
        else:
            filtered_data = data
        for row in filtered_data:
            row_with_file = dict(row)
            row_with_file['data_file'] = filename
            merged.append(row_with_file)
    logger.info(f"[DEBUG] collect_tabular_data: merged data length={len(merged)}")
    return merged

def list_all_tables(data_dir):
    logger.info(f"[DEBUG] list_all_tables called")
    tables_parquet = load_all_parquet_tables(data_dir)
    result_tables = list(tables_parquet.keys())
    logger.info(f"[DEBUG] list_all_tables: result_tables={result_tables}")
    return result_tables

@app.get("/list_tables")
def api_list_tables():
    logger.info("[DEBUG] api_list_tables called")
    return JSONResponse(content={"tables": list_all_tables(DATA_DIR)})

@app.get("/all_data_merge")
def api_all_data_merge(
    limit: int = Query(START_BATCH_SIZE, ge=1, le=BATCH_SIZE),
    offset: int = Query(0, ge=0),
    table: str = Query(None),
    request: Request = None
):
    logger.info(f"[DEBUG] api_all_data_merge called: limit={limit}, offset={offset}, table={table}", extra={"trace_id": request.state.trace_id if request else None})
    try:
        merged = collect_tabular_data(DATA_DIR, only_table=table, include_progress=False, only_processed=True)
        paged_data = merged[offset:offset+limit]
        logger.info(f"[DEBUG] api_all_data_merge: paged_data length={len(paged_data)}", extra={"trace_id": request.state.trace_id if request else None})
        json_compatible = json.loads(json.dumps(paged_data, default=serialize_for_json))
        return JSONResponse(content=json_compatible)
    except Exception as e:
        logger.exception(f"[all_data_merge][ERROR] {e}", extra={"trace_id": request.state.trace_id if request else None})
        return JSONResponse(content=[])

@app.get("/all_data_merge_count")
def api_all_data_merge_count(
    table: str = Query(None),
    request: Request = None
):
    logger.info("[DEBUG] api_all_data_merge_count called", extra={"trace_id": request.state.trace_id if request else None})
    try:
        merged = collect_tabular_data(DATA_DIR, only_table=table, include_progress=False, only_processed=True)
        return {"count": len(merged)}
    except Exception as e:
        logger.exception(f"[all_data_merge_count][ERROR] {e}", extra={"trace_id": request.state.trace_id if request else None})
        return {"count": 0}

@app.post("/all_data_merge")
@app.put("/all_data_merge")
@app.patch("/all_data_merge")
async def api_all_data_merge_post(
    request: Request,
    limit: int = Query(START_BATCH_SIZE, ge=1, le=BATCH_SIZE),
    offset: int = Query(0, ge=0),
    table: str = Query(None)
):
    logger.info(f"[DEBUG] api_all_data_merge_post called: limit={limit}, offset={offset}, table={table}", extra={"trace_id": request.state.trace_id})
    max_size = 100 * 1024 * 1024
    try:
        body = await request.body()
        if len(body) > max_size:
            logger.warning("[DEBUG] api_all_data_merge_post: body too large", extra={"trace_id": request.state.trace_id})
            raise HTTPException(status_code=413, detail="Payload too large (max 100 MB)")
        data = await request.json()
        if isinstance(data, list):
            merged = data[offset:offset+limit]
        elif isinstance(data, dict):
            if 'data' in data and isinstance(data['data'], list):
                merged = data['data'][offset:offset+limit]
            else:
                merged = [data][offset:offset+limit]
        else:
            merged = []
        if not merged:
            logger.warning("[DEBUG] api_all_data_merge_post: no data in body, fallback to local", extra={"trace_id": request.state.trace_id})
            raise Exception("No data in body, fallback to local")
        json_compatible = json.loads(json.dumps(merged, default=serialize_for_json))
        return JSONResponse(content=json_compatible)
    except Exception as e:
        logger.exception(f"[all_data_merge_post][ERROR] {e}", extra={"trace_id": request.state.trace_id})
        try:
            merged = collect_tabular_data(DATA_DIR, only_table=table, include_progress=False, only_processed=True)
            paged_data = merged[offset:offset+limit]
            json_compatible = json.loads(json.dumps(paged_data, default=serialize_for_json))
            return JSONResponse(content=json_compatible)
        except Exception as e2:
            logger.exception(f"[all_data_merge_post][HYBRID-FALLBACK] Fallback total failure: {e2}", extra={"trace_id": request.state.trace_id})
            return JSONResponse(content=[])

@app.get("/download_data")
def download_data(table: str = Query(None, description="Nama table/data yang ingin diunduh (tanpa extensi)"), request: Request = None):
    logger.info(f"[DEBUG] download_data called: table={table}", extra={"trace_id": request.state.trace_id if request else None})
    file_path, file_name, media_type = get_first_parquet_file_path(DATA_DIR, table)
    if not file_path or not os.path.exists(file_path):
        logger.warning(f"[DEBUG] download_data: file not found", extra={"trace_id": request.state.trace_id if request else None})
        raise HTTPException(status_code=404, detail="File data tidak ditemukan")
    logger.info(f"[DEBUG] download_data: sending file {file_path}", extra={"trace_id": request.state.trace_id if request else None})
    return FileResponse(file_path, media_type=media_type, filename=file_name)

def cleanup_orphan_files(data_dir, meta_paths, exclude_files=None):
    if exclude_files is None:
        exclude_files = set()
    else:
        exclude_files = set(exclude_files)
    if isinstance(meta_paths, str):
        meta_paths = [meta_paths]
    expected_files = set()
    for meta_path in meta_paths:
        if not os.path.exists(meta_path):
            continue
        meta = load_meta(meta_path)
        if isinstance(meta, dict):
            meta = meta.get("files", [])
        expected_files.update(m.get("saved_name") or m.get("name") for m in meta if m.get("saved_name") or m.get("name"))
    current_files = set(os.listdir(data_dir))
    protected_files = expected_files | exclude_files
    orphan_files = [f for f in current_files if f not in protected_files]
    deleted = []
    for f in orphan_files:
        file_path = os.path.join(data_dir, f)
        try:
            if os.path.isfile(file_path) or os.path.islink(file_path):
                os.remove(file_path)
                deleted.append(f)
                logger.info(f"[CLEANUP] Deleted orphan file: {f}")
            elif os.path.isdir(file_path):
                shutil.rmtree(file_path)
                deleted.append(f)
                logger.info(f"[CLEANUP] Deleted orphan folder: {f}")
        except Exception as e:
            logger.error(f"[CLEANUP][ERROR] Failed to delete {f}: {e}")
    return deleted

@app.post("/cleanup_orphan_files")
def cleanup_orphan_files_endpoint():
    meta_paths = META_FILES
    exclude = set(EXCLUDE_FILES)
    for mp in meta_paths:
        exclude.add(os.path.basename(mp))
    deleted = cleanup_orphan_files(DATA_DIR, meta_paths, exclude_files=exclude)
    return {"status": "success", "deleted_files": deleted}

@app.post("/trigger_data_cleaner")
def trigger_data_cleaner_endpoint(request: Request):
    try:
        from divisional_level.data_cleaner import cleaner_run
    except Exception as e:
        logger.error(f"Gagal import cleaner_run: {e}", extra={"trace_id": request.state.trace_id})
        return JSONResponse(content={"error": f"Gagal import cleaner_run: {e}"}, status_code=500)
    try:
        result = cleaner_run()
        return JSONResponse(content=result)
    except Exception as e:
        logger.exception(f"[trigger_data_cleaner_endpoint][ERROR] {e}", extra={"trace_id": request.state.trace_id})
        return JSONResponse(content={"error": str(e)}, status_code=500)

from divisional_level.data_eda.eda_prefect_flow import eda_analyze_all_files

def load_eda_logs_from_data_log():
    # PATCH: Use correct log folder
    DATA_LOG_DIR = os.path.join(BASE_DIR, "data_eda", "data_log")
    logs = []
    if not os.path.exists(DATA_LOG_DIR):
        return logs
    for fname in os.listdir(DATA_LOG_DIR):
        if not fname.endswith('.json'):
            continue
        if fname == "baseline_stats.json":
            continue  # Exclude baseline_stats.json
        fpath = os.path.join(DATA_LOG_DIR, fname)
        try:
            with open(fpath, "r", encoding="utf-8") as f:
                logs.append(json.load(f))
        except Exception as e:
            logger.warning(f"[WARNING] Failed to load {fpath}: {e}")
    return logs

def parse_eda_log_info(log):
    info = {}
    eda_result = log.get("eda_result", {})
    # Ambil dari eda_result.sampling
    sampling = eda_result.get("sampling", {})
    sampling_method = sampling.get("strategy")
    n_sample = sampling.get("n_sample_used")
    stratify_col = sampling.get("stratify_col_used") or sampling.get("stratify_col")

    # Filter hanya status success, inject info sampling ke step sampling
    status_monitor_clean = []
    for step in log.get("status_monitor", []):
        if step.get("status") == "success":
            item = dict(step)
            item["step"] = step.get("step")
            if step.get("step") == "sampling":
                if sampling_method:
                    item["sampling_method"] = sampling_method
                if n_sample is not None:
                    item["n_sample"] = n_sample
                if stratify_col:
                    item["stratify_col"] = stratify_col
            status_monitor_clean.append(item)
    info["status_monitor"] = status_monitor_clean
    info["completeness_score"] = eda_result.get("completeness_score")
    info["confidence_score"] = eda_result.get("confidence_score")
    info["confidence_interval"] = eda_result.get("confidence_interval")
    info["eda_methods"] = log.get("eda_methods", [])
    info["eda_tools"] = log.get("eda_tools", [])
    info["audit_testing"] = eda_result.get("audit_testing")
    return info

@app.post("/run_eda_prefect_flow", status_code=status.HTTP_202_ACCEPTED)
async def run_eda_prefect_flow(request: Request):
    """
    Trigger EDA Prefect Flow secara langsung (tanpa agent, tanpa deployment).
    Body (JSON):
    {
      "data_dir": "C:\\Users\\ASUS\\atlas_project\\backend-python\\divisional_level\\data",
      "n_sample": 50,
      "frac": null,
      "stratify_col": null,
      "weight_col": null
    }
    Output: {
      "message": ...,
      "result": ...,
      "status": ...
    }
    """
    try:
        params = await request.json()
    except Exception:
        params = {}

    data_dir = params.get("data_dir", r"C:\Users\ASUS\atlas_project\backend-python\divisional_level\data")
    n_sample = params.get("n_sample")
    frac = params.get("frac")
    stratify_col = params.get("stratify_col", None)
    weight_col = params.get("weight_col", None)

    if n_sample is not None and frac is not None:
        return JSONResponse(
            status_code=400,
            content={
                "error": "Jangan isi keduanya: n_sample dan frac. Pilih salah satu."
            }
        )
    if n_sample is None and frac is None:
        n_sample = 50  # fallback default

    try:
        summary = eda_analyze_all_files(
            data_dir=data_dir,
            n_sample=n_sample,
            frac=frac,
            stratify_col=stratify_col,
            weight_col=weight_col,
        )

        logs = load_eda_logs_from_data_log()

        insights, recommendations, status_monitor = [], [], []
        completeness_scores, confidence_scores, confidence_intervals, audit_testings = [], [], [], []
        eda_methods, eda_tools = set(), set()

        # PATCH: Collect per-file metrics for status
        status_per_file = []
        for log in logs:
            eda_insight = log.get("eda_result", {}).get("eda_insight", {})
            insights.extend(eda_insight.get("insight", []) if eda_insight else [])
            recommendations.extend(eda_insight.get("recommendations", []) if eda_insight else [])
            parsed = parse_eda_log_info(log)
            # Per file status
            status_per_file.append({
                "file": log.get("file"),
                "file_id": log.get("file_id"),
                "completeness_score": parsed.get("completeness_score"),
                "confidence_score": parsed.get("confidence_score"),
                "confidence_interval": parsed.get("confidence_interval"),
                "audit_testing": parsed.get("audit_testing", {}).get("score") if parsed.get("audit_testing") else None,
                "status_monitor": parsed.get("status_monitor"),
            })
            # For global avg calculation
            if parsed.get("completeness_score") is not None:
                completeness_scores.append(parsed.get("completeness_score"))
            if parsed.get("confidence_score") is not None:
                confidence_scores.append(parsed.get("confidence_score"))
            if parsed.get("confidence_interval") is not None:
                confidence_intervals.append(parsed.get("confidence_interval"))
            if parsed.get("audit_testing") and parsed.get("audit_testing").get("score") is not None:
                audit_testings.append(parsed.get("audit_testing").get("score"))
            if parsed.get("eda_methods"):
                eda_methods.update(parsed.get("eda_methods"))
            if parsed.get("eda_tools"):
                eda_tools.update(parsed.get("eda_tools"))

        # Calculate global averages
        def avg(xs):
            xs = [x for x in xs if x is not None]
            return sum(xs) / len(xs) if xs else None

        avg_completeness = avg(completeness_scores)
        avg_confidence = avg(confidence_scores)
        avg_audit_testing = avg(audit_testings)

        # For confidence_interval: average lower and upper bounds
        if confidence_intervals:
            lowers = [ci[0] for ci in confidence_intervals if isinstance(ci, list) and len(ci) == 2]
            uppers = [ci[1] for ci in confidence_intervals if isinstance(ci, list) and len(ci) == 2]
            avg_confidence_interval = [avg(lowers), avg(uppers)] if lowers and uppers else None
        else:
            avg_confidence_interval = None

        errors = summary.get("errors", 0)
        anomaly_rate = summary.get("anomaly_rate", 0)
        success_rate = summary.get("success_rate", 1)
        num_files = summary.get("num_files", 1)
        errors_pct = round(100.0 * errors / max(num_files, 1), 2)
        anomaly_pct = round(100.0 * anomaly_rate, 2)
        success_pct = round(100.0 * success_rate, 2)

        result = {
            "timestamp": summary.get("timestamp"),
            "data_dir": summary.get("data_dir"),
            "num_files": num_files,
            "runtime": summary.get("runtime"),
            "errors": f"{errors_pct}%",
            "anomaly_rate": f"{anomaly_pct}%",
            "success_rate": f"{success_pct}%",
            "completeness_score": avg_completeness,
            "confidence_score": avg_confidence,
            "confidence_interval": avg_confidence_interval,
            "audit_testing": avg_audit_testing,
            "insight": insights,
            "recommendation": recommendations
        }
        # Tambahkan eda_methods hanya jika tidak kosong
        if eda_methods:
            result["eda_methods"] = list(eda_methods)
        # Tambahkan eda_tools sesuai isi, fallback jika kosong
        if eda_tools:
            result["eda_tools"] = list(eda_tools)
        else:
            result["eda_tools"] = ["YDataprofiling", "great_expectatio", "advanced_eda"]

        return JSONResponse(
            status_code=200,
            content=json.loads(json.dumps({
                "message": "EDA Prefect Flow berhasil dijalankan.",
                "result": result,
                "status": status_per_file
            }, default=serialize_for_json)),
        )
    except Exception as e:
        import traceback
        tb = traceback.format_exc()
        logger.exception(f"[run_eda_prefect_flow][ERROR] {e}")
        return JSONResponse(
            status_code=500,
            content=json.loads(json.dumps({
                "error": str(e),
                "traceback": tb,
                "message": "Terjadi error saat menjalankan EDA Prefect Flow.",
            }, default=serialize_for_json)),
        )

if __name__ == "__main__":
    import uvicorn
    logger.info("[DEBUG] __main__ starting uvicorn")
    uvicorn.run("all_data_backend:app", host="0.0.0.0", port=8000, reload=True, workers=1)

2. utils_gdrive.py:

import os
import json
import traceback
import fnmatch
import time
from datetime import datetime, timezone
import pandas as pd
import pyarrow.parquet as pq
from fastapi import APIRouter, Request

router = APIRouter()

DATA_DIR = r"C:\Users\ASUS\atlas_project\backend-python\divisional_level\data"
GDRIVE_FOLDER_ID = "1nDveh8erKuSFTghCHhlPEEZmu9SUo8jB"
SERVICE_ACCOUNT_JSON = os.path.join(os.path.dirname(os.path.abspath(__file__)), "gdrive_service_account.json")
META_MASTER_PATH = os.path.join(DATA_DIR, "other_gdrive_meta.json")

META_EXCLUDE = {
    "other_gdrive_meta.json",
    "csvjson_gdrive_meta.json",
    "file_progress.json",
    "meta_config.txt",
}
META_EXCLUDE_PATTERNS = [
    "*.meta.json",
    "other_gdrive_meta.json",
    "csvjson_gdrive_meta.json",
    "file_progress.json",
    "meta_config.txt",
]

ALLOWED_STATUS_FILE = {"new file", "active", "change", "deleted"}

BUSINESS_KEYWORDS = {
    "finance": "finance",
    "keuangan": "finance",
    "accounting": "finance",
    "akunting": "finance",
    "hr": "hr",
    "humanresource": "hr",
    "personalia": "hr",
    "payroll": "hr",
    "strategic": "strategic",
    "strategy": "strategic",
    "operation": "operation",
    "operasi": "operation",
    "ops": "operation",
    "projectmanagement": "project management",
    "project_mgmt": "project management",
    "project": "project management",
    "salesmarketing": "sales-marketing",
    "sales": "sales-marketing",
    "marketing": "sales-marketing",
    "penjualan": "sales-marketing",
    "pemasaran": "sales-marketing",
    "it": "it",
    "ict": "it",
    "teknologi": "it",
    "technology": "it",
    "procurement": "procurement",
    "pengadaan": "procurement",
    "logistics": "logistics",
    "logistik": "logistics",
    "gudang": "logistics",
    "warehouse": "logistics",
    "risk": "risk",
    "risiko": "risk",
    "legal": "legal",
    "hukum": "legal",
    "audit": "audit",
    "compliance": "audit",
    "compliant": "audit",
    "regulatory": "audit",
    "admin": "admin",
    "administrasi": "admin",
}
BUSINESS_PATTERNS = {k.lower(): v for k, v in BUSINESS_KEYWORDS.items()}

def smart_extract_tags(filename):
    fname = filename.lower()
    fname_noext = os.path.splitext(fname)[0]
    fname_compact = fname_noext.replace("_", "").replace("-", "").replace(" ", "")
    tags = []
    for keyword, tag in BUSINESS_PATTERNS.items():
        if keyword in fname_compact and tag not in tags:
            tags.append(tag)
    if tags:
        return tags
    base = os.path.splitext(os.path.basename(filename))[0]
    split_tags = [t for t in base.replace("-", "_").split("_") if t]
    return split_tags

def is_meta_file(filename):
    if filename in META_EXCLUDE:
        return True
    for pat in META_EXCLUDE_PATTERNS:
        if fnmatch.fnmatch(filename, pat):
            return True
    return False

def debug_print_paths(service_account_json_path, folder_id, data_dir):
    print(f"[DEBUG] SERVICE_ACCOUNT_JSON_PATH_OTHER: {service_account_json_path}")
    print(f"[DEBUG] GDRIVE_FOLDER_ID_OTHER: {folder_id}")
    print(f"[DEBUG] DATA_DIR: {data_dir}")
    print(f"[DEBUG] Akan menulis ke: {os.path.join(data_dir, 'other_gdrive_meta.json')}")

def utc_now_iso():
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat() + "Z"

def read_global_meta_config(data_dir):
    backend_python_dir = os.path.dirname(os.path.abspath(__file__))
    meta_config_path = os.path.join(backend_python_dir, "meta_config.txt")
    meta = {
        "pipeline_job_id": None,
        "operator": None,
        "confidentiality": None,
        "data_owner": None
    }
    if not os.path.exists(meta_config_path):
        return meta
    try:
        with open(meta_config_path, "r", encoding="utf-8") as f:
            for line in f:
                if ":" in line:
                    k, v = line.split(":", 1)
                    key = k.strip().lower()
                    value = v.strip()
                    if key in meta:
                        meta[key] = value if value else None
    except Exception as e:
        print(f"[ERROR][META_CONFIG] Failed to read {meta_config_path}: {e}")
        traceback.print_exc()
    return meta

def get_gdrive_file_list(folder_id, service_account_json_path):
    from googleapiclient.discovery import build
    from google.oauth2 import service_account
    SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
    try:
        creds = service_account.Credentials.from_service_account_file(service_account_json_path, scopes=SCOPES)
        service = build('drive', 'v3', credentials=creds)
        query = f"'{folder_id}' in parents and trashed = false"
        meta_files = []
        page_token = None
        print(f"[DEBUG][GDRIVE] Querying folder_id: {folder_id}")
        while True:
            response = service.files().list(
                q=query,
                spaces='drive',
                fields='nextPageToken, files(id, name, mimeType, md5Checksum, modifiedTime, createdTime, size, owners)',
                pageToken=page_token
            ).execute()
            files = response.get('files', [])
            print(f"[DEBUG][GDRIVE] Found {len(files)} files in this page: {[f['name'] for f in files]}")
            for f in files:
                if f['name'].strip().lower() == "meta_config.txt":
                    continue
                base, ext = os.path.splitext(f['name'].strip())
                meta_files.append({
                    'id': f['id'],
                    'name': base + ".parquet",
                    'md5Checksum': f.get('md5Checksum'),
                    'modifiedTime': f.get('modifiedTime'),
                    'createdTime': f.get('createdTime'),
                    'mimeType': f.get('mimeType', ""),
                    'gdrive_mime_type': f.get('mimeType', ""),
                    'size': int(f.get('size', 0)) if f.get('size') else 0,
                    'owners': f.get('owners', []),
                    'tags': smart_extract_tags(base + ext)
                })
            page_token = response.get('nextPageToken', None)
            if not page_token:
                break
        print(f"[DEBUG][GDRIVE] Total files found: {len(meta_files)}")
        return meta_files
    except Exception as e:
        print(f"[ERROR][GDRIVE] Failed to list files: {e}")
        traceback.print_exc()
        return []

def scan_local_files(data_dir, prev_meta=None):
    files = {}
    prev_hash_index = {}
    if prev_meta and "files" in prev_meta:
        for e in prev_meta["files"]:
            prev_hash_index[e.get("name")] = e
    if not os.path.exists(data_dir):
        print(f"[WARN][LOCAL] Data dir {data_dir} does not exist.")
        return files
    for fname in os.listdir(data_dir):
        if is_meta_file(fname):
            continue
        if not fname.lower().endswith('.parquet'):
            continue
        fpath = os.path.join(data_dir, fname)
        if not os.path.isfile(fpath):
            continue
        try:
            stat = os.stat(fpath)
            mtime = datetime.fromtimestamp(stat.st_mtime, timezone.utc).replace(microsecond=0).isoformat() + "Z"
            prev = prev_hash_index.get(fname)
            cache_valid = prev and prev.get("local_size") == stat.st_size and prev.get("local_modified") == mtime
            files[fname.strip()] = {
                "local_exists": True,
                "local_size": stat.st_size,
                "local_modified": mtime,
                "tags": smart_extract_tags(fname),
                "local_md5": prev.get("local_md5") if cache_valid else None,
                "sha256": prev.get("sha256") if cache_valid else None,
                "row_count": prev.get("row_count") if cache_valid else None,
                "column_count": prev.get("column_count") if cache_valid else None,
            }
        except Exception as e:
            print(f"[WARN][LOCAL] Could not stat file {fpath}: {e}")
            traceback.print_exc()
    return files

def load_meta(meta_path):
    if os.path.exists(meta_path):
        try:
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
                if isinstance(meta, dict) and "files" in meta:
                    return meta
                elif isinstance(meta, list):
                    return {"files": meta}
                else:
                    return {"files": []}
        except Exception as e:
            print(f"[WARN][META] Failed to load {meta_path}: {e}")
            traceback.print_exc()
            return {"files": []}
    return {"files": []}

def save_meta(meta_path, meta, prev_meta=None):
    try:
        def clean_entry(entry, prev_entry=None):
            allowed = [
                "name", "id", "gdrive_exists", "local_exists", "gdrive_md5", "local_md5",
                "gdrive_modified", "gdrive_size", "local_size",
                "last_sync_time", "status_file", "status_process", "history",
                "sha256", "local_modified", "row_count", "column_count", "gdrive_mime_type", "process_duration",
                "created_at", "file_type", "source_modified", "record_count",
                "row_start", "row_end", "upstream_source", "pipeline_job_id", "operator", "tags", "confidentiality", "data_owner", "process_end_time",
                "source_sql_file", "file_role", "derived_file_count", "jsonschema_verified"
            ]
            merged = dict(prev_entry) if prev_entry else {}
            for k in allowed:
                merged[k] = entry.get(k, merged.get(k, None))
            if not merged.get("tags"):
                merged["tags"] = smart_extract_tags(merged.get("name") or "")
            if merged.get("status_file") not in ALLOWED_STATUS_FILE:
                merged["status_file"] = "change"
            if merged.get("status_process") not in {"download", "process"}:
                merged["status_process"] = "process"
            if "history" in merged and isinstance(merged["history"], list):
                merged["history"] = [
                    {kk: vv for kk, vv in h.items() if kk not in {"status_reason", "process_end_time", "last_status_change", "local_modified"}}
                    for h in merged["history"]
                ]
            return merged

        prev_meta_index = {}
        if prev_meta and "files" in prev_meta:
            for e in prev_meta["files"]:
                if "name" in e:
                    prev_meta_index[e["name"]] = e

        if isinstance(meta, dict) and "files" in meta:
            meta["files"] = [
                clean_entry(e, prev_meta_index.get(e.get("name"))) for e in meta["files"]
            ]
        elif isinstance(meta, list):
            meta = {"files": [clean_entry(e, prev_meta_index.get(e.get("name"))) for e in meta]}
        else:
            meta = {"files": []}
        tmp_path = meta_path + ".tmp"
        with open(tmp_path, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)
        os.replace(tmp_path, meta_path)
        print(f"[DEBUG][save_meta] File saved: {meta_path}, size: {os.path.getsize(meta_path)}")
    except Exception as e:
        print(f"[ERROR][META] Failed to save {meta_path}: {e}")
        traceback.print_exc()

def calc_local_md5(fname, data_dir):
    try:
        import hashlib
        file_path = os.path.join(data_dir, fname)
        md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                md5.update(chunk)
        return md5.hexdigest()
    except Exception:
        return None

def calc_local_sha256(fname, data_dir):
    try:
        import hashlib
        file_path = os.path.join(data_dir, fname)
        sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256.update(chunk)
        return sha256.hexdigest()
    except Exception:
        return None

def get_row_and_column_count(fname, data_dir):
    file_path = os.path.join(data_dir, fname)
    ext = os.path.splitext(fname)[1].lower()
    try:
        if ext == ".parquet":
            tbl = pq.read_table(file_path)
            return tbl.num_rows, tbl.num_columns
        elif ext == ".csv":
            df = pd.read_csv(file_path, nrows=10**6)
            return df.shape[0], df.shape[1]
        elif ext in (".xls", ".xlsx"):
            df = pd.read_excel(file_path, nrows=10**6)
            return df.shape[0], df.shape[1]
        else:
            return None, None
    except Exception:
        return None, None

def batch_changed(prev_files, new_files):
    prev_names = sorted([f["name"] for f in prev_files])
    new_names = sorted([f["name"] for f in new_files])
    return prev_names != new_names

def summarize_files(meta_files, synced_at):
    import hashlib

    name = "parquet"
    id_count = len(meta_files)
    gd_exists_vals = [f.get("gdrive_exists", False) for f in meta_files]
    gdrive_exists = "all true" if all(gd_exists_vals) else f"{gd_exists_vals.count(True)} true, {gd_exists_vals.count(False)} false"
    loc_exists_vals = [f.get("local_exists", False) for f in meta_files]
    local_exists = "all true" if all(loc_exists_vals) else f"{loc_exists_vals.count(True)} true, {loc_exists_vals.count(False)} false"
    gdrive_md5_vals = [f.get("gdrive_md5") for f in meta_files]
    gdrive_md5 = "all non-null" if all(gdrive_md5_vals) else f"{sum(1 for v in gdrive_md5_vals if v)} non-null, {sum(1 for v in gdrive_md5_vals if not v)} null"
    local_md5_vals = [f.get("local_md5") for f in meta_files]
    local_md5 = "all non-null" if all(local_md5_vals) else f"{sum(1 for v in local_md5_vals if v)} non-null, {sum(1 for v in local_md5_vals if not v)} null"
    gdrive_modified = "all non-null" if all(f.get("gdrive_modified") for f in meta_files) else "some null"
    gdrive_size = sum(int(f.get("gdrive_size") or 0) for f in meta_files)
    local_size = sum(int(f.get("local_size") or 0) for f in meta_files)
    last_sync_time = "all non-null" if all(f.get("last_sync_time") for f in meta_files) else "some null"
    status_count = {}
    for f in meta_files:
        st = f.get("status_file", "unknown")
        status_count[st] = status_count.get(st, 0) + 1
    status_str = ", ".join([f"{k}: {v}" for k, v in status_count.items()])
    history = f"File distribution status: {status_str}. Last synchronization: {synced_at}"
    summary_bytes = json.dumps(meta_files, sort_keys=True, default=str).encode("utf-8")
    sha256 = hashlib.sha256(summary_bytes).hexdigest()
    local_modified_count = sum(1 for f in meta_files if f.get("local_modified"))
    row_count_sum = sum(int(f.get("row_count") or 0) for f in meta_files)
    column_count_sum = sum(int(f.get("column_count") or 0) for f in meta_files)
    mime_types = {}
    for f in meta_files:
        mt = f.get("gdrive_mime_type", "unknown")
        mime_types[mt] = mime_types.get(mt, 0) + 1
    gdrive_mime_type = ", ".join([f"{mt}: {cnt}" for mt, cnt in mime_types.items()])
    process_duration_sum = sum(float(f.get("process_duration") or 0) for f in meta_files)

    # --- Build category_file summary for all files (main and derived) ---
    category_file = {}
    # Pre-calculate deriveds for main files
    derived_map = {}
    for f in meta_files:
        if f.get("file_role") == "main file":
            deriveds = [d for d in meta_files if d.get("source_sql_file") == f["name"]]
            derived_map[f["name"]] = deriveds

    for f in meta_files:
        entry = {
            "local_exists": f.get("local_exists"),
            "gdrive_exists": f.get("gdrive_exists"),
            "last_modified": f.get("local_modified"),
            "file_role": f.get("file_role"),
            "jsonschema_verified": f.get("jsonschema_verified"),
        }
        # Tambahkan derived_file_count dan sync_status hanya untuk main file
        if f.get("file_role") == "main file":
            deriveds = derived_map.get(f["name"], [])
            entry["derived_file_count"] = len(deriveds)
            if len(deriveds) > 0:
                synced_count = sum(1 for d in deriveds if d.get("local_exists"))
                entry["sync_status"] = f"{synced_count}/{len(deriveds)} derived synced, {len(deriveds)-synced_count} not synced"
            else:
                entry["sync_status"] = "0/0 derived synced, 0 not synced"
        category_file[f["name"]] = entry

    # Extra issues: find missing or stale derived files
    issues = []
    for f in meta_files:
        if f.get("file_role") == "main file":
            deriveds = derived_map.get(f["name"], [])
            for derived in deriveds:
                if not derived.get("local_exists"):
                    issues.append(f"Derived file {derived['name']} from main {f['name']} is missing/not processed.")
                elif f.get("local_modified") and derived.get("local_modified") and derived["local_modified"] < f["local_modified"]:
                    issues.append(f"Derived file {derived['name']} from main {f['name']} is not refreshed after main file changed.")

    # --- PATCH: JSONSchema summary status ---
    verified_list = [f.get("jsonschema_verified") for f in meta_files]
    verified_true = sum(1 for v in verified_list if v is True)
    verified_false = sum(1 for v in verified_list if v is False)
    verified_none = sum(1 for v in verified_list if v is None)
    total_files = len(meta_files)
    if verified_true == total_files and total_files > 0:
        verified_status = "true"
    elif verified_true > 0:
        verified_status = "partial"
    else:
        verified_status = "false"
    jsonschema_verified_summary = {
        "total_files": total_files,
        "verified_true": verified_true,
        "verified_false": verified_false,
        "verified_none": verified_none,
        "status": verified_status
    }

    return {
        "name": name,
        "id": id_count,
        "gdrive_exists": gdrive_exists,
        "local_exists": local_exists,
        "gdrive_md5": gdrive_md5,
        "local_md5": local_md5,
        "gdrive_modified": gdrive_modified,
        "gdrive_size": gdrive_size,
        "local_size": local_size,
        "last_sync_time": last_sync_time,
        "history": history,
        "sha256": sha256,
        "local_modified": local_modified_count,
        "row_count": row_count_sum,
        "column_count": column_count_sum,
        "gdrive_mime_type": gdrive_mime_type,
        "process_duration": process_duration_sum,
        "category_file": category_file,
        "issues": issues,
        "jsonschema_verified_summary": jsonschema_verified_summary
    }

def _derive_status_process(status_file):
    if status_file in ("new file", "change"):
        return "download"
    elif status_file in ("active", "deleted"):
        return "process"
    else:
        return "process"

def update_meta(meta_path, gdrive_files, local_files):
    print("[DEBUG][PATCHED] update_meta running...")
    now = utc_now_iso()
    prev_meta = load_meta(meta_path)
    prev_files = {f["name"]: f for f in prev_meta.get("files", [])}
    gdrive_names = {f['name'] for f in gdrive_files}
    local_names = set(local_files.keys())
    new_meta = {"files": []}
    gdrive_index = {f["name"]: f for f in gdrive_files}
    global_meta = read_global_meta_config(DATA_DIR)

    for fname, gfile in gdrive_index.items():
        if is_meta_file(fname):
            continue
        prev = prev_files.get(fname, {})
        local_info = local_files.get(fname, {})
        process_start = time.perf_counter()
        local_md5 = local_info.get("local_md5")
        if local_md5 is None and local_info.get("local_exists"):
            local_md5 = calc_local_md5(fname, DATA_DIR)
        sha256 = local_info.get("sha256")
        if sha256 is None and local_info.get("local_exists"):
            sha256 = calc_local_sha256(fname, DATA_DIR)
        row_count = local_info.get("row_count")
        column_count = local_info.get("column_count")
        if (row_count is None or column_count is None) and local_info.get("local_exists"):
            row_count, column_count = get_row_and_column_count(fname, DATA_DIR)
        local_modified = local_info.get("local_modified", None)
        process_end = time.perf_counter()
        process_duration = process_end - process_start
        entry = {
            "name": fname,
            "id": gfile.get("id"),
            "file_type": gfile.get("mimeType", ""),
            "created_at": gfile.get("createdTime", None),
            "source_modified": gfile.get("modifiedTime"),
            "gdrive_exists": True,
            "local_exists": local_info.get("local_exists", False),
            "gdrive_md5": gfile.get("md5Checksum"),
            "local_md5": local_md5,
            "gdrive_modified": gfile.get("modifiedTime"),
            "gdrive_size": int(gfile.get("size") or 0),
            "local_size": int(local_info.get("local_size") or 0),
            "last_sync_time": now,
            "history": prev.get("history", []) if prev else [],
            "sha256": sha256,
            "local_modified": local_modified,
            "row_count": row_count,
            "column_count": column_count,
            "gdrive_mime_type": gfile.get("gdrive_mime_type"),
            "process_duration": process_duration,
            "record_count": row_count,
            "row_start": None,
            "row_end": None,
            "upstream_source": GDRIVE_FOLDER_ID,
            "status_process": prev.get("status_process") if "status_process" in prev else None,
            "status_file": prev.get("status_file", None) if prev else "new file",
            "last_status_change": now,
            "process_end_time": now,
            "pipeline_job_id": global_meta.get("pipeline_job_id"),
            "operator": global_meta.get("operator"),
            "tags": smart_extract_tags(fname),
            "confidentiality": global_meta.get("confidentiality"),
            "data_owner": global_meta.get("data_owner") or (gfile.get("owners", [{}])[0].get("emailAddress", None) if gfile.get("owners") else None),
            "file_role": prev.get("file_role", None),
            "source_sql_file": prev.get("source_sql_file", None),
            "derived_file_count": prev.get("derived_file_count", 0),
            "jsonschema_verified": prev.get("jsonschema_verified", None)
        }
        if not entry.get("tags"):
            entry["tags"] = smart_extract_tags(fname)
        if not entry["local_exists"]:
            entry["status_file"] = "new file"
        elif entry["gdrive_md5"] and entry["local_md5"] and entry["gdrive_md5"] == entry["local_md5"]:
            entry["status_file"] = "active"
        else:
            prev_gdrive_md5 = prev.get("gdrive_md5")
            if prev_gdrive_md5 and prev_gdrive_md5 != entry["gdrive_md5"]:
                entry["status_file"] = "change"
            elif not prev:
                entry["status_file"] = "active"
            else:
                entry["status_file"] = "active"
        if entry["status_file"] in ("new file", "change"):
            entry["status_process"] = "download"
        else:
            entry["status_process"] = "process"
        new_meta["files"].append(entry)

    parquet_local_names = set()
    for fname in local_names - gdrive_names - set(prev_files.keys()):
        if is_meta_file(fname):
            continue
        if not fname.lower().endswith('.parquet'):
            continue
        base, ext = os.path.splitext(fname)
        parquet_name = fname
        local_info = local_files[fname]
        process_start = time.perf_counter()
        local_md5 = local_info.get("local_md5")
        if local_md5 is None:
            local_md5 = calc_local_md5(fname, DATA_DIR)
        sha256 = local_info.get("sha256")
        if sha256 is None:
            sha256 = calc_local_sha256(fname, DATA_DIR)
        row_count = local_info.get("row_count")
        column_count = local_info.get("column_count")
        if row_count is None or column_count is None:
            row_count, column_count = get_row_and_column_count(fname, DATA_DIR)
        local_modified = local_info.get("local_modified", None)
        process_end = time.perf_counter()
        process_duration = process_end - process_start
        entry = {
            "name": parquet_name,
            "id": None,
            "file_type": None,
            "created_at": None,
            "source_modified": None,
            "gdrive_exists": False,
            "local_exists": True,
            "gdrive_md5": None,
            "local_md5": local_md5,
            "gdrive_modified": None,
            "gdrive_size": 0,
            "local_size": int(local_info.get("local_size") or 0),
            "last_sync_time": now,
            "history": [],
            "sha256": sha256,
            "local_modified": local_modified,
            "row_count": row_count,
            "column_count": column_count,
            "gdrive_mime_type": None,
            "process_duration": process_duration,
            "record_count": row_count,
            "row_start": None,
            "row_end": None,
            "upstream_source": GDRIVE_FOLDER_ID,
            "status_file": "deleted",
            "status_process": _derive_status_process("deleted"),
            "last_status_change": now,
            "process_end_time": now,
            "pipeline_job_id": global_meta.get("pipeline_job_id"),
            "operator": global_meta.get("operator"),
            "tags": smart_extract_tags(parquet_name),
            "confidentiality": global_meta.get("confidentiality"),
            "data_owner": global_meta.get("data_owner"),
            "file_role": None,
            "source_sql_file": None,
            "derived_file_count": 0,
            "jsonschema_verified": None
        }
        if not entry.get("tags"):
            entry["tags"] = smart_extract_tags(parquet_name)
        new_meta["files"].append(entry)
        parquet_local_names.add(parquet_name)

    for fname, prev in prev_files.items():
        if is_meta_file(fname):
            continue
        if not fname.lower().endswith('.parquet'):
            continue
        if fname not in gdrive_names and fname not in parquet_local_names:
            entry = prev.copy()
            entry["gdrive_exists"] = False
            entry["status_file"] = "deleted"
            entry["status_process"] = _derive_status_process("deleted")
            entry["last_sync_time"] = now
            if fname in local_files:
                stat = local_files[fname]
                entry["local_modified"] = stat.get("local_modified", None)
                entry["local_size"] = stat.get("local_size", 0)
                entry["sha256"] = stat.get("sha256")
                entry["row_count"] = stat.get("row_count")
                entry["column_count"] = stat.get("column_count")
            if "row_start" not in entry:
                entry["row_start"] = None
            if "row_end" not in entry:
                entry["row_end"] = None
            entry["pipeline_job_id"] = global_meta.get("pipeline_job_id")
            entry["operator"] = global_meta.get("operator")
            entry["tags"] = smart_extract_tags(fname)
            entry["confidentiality"] = global_meta.get("confidentiality")
            entry["data_owner"] = global_meta.get("data_owner")
            if not entry.get("tags"):
                entry["tags"] = smart_extract_tags(fname)
            if "file_role" not in entry:
                entry["file_role"] = None
            if "source_sql_file" not in entry:
                entry["source_sql_file"] = None
            if "derived_file_count" not in entry:
                entry["derived_file_count"] = 0
            if "jsonschema_verified" not in entry:
                entry["jsonschema_verified"] = None
            new_meta["files"].append(entry)

    meta_dict = {e["name"]: e for e in prev_meta.get("files", []) if "name" in e}
    for entry in new_meta["files"]:
        fname = entry.get("name")
        if fname:
            meta_dict[fname] = entry
    meta_merged = {"files": list(meta_dict.values())}

    required_summary_keys = [
        "name", "id", "gdrive_exists", "local_exists", "gdrive_md5", "local_md5",
        "gdrive_modified", "gdrive_size", "local_size", "last_sync_time", "history",
        "sha256", "local_modified", "row_count", "column_count", "gdrive_mime_type", "process_duration"
    ]
    if not meta_merged.get("files"):
        summary = {k: None for k in required_summary_keys}
    else:
        summary = summarize_files(meta_merged["files"], now)
        for k in required_summary_keys:
            if k not in summary:
                summary[k] = None
    meta_merged["summary"] = summary

    prev_meta_for_merge = load_meta(meta_path)
    save_meta(meta_path, meta_merged, prev_meta=prev_meta_for_merge)
    print(f"[META][UPDATE] Overwritten (MERGE) {meta_path} ({len(meta_merged['files'])} entries)")
    return meta_merged

def trigger_gdrive_sync(
    folder_id,
    data_dir,
    service_account_json_path,
    meta_prefix="other"
):
    debug_print_paths(service_account_json_path, folder_id, data_dir)
    os.makedirs(data_dir, exist_ok=True)
    meta_path = os.path.join(data_dir, "other_gdrive_meta.json")
    prev_meta = load_meta(meta_path)
    gdrive_files = get_gdrive_file_list(folder_id, service_account_json_path)
    print(f"[DEBUG][SYNC] meta_prefix={meta_prefix} folder_id={folder_id} files={len(gdrive_files)}")
    local_files = scan_local_files(data_dir, prev_meta)
    print(f"[DEBUG] Akan menulis ke: {meta_path}")
    try:
        meta_other = update_meta(meta_path, gdrive_files, local_files)
    except Exception as e:
        print(f"[ERROR][SYNC] Failed meta update: {e}")
        traceback.print_exc()
        return {
            "status": "error",
            "error": str(e),
            "meta_file_main": meta_path,
            "file_count_main": 0,
            "files_main": [],
            "synced_at": utc_now_iso(),
        }
    return {
        "status": "success",
        "meta_file_main": meta_path,
        "file_count_main": len(meta_other["files"]),
        "files_main": [entry["name"] for entry in meta_other["files"]],
        "synced_at": utc_now_iso(),
        "summary": meta_other.get("summary")
    }

@router.post("/trigger_gdrive_sync")
async def trigger_gdrive_sync_endpoint(request: Request):
    try:
        if request.headers.get("content-type", "").startswith("application/json"):
            _ = await request.json()
    except Exception:
        pass
    result = trigger_gdrive_sync(
        folder_id=GDRIVE_FOLDER_ID,
        data_dir=DATA_DIR,
        service_account_json_path=SERVICE_ACCOUNT_JSON,
        meta_prefix="other"
    )
    return result

if __name__ == "__main__":
    print("=== Running GDrive Sync (CLI Mode) ===")
    debug_print_paths(SERVICE_ACCOUNT_JSON, GDRIVE_FOLDER_ID, DATA_DIR)
    result = trigger_gdrive_sync(
        folder_id=GDRIVE_FOLDER_ID,
        data_dir=DATA_DIR,
        service_account_json_path=SERVICE_ACCOUNT_JSON,
        meta_prefix="other"
    )
    print(json.dumps(result, indent=2, ensure_ascii=False))

3. download_gdrive_files.py:

import os
import json
import re
from datetime import datetime, timezone
from fastapi import APIRouter, Request
from googleapiclient.discovery import build
from google.oauth2 import service_account
from googleapiclient.http import MediaIoBaseDownload
from pathlib import Path
import duckdb
import pandas as pd
import numpy as np
import tempfile
import traceback
import sqlparse

import docling  # INTEGRASI DOCLING

from divisional_level.utils_gdrive import load_meta as load_meta_external  # To avoid name clash

# PATCH: Tambahkan jsonschema validator
from jsonschema import validate as jsonschema_validate, ValidationError as JsonSchemaValidationError

router = APIRouter()

DATA_DIR = r"C:\Users\ASUS\atlas_project\backend-python\divisional_level\data"
META_PATH = os.path.join(DATA_DIR, "other_gdrive_meta.json")
SERVICE_ACCOUNT_JSON = os.path.join(os.path.dirname(os.path.abspath(__file__)), "gdrive_service_account.json")

def utc_now_iso():
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat() + "Z"

def load_meta(meta_path):
    if not os.path.exists(meta_path):
        print(f"[ERROR] Meta file not found: {meta_path}")
        return []
    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
            if isinstance(meta, dict) and "files" in meta:
                return meta["files"]
            return meta
    except Exception as e:
        print(f"[DOWNLOAD][META LOAD ERROR] {e}")
        return []

# PATCH: JSONSchema sesuai struktur meta master yang sudah ada (tidak menambah data baru)
META_ENTRY_SCHEMA = {
    "type": "object",
    "properties": {
        "name": {"type": "string"},
        "id": {"type": ["string", "null"]},
        "gdrive_exists": {"type": ["boolean", "null"]},
        "local_exists": {"type": ["boolean", "null"]},
        "gdrive_md5": {"type": ["string", "null"]},
        "local_md5": {"type": ["string", "null"]},
        "gdrive_modified": {"type": ["string", "null"]},
        "gdrive_size": {"type": ["integer", "null"]},
        "local_size": {"type": ["integer", "null"]},
        "last_sync_time": {"type": ["string", "null"]},
        "status_file": {"type": ["string", "null"]},
        "status_process": {"type": ["string", "null"]},
        "history": {"type": "array"},
        "sha256": {"type": ["string", "null"]},
        "local_modified": {"type": ["string", "null"]},
        "row_count": {"type": ["integer", "null"]},
        "column_count": {"type": ["integer", "null"]},
        "gdrive_mime_type": {"type": ["string", "null"]},
        "process_duration": {"type": ["number", "null"]},
        "created_at": {"type": ["string", "null"]},
        "file_type": {"type": ["string", "null"]},
        "source_modified": {"type": ["string", "null"]},
        "record_count": {"type": ["integer", "null"]},
        "row_start": {"type": ["integer", "null"]},
        "row_end": {"type": ["integer", "null"]},
        "upstream_source": {"type": ["string", "null"]},
        "pipeline_job_id": {"type": ["string", "null"]},
        "operator": {"type": ["string", "null"]},
        "tags": {"type": ["array", "null"]},
        "confidentiality": {"type": ["string", "null"]},
        "data_owner": {"type": ["string", "null"]},
        "process_end_time": {"type": ["string", "null"]},
        "cleaning_status": {"type": ["string", "null"]},
        "schema_check": {"type": ["string", "null"]},
        "schema": {"type": ["object", "null"]},
        "null_stats": {"type": ["object", "null"]},
        "cleaning_log": {"type": ["array", "null"]},
        "file_role": {"type": ["string", "null"]},
        "last_status_change": {"type": ["string", "null"]},
        "parquet_outputs": {"type": ["array", "null"]},
        "sql_errors": {"type": ["array", "null"]},
        "duckdb_log_content": {"type": ["string", "null"]},
        "sql_table_info": {"type": ["object", "null"]},
        "derived_file_count": {"type": ["integer", "null"]},
        "jsonschema_verified": {"type": ["boolean", "null"]},
    },
    "required": ["name", "status_file"],
    "additionalProperties": True
}

def save_meta_agentic(meta_path, new_meta_entries, summary=None):
    old_meta = {}
    if os.path.exists(meta_path):
        with open(meta_path, "r", encoding="utf-8") as f:
            try:
                old_meta = json.load(f)
            except Exception:
                old_meta = {}
    old_files = {e["name"]: e for e in old_meta.get("files", [])}
    new_files = {e["name"]: e for e in new_meta_entries}
    merged_files = {**old_files, **new_files}
    meta_clean = list(merged_files.values())
    meta_clean.sort(key=lambda x: x.get("name", ""))

    # PATCH: Validasi setiap entry dengan jsonschema sebelum save (tidak menambah data baru, hanya memeriksa data yang ada)
    for entry in meta_clean:
        try:
            jsonschema_validate(instance=entry, schema=META_ENTRY_SCHEMA)
            entry["jsonschema_verified"] = True
        except JsonSchemaValidationError as e:
            print(f"[META][VALIDATION][ERROR] Entry {entry.get('name')}: {e}")
            entry["jsonschema_verified"] = False
            raise

    # Tambahkan derived_file_count untuk setiap main file
    # Main file: file_role == "main file"
    main_files = [e for e in meta_clean if e.get("file_role") == "main file"]
    for main_entry in main_files:
        derived_files = [
            e for e in meta_clean
            if e.get("file_role") == "derived file" and e.get("source_sql_file") == main_entry["name"]
        ]
        main_entry["derived_file_count"] = len(derived_files)

    meta_to_save = {"files": meta_clean}
    if summary is not None:
        meta_to_save["summary"] = summary
    elif "summary" in old_meta:
        meta_to_save["summary"] = old_meta["summary"]
    for k, v in old_meta.items():
        if k not in meta_to_save and k != "files":
            meta_to_save[k] = v
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta_to_save, f, indent=2, ensure_ascii=False)
    print(f"[META][AGENTIC][REWRITE] Meta clean rewritten: {meta_path} ({len(meta_clean)} entries)")
    return meta_clean

def update_meta_entry(meta, fname, new_info: dict):
    updated = False
    for entry in meta:
        if entry.get("name") == fname:
            entry.update(new_info)
            updated = True
            break
    return updated

def build_gdrive_service(service_account_json_path):
    SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
    creds = service_account.Credentials.from_service_account_file(service_account_json_path, scopes=SCOPES)
    return build('drive', 'v3', credentials=creds)

def download_file(service, file_id, download_path):
    request = service.files().get_media(fileId=file_id)
    with open(download_path, "wb") as f:
        downloader = MediaIoBaseDownload(f, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
    print(f"[DOWNLOAD] Downloaded (ID={file_id}) as: {download_path}")

def detect_schema_from_raw(df):
    return {col: str(dtype) for col, dtype in zip(df.columns, df.dtypes)}

def get_null_stats(df):
    return {col: int(df[col].isnull().sum()) for col in df.columns}

def auto_cast_string_columns(df):
    for col in df.select_dtypes(include="object").columns:
        sample = df[col].dropna()
        if len(sample) > 0 and sample.apply(lambda x: isinstance(x, str)).mean() > 0.8:
            df[col] = df[col].astype("string")
    return df

def strip_and_cast_df_agentic(df, schema: dict):
    status_msgs = []
    detailed_log = []
    cleaning_status = "success"
    type_changed = False
    cast_failures = {}
    for col in df.columns:
        try:
            df[col] = df[col].map(lambda v: v.strip() if isinstance(v, str) else v)
            detailed_log.append(f"[strip] {col}: success")
        except Exception as e:
            cleaning_status = "partial"
            detailed_log.append(f"[strip] {col}: failed ({e})")
    for col, dtype in schema.items():
        if col not in df.columns:
            cleaning_status = "partial"
            detailed_log.append(f"[cast] {col}: column missing in data")
            continue
        orig_dtype = str(df[col].dtype)
        try:
            if dtype.startswith("int"):
                df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')
            elif dtype.startswith("float"):
                df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')
            elif dtype in ["object", "string"]:
                df[col] = df[col].astype('string')
            new_dtype = str(df[col].dtype)
            if orig_dtype != new_dtype:
                type_changed = True
                detailed_log.append(f"[cast] {col}: {orig_dtype} -> {new_dtype}")
            else:
                detailed_log.append(f"[cast] {col}: type unchanged ({new_dtype})")
        except Exception as e:
            cleaning_status = "failed"
            cast_failures[col] = str(e)
            detailed_log.append(f"[cast] {col}: failed to cast to {dtype} ({e})")
    null_stats = get_null_stats(df)
    final_schema = detect_schema_from_raw(df)
    cleaning_status_str = "; ".join(status_msgs)
    return df, cleaning_status_str, "match", null_stats, detailed_log, final_schema

def clean_sql_for_duckdb(sql: str) -> str:
    sql = re.sub(
        r"DO\s+\$\$.*?END\s*\$\$;", "", sql, flags=re.IGNORECASE | re.DOTALL
    )
    sql = re.sub(
        r"\b(public|stg|dwh|dm)\.", "", sql, flags=re.IGNORECASE
    )
    sql = re.sub(r"\bint4\b", "INTEGER", sql, flags=re.IGNORECASE)
    sql = re.sub(r"\bnumeric\([^)]+\)", "DOUBLE", sql, flags=re.IGNORECASE)
    sql = re.sub(r"\bnumeric\b", "DOUBLE", sql, flags=re.IGNORECASE)
    sql = re.sub(r"\bserial\b", "INTEGER", sql, flags=re.IGNORECASE)
    sql = re.sub(r"\bvarchar\(\d+\)", "VARCHAR", sql, flags=re.IGNORECASE)
    sql = re.sub(r"--[^\n]*", "", sql)
    sql = re.sub(r"\n\s*\n", "\n", sql)
    return sql.strip()

def robust_sql_to_parquet(
    source_path, dest_dir, schema=None, update_schema_callback=None,
    meta_entry=None, meta_file_entries=None, write_log_file=False
):
    cleaning_log = []
    parquet_files = []
    parquet_relative_names = []
    sql_errors = []
    log_filename = os.path.join(dest_dir, f"{Path(source_path).stem}_duckdb.log")
    try:
        with open(source_path, "r", encoding="utf-8") as f:
            sql_raw = f.read()
        sql = clean_sql_for_duckdb(sql_raw)

        def filter_supported_statements(sql: str):
            import sqlparse
            stmts = sqlparse.split(sql)
            allowed = []
            for stmt in stmts:
                s = stmt.strip()
                if s == "":
                    cleaning_log.append(f"[autoclean][skip-empty]")
                    continue
                allowed.append(stmt)
            return allowed

        statements = filter_supported_statements(sql)
        con = duckdb.connect()
        for idx, stmt in enumerate(statements):
            try:
                con.execute(stmt)
                cleaning_log.append(f"[exec] Statement {idx+1} executed OK.")
            except Exception as e:
                cleaning_log.append(f"[exec][error] Statement {idx+1} failed: {e}")
                continue

        tables = [t[0] for t in con.execute("SHOW TABLES").fetchall()]
        for t in tables:
            try:
                df = con.execute(f"SELECT * FROM {t}").df()
                if df.shape[0] == 0:
                    cleaning_log.append(f"[skip] Table {t} has no rows, skip parquet write.")
                    continue
                # STRIP & CLEANING PATCH HERE:
                df, cleaning_status, schema_check, null_stats, detailed_log, final_schema = strip_and_cast_df_agentic(
                    df, detect_schema_from_raw(df)
                )
                parquet_path = os.path.join(dest_dir, f"{t}.parquet")
                df.to_parquet(parquet_path, index=False, compression="snappy", engine="pyarrow")
                cleaning_log.append(f"[parquet] {t} → {parquet_path} ({len(df)} rows, {len(df.columns)} columns)")
                sql_table_info = {
                    "table": t,
                    "row_count": len(df),
                    "column_count": len(df.columns)
                }
                pq_entry = {
                    "name": f"{t}.parquet",
                    "id": f"{t}.parquet",
                    "file_type": "application/parquet",
                    "file_role": "derived file",
                    "source_sql_file": os.path.basename(source_path),
                    "status_file": "new file",
                    "local_exists": True,
                    "local_size": os.path.getsize(parquet_path) if os.path.exists(parquet_path) else 0,
                    "local_modified": datetime.utcfromtimestamp(os.path.getmtime(parquet_path)).replace(microsecond=0).isoformat() + "Z" if os.path.exists(parquet_path) else None,
                    "data_owner": meta_entry.get("data_owner") if meta_entry else None,
                    "confidentiality": meta_entry.get("confidentiality") if meta_entry else None,
                    "tags": meta_entry.get("tags") if meta_entry else None,
                    "created_at": meta_entry.get("created_at") if meta_entry else None,
                    "operator": meta_entry.get("operator") if meta_entry else None,
                    "upstream_source": meta_entry.get("upstream_source") if meta_entry else None,
                    "pipeline_job_id": meta_entry.get("pipeline_job_id") if meta_entry else None,
                    "schema": final_schema,
                    "null_stats": null_stats,
                    "sql_table_info": sql_table_info,
                    "cleaning_log": detailed_log,
                    "cleaning_status": cleaning_status,
                    "schema_check": schema_check,
                }
                idx = next((i for i, e in enumerate(meta_file_entries) if e["name"] == pq_entry["name"]), None)
                if idx is not None:
                    meta_file_entries[idx] = pq_entry
                else:
                    meta_file_entries.append(pq_entry)
            except Exception as e:
                cleaning_log.append(f"[table][error] {t}: {e}")
                continue

        if meta_entry is not None:
            meta_entry["parquet_outputs"] = [f"{t}.parquet" for t in tables]
            meta_entry["sql_errors"] = sql_errors
            meta_entry["duckdb_log_content"] = "\n".join(cleaning_log)
            if tables:
                meta_entry["file_role"] = "both file"
            else:
                meta_entry["file_role"] = "main file"
        if write_log_file:
            with open(log_filename, "w", encoding="utf-8") as flog:
                flog.write("\n".join(cleaning_log))

        if not tables:
            if update_schema_callback:
                update_schema_callback({}, "failed: No table with data", "mismatch", {}, cleaning_log)
            return False
        return True
    except Exception as e:
        cleaning_log.append(f"[sql][fatal] Unhandled error: {e}")
        if update_schema_callback:
            update_schema_callback({}, "failed", "mismatch", {}, cleaning_log)
        if meta_entry is not None:
            meta_entry["parquet_outputs"] = []
            meta_entry["sql_errors"] = sql_errors
            meta_entry["duckdb_log_content"] = "\n".join(cleaning_log)
        if write_log_file:
            with open(log_filename, "w", encoding="utf-8") as flog:
                flog.write("\n".join(cleaning_log))
        return False

def convert_anything_to_parquet_snappy(
    source_path, dest_path, schema=None, update_schema_callback=None,
    meta_entry=None, meta_file_entries=None, write_log_file=False
):
    ext = Path(source_path).suffix.lower()
    if ext == ".sql":
        dest_dir = os.path.dirname(dest_path)
        os.makedirs(dest_dir, exist_ok=True)
        return robust_sql_to_parquet(
            source_path, dest_dir, schema, update_schema_callback,
            meta_entry=meta_entry, meta_file_entries=meta_file_entries,
            write_log_file=write_log_file
        )
    detected_schema = None
    try:
        doc = docling.Doc.from_file(source_path)
        if hasattr(doc, "to_df"):
            df = doc.to_df()
        elif hasattr(doc, "tables") and doc.tables:
            df = doc.tables[0].to_df()
        else:
            df = pd.DataFrame({"text": doc.text.splitlines()})
        detected_schema = detect_schema_from_raw(df)
        if schema is None:
            schema = detected_schema
        for col, dtype in (schema or {}).items():
            if dtype == "string" and col in df.columns:
                df[col] = df[col].astype("string")
        df = auto_cast_string_columns(df)
        df, cleaning_status, schema_check, null_stats, detailed_log, final_schema = strip_and_cast_df_agentic(df, schema)
        df.to_parquet(dest_path, index=False, compression="snappy", engine="pyarrow")
        print(f"[CONVERT][DOCLING] Converted {source_path} → {dest_path} via docling")
        if update_schema_callback:
            update_schema_callback(final_schema, cleaning_status, schema_check, null_stats, detailed_log)
        if meta_entry is not None:
            meta_entry["file_role"] = "main file"
        return True
    except Exception as e:
        print(f"[CONVERT][DOCLING][WARN] Failed docling parse {source_path}: {e}")
    try:
        ext = Path(source_path).suffix.lower()
        if ext == ".parquet":
            df = pd.read_parquet(source_path, engine="pyarrow")
            detected_schema = detect_schema_from_raw(df)
        elif ext in [".csv", ".tsv"]:
            sep = "," if ext == ".csv" else "\t"
            df = pd.read_csv(source_path, sep=sep)
            detected_schema = detect_schema_from_raw(df)
        elif ext in [".xls", ".xlsx"]:
            df = pd.read_excel(source_path)
            detected_schema = detect_schema_from_raw(df)
        elif ext == ".json":
            df = pd.read_json(source_path)
            detected_schema = detect_schema_from_raw(df)
        elif ext == ".feather":
            df = pd.read_feather(source_path)
            detected_schema = detect_schema_from_raw(df)
        else:
            try:
                duckdb.sql(
                    f"COPY (SELECT * FROM read_csv_auto('{source_path}')) TO '{dest_path}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');"
                )
                print(f"[CONVERT][FALLBACK][DUCKDB] Converted {source_path} → {dest_path}")
                if update_schema_callback:
                    df = pd.read_parquet(dest_path, engine="pyarrow")
                    for col, dtype in (schema or {}).items():
                        if dtype == "string" and col in df.columns:
                            df[col] = df[col].astype("string")
                    df = auto_cast_string_columns(df)
                    final_schema = detect_schema_from_raw(df)
                    _, cleaning_status, schema_check, null_stats, detailed_log, _ = strip_and_cast_df_agentic(df, schema if schema else final_schema)
                    update_schema_callback(final_schema, cleaning_status, schema_check, null_stats, detailed_log)
                    df.to_parquet(dest_path, index=False, compression="snappy", engine="pyarrow")
                if meta_entry is not None:
                    meta_entry["file_role"] = "main file"
                return True
            except Exception as e:
                print(f"[CONVERT][FALLBACK][DUCKDB] {e}")
                if update_schema_callback:
                    update_schema_callback({}, f"failed: duckdb conversion error: {e}", "mismatch", {}, [f"duckdb error: {e}"])
                return False
        if schema is None:
            schema = detected_schema
        for col, dtype in (schema or {}).items():
            if dtype == "string" and col in df.columns:
                df[col] = df[col].astype("string")
        df = auto_cast_string_columns(df)
        df, cleaning_status, schema_check, null_stats, detailed_log, final_schema = strip_and_cast_df_agentic(df, schema)
        df.to_parquet(dest_path, index=False, compression="snappy", engine="pyarrow")
        print(f"[CONVERT][SNAPPY] Converted {source_path} → {dest_path}")
        if update_schema_callback:
            update_schema_callback(final_schema, cleaning_status, schema_check, null_stats, detailed_log)
        if meta_entry is not None:
            meta_entry["file_role"] = "main file"
        return True
    except Exception as e:
        print(f"[CONVERT][ERROR] Failed to convert {source_path} to snappy parquet: {e}")
        if update_schema_callback:
            update_schema_callback({}, f"failed: {e}", "mismatch", {}, [f"fatal conversion error: {e}"])
        return False

def download_missing_files(
    data_dir=DATA_DIR,
    meta_path=META_PATH,
    service_account_json_path=SERVICE_ACCOUNT_JSON,
):
    if not os.path.exists(meta_path):
        print(f"[ERROR] Meta file not found: {meta_path}")
        return {"status": "no-meta", "downloaded": [], "skipped": []}
    with open(meta_path, "r", encoding="utf-8") as f:
        meta_dict = json.load(f)
    meta_file_entries = meta_dict["files"] if "files" in meta_dict else []
    summary = meta_dict.get("summary", None)
    if not meta_file_entries:
        print("[DOWNLOAD] Meta file kosong/tidak ditemukan. Tidak ada file yang diunduh.")
        return {"status": "no-meta", "downloaded": [], "skipped": []}
    service = build_gdrive_service(service_account_json_path)
    downloaded, skipped, converted = [], [], []
    now = utc_now_iso()
    to_download = [entry for entry in meta_file_entries if str(entry.get("status_process", "")).lower() == "download"]
    if not to_download:
        print("[DOWNLOAD] Tidak ada file dengan status_process 'download'. Tidak ada file didownload.")
        return {
            "status": "skipped-none-to-download",
            "downloaded": [],
            "skipped": [entry.get("name") for entry in meta_file_entries if str(entry.get("status_process", "")).lower() != "download"]
        }
    for entry in to_download:
        fname = entry.get("name")
        file_id = entry.get("id")
        schema = entry.get("schema", None)
        file_type = (entry.get("file_type") or "").lower()
        gdrive_mime_type = (entry.get("gdrive_mime_type") or "").lower()
        is_sql = (
            (fname and fname.lower().endswith('.sql')) or
            (file_type in ('text/x-sql', 'application/sql')) or
            (gdrive_mime_type in ('text/x-sql', 'application/sql'))
        )
        use_ext = ".sql" if is_sql else ""
        def update_schema_callback(new_schema, cleaning_status, schema_check, null_stats, detailed_log):
            entry["cleaning_status"] = cleaning_status
            entry["schema_check"] = schema_check
            entry["schema"] = new_schema
            entry["null_stats"] = null_stats
            entry["cleaning_log"] = detailed_log
        with tempfile.NamedTemporaryFile(delete=False, suffix=use_ext) as tmpf:
            tmp_path = tmpf.name
        try:
            download_file(service, file_id, tmp_path)
            dest_path = os.path.join(data_dir, fname)
            ok = convert_anything_to_parquet_snappy(
                tmp_path, dest_path, schema, update_schema_callback=update_schema_callback,
                meta_entry=entry, meta_file_entries=meta_file_entries, write_log_file=False
            )
            if ok:
                stat = os.stat(dest_path)
                new_info = {
                    "local_exists": True,
                    "local_modified": datetime.utcfromtimestamp(stat.st_mtime).replace(microsecond=0).isoformat() + "Z",
                    "local_size": stat.st_size,
                    "status_process": "process",
                    "last_status_change": now,
                    "process_end_time": now,
                }
                update_meta_entry(meta_file_entries, fname, new_info)
                if "parquet_outputs" in entry:
                    converted.extend(entry["parquet_outputs"])
                else:
                    converted.append(os.path.basename(dest_path))
                downloaded.append(fname)
            else:
                skipped.append(fname)
        except Exception as e:
            print(f"[DOWNLOAD][ERROR] Failed to download/convert {fname} (id={file_id}): {e}")
            skipped.append(fname)
        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass
    for entry in meta_file_entries:
        if "id" not in entry or not entry["id"]:
            entry["id"] = entry.get("name", "")
    # Tambahkan derived_file_count untuk setiap main file
    main_files = [e for e in meta_file_entries if e.get("file_role") == "main file"]
    for main_entry in main_files:
        derived_files = [
            e for e in meta_file_entries
            if e.get("file_role") == "derived file" and e.get("source_sql_file") == main_entry["name"]
        ]
        main_entry["derived_file_count"] = len(derived_files)
    save_meta_agentic(meta_path, meta_file_entries, summary=summary)
    print(f"[DOWNLOAD] Selesai. Downloaded: {downloaded}, Skipped: {skipped}, Converted: {converted}")
    return {
        "status": "success",
        "downloaded": downloaded,
        "converted": converted,
        "skipped": skipped,
    }

@router.post("/trigger_download_missing_files")
async def trigger_download_missing_files_endpoint(request: Request):
    try:
        if request.headers.get("content-type", "").startswith("application/json"):
            _ = await request.json()
    except Exception:
        pass
    result = download_missing_files()
    return result

if __name__ == "__main__":
    result = download_missing_files()
    print(result)

4. data_cleaner.py:

import os
import json
import sys
import traceback
from fastapi import FastAPI
from fastapi.responses import JSONResponse

# REVISI: Path sesuai permintaan
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = r"C:\Users\ASUS\atlas_project\backend-python\divisional_level\data"
DATA_ETL_DIR = r"C:\Users\ASUS\atlas_project\backend-python\divisional_level\data_etl"
EDA_RESULT_DIR = r"C:\Users\ASUS\atlas_project\backend-python\divisional_level\data_eda\eda_result"
EDA_LOG_DIR = r"C:\Users\ASUS\atlas_project\backend-python\divisional_level\data_eda\data_log"
META_FILE = os.path.join(DATA_DIR, "other_gdrive_meta.json")

app = FastAPI()

def load_meta(meta_path):
    if not os.path.exists(meta_path):
        print(f"[ERROR] Meta file not found: {meta_path}")
        return []
    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            meta_data = json.load(f)
        if isinstance(meta_data, dict) and "files" in meta_data:
            return meta_data["files"]
        return meta_data
    except Exception as e:
        print(f"[ERROR] Failed to load meta master: {e}")
        traceback.print_exc()
        return []

def save_meta(meta_path, meta_list):
    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            raw = json.load(f)
        if isinstance(raw, dict) and "files" in raw:
            to_save = {"files": meta_list}
        else:
            to_save = meta_list
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(to_save, f, indent=2, ensure_ascii=False)
        print(f"[INFO] Meta master updated: {meta_path}")
    except Exception as e:
        print(f"[ERROR] Failed to save meta master: {e}")
        traceback.print_exc()

def build_id_to_name_map(meta_list):
    id_to_name = {}
    for entry in meta_list:
        file_id = entry.get("id") or entry.get("file_id")
        file_name = entry.get("name") or entry.get("saved_name")
        if file_id and file_name:
            id_to_name[file_id] = file_name
    return id_to_name

def scan_files_by_id(folder, id_to_name):
    results = {}
    if not os.path.exists(folder):
        return results
    for fname in os.listdir(folder):
        fpath = os.path.join(folder, fname)
        if not os.path.isfile(fpath):
            continue
        for file_id, meta_name in id_to_name.items():
            # Cocokkan file parquet utama atau file_progress_batching.json
            if (
                fname == meta_name
                or fname.startswith(meta_name)
                or file_id in fname
                or (fname == "file_progress_batching.json" and file_id in fname)
            ):
                results.setdefault(file_id, []).append(fpath)
    return results

def scan_etl_files_by_id(folder, id_to_name):
    """
    Scan for parquet and file_progress_batching.json in folder based on file_id or meta_name.
    """
    results = {}
    if not os.path.exists(folder):
        return results
    for fname in os.listdir(folder):
        fpath = os.path.join(folder, fname)
        if not os.path.isfile(fpath):
            continue
        for file_id, meta_name in id_to_name.items():
            # Cocokkan file parquet (misal churn.parquet), atau file_progress_batching.json
            if (
                fname.endswith(".parquet") and (
                    fname == meta_name or fname.startswith(meta_name) or file_id in fname
                )
            ) or (
                fname == "file_progress_batching.json"
            ):
                results.setdefault(file_id, []).append(fpath)
    return results

def scan_eda_files_by_id(folder, id_to_name):
    """
    Scan for result/log files (json/pdf/html and their meta) in folder based on file_id or meta_name.
    """
    results = {}
    if not os.path.exists(folder):
        return results
    for fname in os.listdir(folder):
        fpath = os.path.join(folder, fname)
        if not os.path.isfile(fpath):
            continue
        for file_id, meta_name in id_to_name.items():
            # Cocokkan semua file yang mengandung id atau nama file parquet di namanya
            if file_id and (file_id in fname):
                results.setdefault(file_id, []).append(fpath)
                continue
            if meta_name and meta_name.split(".")[0] in fname:  # ignore ext for matching
                results.setdefault(file_id, []).append(fpath)
    return results

def find_files_to_delete(meta_list, data_dir, data_etl_dir, eda_result_dir, eda_log_dir):
    files_to_delete = []
    id_to_name = build_id_to_name_map(meta_list)
    data_files = scan_files_by_id(data_dir, id_to_name)
    etl_files = scan_etl_files_by_id(data_etl_dir, id_to_name)
    result_files = scan_eda_files_by_id(eda_result_dir, id_to_name)
    log_files = scan_eda_files_by_id(eda_log_dir, id_to_name)

    for entry in meta_list:
        file_id = entry.get("id") or entry.get("file_id")
        status_file = entry.get("status_file", "").lower()
        if file_id and status_file == "deleted":
            for fpath in data_files.get(file_id, []):
                files_to_delete.append(fpath)
            for fpath in etl_files.get(file_id, []):
                files_to_delete.append(fpath)
            for fpath in result_files.get(file_id, []):
                files_to_delete.append(fpath)
            for fpath in log_files.get(file_id, []):
                files_to_delete.append(fpath)
    # Unikkan (bisa jadi ada duplikat)
    return list(sorted(set(files_to_delete)))

def find_orphan_etl_files(data_etl_dir, meta_list):
    """
    Temukan file parquet atau file_progress_batching.json di data_etl_dir yang tidak punya entry di meta master (meta_list).
    Return: list of orphan file paths.
    """
    if not os.path.exists(data_etl_dir):
        return []
    # Kumpulkan semua nama file parquet valid dari meta
    meta_names = set()
    for entry in meta_list:
        name = entry.get("name") or entry.get("saved_name")
        if name:
            meta_names.add(name)
    orphan_files = []
    for fname in os.listdir(data_etl_dir):
        if fname.endswith(".parquet") and fname not in meta_names:
            orphan_files.append(os.path.join(data_etl_dir, fname))
        elif fname == "file_progress_batching.json":
            # orphan jika tidak ada file parquet yang valid di meta
            orphan_files.append(os.path.join(data_etl_dir, fname))
    return orphan_files

def delete_files(files):
    deleted = []
    failed = []
    for fpath in files:
        try:
            if os.path.exists(fpath) and os.path.isfile(fpath):
                os.remove(fpath)
                print(f"[INFO] Deleted file: {fpath}")
                deleted.append(fpath)
        except Exception as e:
            print(f"[ERROR] Failed to delete {fpath}: {e}")
            traceback.print_exc()
            failed.append(fpath)
    return deleted, failed

def remove_deleted_entries_from_meta(meta_list):
    # Hapus entry dengan status_file == "deleted"
    new_meta = [entry for entry in meta_list if entry.get("status_file", "").lower() != "deleted"]
    n_removed = len(meta_list) - len(new_meta)
    return new_meta, n_removed

def cleaner_run():
    print("[DATA_CLEANER] Loading meta master...")
    meta_list = load_meta(META_FILE)
    if not meta_list:
        print("[DATA_CLEANER] No meta entries found. Aborting.")
        return {"deleted": [], "failed": [], "meta_removed": 0, "message": "No meta entries found."}

    print("[DATA_CLEANER] Finding files to delete...")
    files_to_delete = find_files_to_delete(
        meta_list,
        DATA_DIR,
        DATA_ETL_DIR,
        EDA_RESULT_DIR,
        EDA_LOG_DIR
    )

    # --- Tambahan: Cari orphan di data_etl (tidak ada di meta master) ---
    print("[DATA_CLEANER] Finding orphan etl files...")
    orphan_etl_files = find_orphan_etl_files(DATA_ETL_DIR, meta_list)
    if orphan_etl_files:
        print(f"[DATA_CLEANER] Will also remove orphan etl files: {orphan_etl_files}")
        files_to_delete.extend(orphan_etl_files)
    # -----------------------------------------------------------------------

    if not files_to_delete:
        print("[DATA_CLEANER] No files to delete found (status_file: deleted or orphan etl).")
        new_meta, n_removed = remove_deleted_entries_from_meta(meta_list)
        if n_removed > 0:
            save_meta(META_FILE, new_meta)
        return {"deleted": [], "failed": [], "meta_removed": n_removed, "message": "No files to delete, but meta cleaned."}

    print(f"[DATA_CLEANER] Deleting {len(files_to_delete)} file(s)...")
    deleted, failed = delete_files(files_to_delete)

    new_meta, n_removed = remove_deleted_entries_from_meta(meta_list)
    if n_removed > 0:
        save_meta(META_FILE, new_meta)
        print(f"[DATA_CLEANER] Removed {n_removed} deleted entries from meta master.")

    print(f"[DATA_CLEANER] Deleted {len(deleted)} file(s), failed to delete {len(failed)} file(s).")
    return {
        "deleted": deleted,
        "failed": failed,
        "meta_removed": n_removed,
        "message": f"Deleted {len(deleted)} file(s), failed to delete {len(failed)} file(s), removed {n_removed} meta entries."
    }

@app.post("/trigger_data_cleaner")
def trigger_data_cleaner():
    """
    Endpoint untuk trigger pembersihan file yang status_file-nya 'deleted' dan orphan etl.
    Bisa dipanggil via HTTP POST dari n8n.
    """
    try:
        result = cleaner_run()
        return JSONResponse(content=result)
    except Exception as e:
        traceback.print_exc()
        return JSONResponse(content={"error": str(e)}, status_code=500)

if __name__ == "__main__":
    try:
        result = cleaner_run()
        print(json.dumps(result, indent=2))
    except Exception as e:
        print(f"[FATAL ERROR] {e}")
        traceback.print_exc()
        sys.exit(2)

5. batch_agentic.py:

import os
import json
import hashlib
import time
from typing import Any, Dict, List, Optional, Tuple
import pandas as pd
from datetime import datetime, timezone

# --- CONFIG AREA: Centralized from config.py ---
try:
    from divisional_level import config
    batch_cfg = config.get_batch_config()
    BATCH_SIZE = batch_cfg["BATCH_SIZE"]
    START_BATCH_SIZE = batch_cfg["START_BATCH_SIZE"]
    MAX_BATCH_SIZE = batch_cfg["MAX_BATCH_SIZE"]
    MIN_BATCH_SIZE = batch_cfg["MIN_BATCH_SIZE"]
    TIME_FAST = batch_cfg["TIME_FAST"]
    TIME_SLOW = batch_cfg["TIME_SLOW"]
    print(f"[DEBUG] BATCH_SIZE loaded: {BATCH_SIZE}")
except ImportError as e:
    print("[ERROR] config.py not found or import error:", e)
    BATCH_SIZE = 1000
    START_BATCH_SIZE = 1000
    MAX_BATCH_SIZE = 2000
    MIN_BATCH_SIZE = 1000
    TIME_FAST = 2.0
    TIME_SLOW = 10.0

ALLOWED_STATUS = {"active", "new file", "change", "done"}

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = r"C:\Users\ASUS\atlas_project\backend-python\divisional_level\data"
DATA_SCHEMA_DIR = os.path.join(BASE_DIR, "data_schema")
DATA_ETL_DIR = r"C:\Users\ASUS\atlas_project\backend-python\divisional_level\data_etl"
os.makedirs(DATA_ETL_DIR, exist_ok=True)
PROGRESS_FILE = os.path.join(DATA_DIR, "file_progress.json")
META_FILE = os.path.join(DATA_DIR, "other_gdrive_meta.json")
PROGRESS_BATCHING_FILE = os.path.join(DATA_ETL_DIR, "file_progress_batching.json")

from filelock import FileLock

def load_meta(meta_path: str):
    if not os.path.exists(meta_path):
        return []
    with open(meta_path, "r", encoding="utf-8") as f:
        meta = json.load(f)
    if isinstance(meta, dict) and "files" in meta:
        meta = meta["files"]
    return meta

def get_row_count_from_file(fpath):
    try:
        if os.path.exists(fpath) and fpath.lower().endswith(".parquet"):
            return len(pd.read_parquet(fpath))
        elif os.path.exists(fpath) and fpath.lower().endswith(".csv"):
            return len(pd.read_csv(fpath))
    except Exception as e:
        print(f"[SYNC][FALLBACK] Failed to read {fpath}: {e}")
    return 0

def get_row_start_end_from_file(fpath):
    try:
        if os.path.exists(fpath):
            if fpath.lower().endswith('.parquet'):
                df = pd.read_parquet(fpath)
            elif fpath.lower().endswith('.csv'):
                df = pd.read_csv(fpath)
            else:
                return None, None
            if not df.empty:
                return int(df.index[0]), int(df.index[-1])
            else:
                return None, None
        else:
            return None, None
    except Exception as e:
        print(f"[ROW_START_END][ERROR] {fpath}: {e}")
        return None, None

def push_to_data_etl(file_name, data_dir=DATA_DIR, data_etl_dir=DATA_ETL_DIR, processed=None):
    import shutil
    src = os.path.join(data_dir, file_name)
    dst = os.path.join(data_etl_dir, file_name)
    try:
        if os.path.exists(src):
            if processed is not None and file_name.lower().endswith('.parquet'):
                df = pd.read_parquet(src)
                max_idx = min(processed, len(df))
                if max_idx > 0:
                    df.iloc[:max_idx].to_parquet(dst)
                else:
                    pd.DataFrame().to_parquet(dst)
            else:
                shutil.copy2(src, dst)
        if os.path.exists(dst):
            return os.path.getsize(dst)
        else:
            return 0
    except Exception as e:
        print(f"[DATA_ETL][ERROR] Failed to push {file_name}: {e}")
        return 0

def push_to_data_schema(file_name, data_dir=DATA_DIR, data_schema_dir=DATA_SCHEMA_DIR, processed=None):
    return push_to_data_etl(file_name, data_dir=DATA_DIR, data_etl_dir=DATA_ETL_DIR, processed=processed)

def clone_progress_to_batching():
    try:
        src = PROGRESS_FILE
        dst = PROGRESS_BATCHING_FILE
        with open(src, "r", encoding="utf-8") as fsrc:
            content = fsrc.read()
        with open(dst, "w", encoding="utf-8") as fdst:
            fdst.write(content)
        print(f"[CLONE] file_progress.json cloned to file_progress_batching.json in data_etl")
    except Exception as e:
        print(f"[CLONE][ERROR] Failed to clone progress file: {e}")

def preprocess_batch(batch_rows):
    obj_cols = batch_rows.select_dtypes(include='object').columns
    batch_rows[obj_cols] = batch_rows[obj_cols].apply(lambda x: x.str.strip())
    batch_rows.columns = [c.strip().lower() for c in batch_rows.columns]
    batch_rows = batch_rows.dropna(axis=1, how='all')
    batch_rows = batch_rows.dropna(axis=0, how='all')
    return batch_rows

class ProgressManager:
    def __init__(self, data_dir: Optional[str] = None, progress_file: Optional[str] = None, meta_master_file: Optional[str] = None):
        self.data_dir = data_dir or DATA_DIR
        self.progress_file = progress_file or PROGRESS_FILE
        self.meta_master_file = meta_master_file or META_FILE
        self.lock = FileLock(self.progress_file + ".lock")
        print(f"[ProgressManager] Initialized with data_dir={self.data_dir}, progress_file={self.progress_file}, meta_master_file={self.meta_master_file}")

    def _load_json(self, path):
        if not os.path.exists(path):
            return {} if path.endswith("file_progress.json") else []
        try:
            if path.endswith("_gdrive_meta.json"):
                meta = load_meta(path)
                return [e if isinstance(e, dict) else {"name": e} for e in meta]
            else:
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception as e:
            print(f"[ProgressManager][ERROR] Failed to load {path}: {e}")
            return {} if path.endswith("file_progress.json") else []

    def _save_json(self, path, data):
        if isinstance(data, dict):
            for fname, entry in data.items():
                processed = entry.get("processed", 0)
                total = entry.get("total", 0)
                if total and total > 0:
                    percent_processed = round(100 * processed / total, 2)
                else:
                    percent_processed = 0.0
                entry["percent_processed"] = percent_processed

        tmp_path = path + ".tmp"
        try:
            with open(tmp_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                f.flush()
                os.fsync(f.fileno())
            os.replace(tmp_path, path)
            count = len(data) if isinstance(data, (dict, list)) else "?"
            print(f"[ProgressManager][REWRITE] Overwritten atomically: {path} ({count} entries)")
        except Exception as e:
            print(f"[ProgressManager][ERROR] Failed to write {path}: {e}")
            if os.path.exists(tmp_path):
                try:
                    os.remove(tmp_path)
                except Exception:
                    pass

    def save_progress(self, progress):
        with self.lock:
            self._save_json(self.progress_file, progress)
        clone_progress_to_batching()

    def _meta_map(self):
        meta = self._load_json(self.meta_master_file)
        result = {}
        for entry in meta:
            fname = entry.get("name")
            status_file = entry.get("status_file")
            file_id = entry.get("id")
            if fname and status_file and file_id:
                result[fname] = {"status_file": status_file, "id": file_id, "meta": entry}
        return result

    def get_total_items(self, file_name):
        meta = self._load_json(self.meta_master_file)
        for entry in meta:
            fname = entry.get("name")
            status = entry.get("status_file", "")
            if fname == file_name and status in ALLOWED_STATUS:
                return entry.get("record_count") or entry.get("total_items") or get_row_count_from_file(os.path.join(self.data_dir, file_name))
        return get_row_count_from_file(os.path.join(self.data_dir, file_name))

    def load_progress(self):
        with self.lock:
            return self._load_json(self.progress_file)

    def update_progress(self, file_name, processed, **kwargs):
        meta_map = self._meta_map()
        info = meta_map.get(file_name)
        if not info or info["status_file"] not in ALLOWED_STATUS:
            print(f"[ProgressManager][WARNING] {file_name} is not active/new/change in meta. SKIP update_progress.")
            return
        row_start, row_end = get_row_start_end_from_file(os.path.join(self.data_dir, file_name))
        with self.lock:
            progress = self._load_json(self.progress_file)
            prev_entry = progress.get(file_name, {
                "file_id": info["id"],
                "processed": 0,
                "last_batch": 0,
                "retry_count": 0,
                "last_batch_size": None,
                "next_batch_size": None,
                "last_error_type": None,
                "consecutive_success_count": 0,
                "last_batch_time": 0.0,
                "size_batch": 0
            })
            meta_entry = info["meta"]
            total = kwargs.get(
                "total",
                meta_entry.get("record_count")
                or meta_entry.get("total_items")
                or get_row_count_from_file(os.path.join(self.data_dir, file_name))
            )
            entry = dict(prev_entry)
            entry["total"] = total
            entry["file_id"] = info["id"]
            prev_processed = prev_entry.get("processed", 0)
            now_utc = datetime.now(timezone.utc).isoformat()
            if "process_start_time_utc" not in entry or not entry.get("process_start_time_utc"):
                entry["process_start_time_utc"] = now_utc
            if processed >= total and total > 0:
                entry["process_end_time_utc"] = now_utc
            elif processed > prev_processed:
                entry["process_end_time_utc"] = now_utc
            if processed > prev_processed:
                entry["processed"] = min(processed, total)
                entry["last_batch"] = prev_entry.get("last_batch", 0) + 1
                entry["last_batch_size"] = kwargs.get("last_batch_size", entry.get("last_batch_size"))
                entry["next_batch_size"] = kwargs.get("next_batch_size", entry.get("next_batch_size"))
            else:
                entry["processed"] = prev_processed
                entry["last_batch"] = prev_entry.get("last_batch", 0)
                entry["last_batch_size"] = prev_entry.get("last_batch_size", None)
                entry["next_batch_size"] = prev_entry.get("next_batch_size", None)
            for key, value in kwargs.items():
                if key not in ("total", "last_batch_size", "next_batch_size"):
                    entry[key] = value
            entry["row_start"] = row_start
            entry["row_end"] = row_end
            if entry["processed"] >= entry["total"] and entry["total"] > 0:
                entry["status_batch"] = "done"
            else:
                entry["status_batch"] = "pending"
            data_matching = None
            try:
                fpath = os.path.join(self.data_dir, file_name)
                if os.path.exists(fpath):
                    if fpath.lower().endswith('.parquet'):
                        df = pd.read_parquet(fpath)
                    elif fpath.lower().endswith('.csv'):
                        df = pd.read_csv(fpath)
                    else:
                        df = None
                    if df is not None and entry["processed"] > 0 and entry["processed"] <= len(df):
                        last_idx = entry["processed"] - 1
                        actual_row_idx = df.index[last_idx]
                        diff = abs(actual_row_idx - last_idx)
                        data_matching = max(0, 100 - diff)
            except Exception as e:
                print(f"[DATA_MATCHING][ERROR] {file_name}: {e}")
                data_matching = None
            entry["data_matching"] = data_matching
            last_batch_time_arg = kwargs.get("last_batch_time", None)
            if last_batch_time_arg is not None:
                entry["last_batch_time"] = float(last_batch_time_arg)
            else:
                entry["last_batch_time"] = prev_entry.get("last_batch_time", 0.0)
            size_batch_arg = kwargs.get("size_batch", None)
            if size_batch_arg is not None:
                entry["size_batch"] = size_batch_arg
            else:
                f_schema = os.path.join(DATA_SCHEMA_DIR, file_name)
                entry["size_batch"] = os.path.getsize(f_schema) if os.path.exists(f_schema) else 0

            processed_now = entry.get("processed", 0)
            total_now = entry.get("total", 0)
            if total_now and total_now > 0:
                percent_processed = round(100 * processed_now / total_now, 2)
            else:
                percent_processed = 0.0
            entry["percent_processed"] = percent_processed

            progress[file_name] = entry
            self.save_progress(progress)
            print(f"[ProgressManager][UPDATE] Progress updated for {file_name}: {entry}")

    def get_file_progress(self, file_name):
        row_start, row_end = get_row_start_end_from_file(os.path.join(self.data_dir, file_name))
        with self.lock:
            progress = self._load_json(self.progress_file)
            entry = progress.get(file_name, {}).copy()
        total = entry.get("total", None)
        if total is None or total == 0:
            total = self.get_total_items(file_name)
        entry["total"] = total
        entry["is_estimated"] = False
        processed = entry.get("processed", 0)
        if total and total > 0:
            entry["percent_processed"] = round((processed / total) * 100, 2)
        else:
            entry["percent_processed"] = 0.0
        entry["row_start"] = row_start
        entry["row_end"] = row_end
        if processed >= total and total > 0:
            entry["status_batch"] = "done"
        else:
            entry["status_batch"] = "pending"
        data_matching = entry.get("data_matching")
        if data_matching is None:
            try:
                fpath = os.path.join(self.data_dir, file_name)
                if os.path.exists(fpath):
                    if fpath.lower().endswith('.parquet'):
                        df = pd.read_parquet(fpath)
                    elif fpath.lower().endswith('.csv'):
                        df = pd.read_csv(fpath)
                    else:
                        df = None
                    if df is not None and entry["processed"] > 0 and entry["processed"] <= len(df):
                        last_idx = entry["processed"] - 1
                        actual_row_idx = df.index[last_idx]
                        diff = abs(actual_row_idx - last_idx)
                        data_matching = max(0, 100 - diff)
            except Exception as e:
                print(f"[DATA_MATCHING][ERROR-read] {file_name}: {e}")
                data_matching = None
            entry["data_matching"] = data_matching
        if "last_batch_time" not in entry:
            entry["last_batch_time"] = 0.0
        if "size_batch" not in entry:
            f_schema = os.path.join(DATA_SCHEMA_DIR, file_name)
            entry["size_batch"] = os.path.getsize(f_schema) if os.path.exists(f_schema) else 0
        return entry

    def get_all_progress(self):
        with self.lock:
            progress = self._load_json(self.progress_file)
        all_result = {}
        for fname, entry in progress.items():
            rec = entry.copy()
            total = rec.get("total", None)
            if total is None or total == 0:
                total = self.get_total_items(fname)
            rec["total"] = total
            rec["is_estimated"] = False
            processed = rec.get("processed", 0)
            if total and total > 0:
                rec["percent_processed"] = round((processed / total) * 100, 2)
            else:
                rec["percent_processed"] = 0.0
            row_start, row_end = get_row_start_end_from_file(os.path.join(self.data_dir, fname))
            rec["row_start"] = row_start
            rec["row_end"] = row_end
            if rec["processed"] > rec["total"]:
                rec["processed"] = rec["total"]
            if rec["processed"] >= rec["total"] and rec["total"] > 0:
                rec["status_batch"] = "done"
            else:
                rec["status_batch"] = "pending"
            data_matching = rec.get("data_matching")
            if data_matching is None:
                try:
                    fpath = os.path.join(self.data_dir, fname)
                    if os.path.exists(fpath):
                        if fpath.lower().endswith('.parquet'):
                            df = pd.read_parquet(fpath)
                        elif fpath.lower().endswith('.csv'):
                            df = pd.read_csv(fpath)
                        else:
                            df = None
                        if df is not None and rec["processed"] > 0 and rec["processed"] <= len(df):
                            last_idx = rec["processed"] - 1
                            actual_row_idx = df.index[last_idx]
                            diff = abs(actual_row_idx - last_idx)
                            data_matching = max(0, 100 - diff)
                except Exception as e:
                    print(f"[DATA_MATCHING][ERROR-all] {fname}: {e}")
                    data_matching = None
                rec["data_matching"] = data_matching
            if "last_batch_time" not in rec:
                rec["last_batch_time"] = 0.0
            if "size_batch" not in rec:
                f_schema = os.path.join(DATA_SCHEMA_DIR, fname)
                rec["size_batch"] = os.path.getsize(f_schema) if os.path.exists(f_schema) else 0
            all_result[fname] = rec
        return all_result

    def remove_file_progress(self, file_name):
        with self.lock:
            progress = self._load_json(self.progress_file)
            if file_name in progress:
                del progress[file_name]
                self.save_progress(progress)
                print(f"[ProgressManager][REMOVE] Progress entry removed for {file_name}")

    def reset_progress(self, file_name):
        meta_map = self._meta_map()
        info = meta_map.get(file_name)
        if not info:
            print(f"[ProgressManager][RESET] {file_name} not found in meta. SKIP reset.")
            return
        row_start, row_end = get_row_start_end_from_file(os.path.join(self.data_dir, file_name))
        with self.lock:
            progress = self._load_json(self.progress_file)
            meta_entry = info["meta"]
            total_items = meta_entry.get("record_count") or meta_entry.get("total_items") or get_row_count_from_file(os.path.join(self.data_dir, file_name))
            progress[file_name] = {
                "file_id": info["id"],
                "processed": 0,
                "last_batch": 0,
                "retry_count": 0,
                "last_batch_size": None,
                "next_batch_size": None,
                "last_error_type": None,
                "consecutive_success_count": 0,
                "total": total_items,
                "status_batch": "pending",
                "row_start": row_start,
                "row_end": row_end,
                "data_matching": None,
                "process_start_time_utc": None,
                "process_end_time_utc": None,
                "last_batch_time": 0.0,
                "size_batch": 0,
                "percent_processed": 0.0
            }
            self.save_progress(progress)
            print(f"[ProgressManager][RESET] Progress reset for {file_name}")

    def sync_progress(self):
        print("[ProgressManager][SYNC] Sync file_progress.json with meta status_file...")
        meta = self._load_json(self.meta_master_file)
        with self.lock:
            progress = self._load_json(self.progress_file)
            meta_map = {}
            for entry in meta:
                fname = entry.get("name")
                fid = entry.get("id")
                status = entry.get("status_file")
                if not fname or not fid or not status:
                    continue
                # PATCH: only if file exists in data folder
                fpath = os.path.join(self.data_dir, fname)
                if not os.path.isfile(fpath):
                    continue
                meta_map[fname] = {"status_file": status, "id": fid, "meta": entry}
            for fname, v in meta_map.items():
                status = v["status_file"]
                fid = v["id"]
                meta_entry = v["meta"]
                row_start, row_end = get_row_start_end_from_file(os.path.join(self.data_dir, fname))
                if status == "new file":
                    if fname not in progress:
                        total_items = meta_entry.get("record_count") or meta_entry.get("total_items") or get_row_count_from_file(os.path.join(self.data_dir, fname))
                        progress[fname] = {
                            "file_id": fid,
                            "processed": 0,
                            "last_batch": 0,
                            "retry_count": 0,
                            "last_batch_size": None,
                            "next_batch_size": None,
                            "last_error_type": None,
                            "consecutive_success_count": 0,
                            "total": total_items,
                            "status_batch": "pending",
                            "row_start": row_start,
                            "row_end": row_end,
                            "data_matching": None,
                            "process_start_time_utc": None,
                            "process_end_time_utc": None,
                            "last_batch_time": 0.0,
                            "size_batch": 0,
                            "percent_processed": 0.0
                        }
                elif status == "change":
                    total_items = meta_entry.get("record_count") or meta_entry.get("total_items") or get_row_count_from_file(os.path.join(self.data_dir, fname))
                    progress[fname] = {
                        "file_id": fid,
                        "processed": 0,
                        "last_batch": 0,
                        "retry_count": 0,
                        "last_batch_size": None,
                        "next_batch_size": None,
                        "last_error_type": None,
                        "consecutive_success_count": 0,
                        "total": total_items,
                        "status_batch": "pending",
                        "row_start": row_start,
                        "row_end": row_end,
                        "data_matching": None,
                        "process_start_time_utc": None,
                        "process_end_time_utc": None,
                        "last_batch_time": 0.0,
                        "size_batch": 0,
                        "percent_processed": 0.0
                    }
                elif status == "active":
                    if fname not in progress:
                        total_items = meta_entry.get("record_count") or meta_entry.get("total_items") or get_row_count_from_file(os.path.join(self.data_dir, fname))
                        progress[fname] = {
                            "file_id": fid,
                            "processed": 0,
                            "last_batch": 0,
                            "retry_count": 0,
                            "last_batch_size": None,
                            "next_batch_size": None,
                            "last_error_type": None,
                            "consecutive_success_count": 0,
                            "total": total_items,
                            "status_batch": "pending",
                            "row_start": row_start,
                            "row_end": row_end,
                            "data_matching": None,
                            "process_start_time_utc": None,
                            "process_end_time_utc": None,
                            "last_batch_time": 0.0,
                            "size_batch": 0,
                            "percent_processed": 0.0
                        }
                elif status == "deleted":
                    if fname in progress:
                        del progress[fname]
            to_remove = []
            for fname in progress:
                if fname not in meta_map or meta_map[fname]["status_file"] == "deleted":
                    to_remove.append(fname)
            for fname in to_remove:
                del progress[fname]
            self.save_progress(progress)
            print(f"[ProgressManager][SYNC] Sync selesai. Entry: {len(progress)}")

def calc_sha256_from_file(path):
    try:
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        print(f"[ERROR] calc_sha256_from_file failed: {e}")
        return ""

def get_file_info_from_meta(pm: ProgressManager, status_allowed=ALLOWED_STATUS):
    meta = load_meta(pm.meta_master_file)
    info_list = []
    for entry in meta:
        fname = (entry.get("name") or entry.get("saved_name") or "").strip()
        status = entry.get("status_file", entry.get("status", ""))
        if not fname or not fname.lower().endswith('.parquet') or fname.endswith('.parquet.meta.json'):
            continue
        if status not in status_allowed:
            continue
        fpath = os.path.join(pm.data_dir, fname)
        if not os.path.isfile(fpath):
            continue
        try:
            size_bytes = os.path.getsize(fpath)
        except Exception as e:
            print(f"[ERROR] get_file_info os.path.getsize failed for {fname}: {e}")
            size_bytes = 0
        sha256 = calc_sha256_from_file(fpath)
        try:
            modified_time = str(os.path.getmtime(fpath))
        except Exception as e:
            print(f"[ERROR] get_file_info os.path.getmtime failed for {fname}: {e}")
            modified_time = ""
        progress_entry = pm.get_file_progress(fname)
        total_items = progress_entry.get("total", 0)
        info_list.append({
            "file": fname,
            "size_bytes": size_bytes,
            "total_items": total_items,
            "sha256": sha256,
            "modified_time": modified_time
        })
    return info_list

def build_file_status(file_info, progress):
    status_list = []
    for info in file_info:
        fname = info["file"]
        entry = progress.get(fname, {})
        processed = entry.get("processed", 0) if isinstance(entry, dict) else 0
        last_batch_size = entry.get("last_batch_size")
        next_batch_size = entry.get("next_batch_size")
        status_batch = entry.get("status_batch", "pending")
        if not isinstance(next_batch_size, int) or next_batch_size <= 0:
            if not isinstance(last_batch_size, int) or last_batch_size <= 0:
                batch_size = START_BATCH_SIZE
            else:
                batch_size = last_batch_size
        else:
            batch_size = next_batch_size
        status_list.append({
            "name": fname,
            "size": info["total_items"],
            "total": info["total_items"],
            "processed": processed,
            "last_batch_size": last_batch_size,
            "next_batch_size": next_batch_size,
            "status_batch": status_batch,
            "batch_size": batch_size
        })
    return status_list

def batch_distributor(file_info, progress) -> List[Tuple[str, int, int, int, str]]:
    file_status_list = build_file_status(file_info, progress)
    pending_list = [x for x in file_status_list if x.get("status_batch", "pending") != "done"]
    pending_list = sorted(pending_list, key=lambda x: x["total"])
    allocations = []

    for idx, status in enumerate(pending_list):
        fname = status["name"]
        processed = status["processed"]
        total = status["total"]
        remain = max(total - processed, 0)
        batch_size = status["batch_size"]
        actual_batch_size = min(batch_size, remain)
        if actual_batch_size <= 0:
            continue
        start_idx = processed
        end_idx = processed + actual_batch_size
        reason = f"Agentic batch size for {fname}: {actual_batch_size} (batch_size: {batch_size})"
        allocations.append((fname, start_idx, end_idx, actual_batch_size, reason))
    return allocations

def simulate_batch_process(file_name, start_idx, end_idx):
    if "error" in file_name and (end_idx - start_idx) > 1000:
        return False, "timeout"
    return True, None

def process_file_batch(pm: ProgressManager, file_name, start_idx, end_idx, batch_size, progress_entry):
    t0 = time.time()
    try:
        fpath = os.path.join(pm.data_dir, file_name)
        total_items = progress_entry.get("total", 0)
        try:
            df = pd.read_parquet(fpath)
            batch_rows = df.iloc[start_idx:end_idx]
            batch_count = len(batch_rows)
        except Exception as e:
            print(f"[BATCH][ERROR] Gagal baca parquet {file_name}: {e}")
            batch_rows = None
            batch_count = 0

        if batch_rows is None or batch_count == 0:
            print(f"[BATCH][SKIP] No data loaded for {file_name} idx {start_idx}-{end_idx}, skip progress update.")
            pm.update_progress(
                file_name,
                processed=progress_entry.get("processed", 0),
                cleaned_status="failed"
            )
            return False, batch_size

        try:
            batch_rows = preprocess_batch(batch_rows)
            cleaning_success = True
        except Exception as e:
            print(f"[CLEAN][ERROR] Preprocessing failed for {file_name}: {e}")
            cleaning_success = False

        success, error_type = simulate_batch_process(file_name, start_idx, end_idx)
        t1 = time.time()
        elapsed = t1 - t0

        prev_last_batch_time = progress_entry.get("last_batch_time", 0.0) or 0.0
        new_last_batch_time = prev_last_batch_time

        if success and cleaning_success:
            new_last_batch_time += elapsed
            consecutive_success_count = progress_entry.get("consecutive_success_count", 0) + 1
            if not isinstance(batch_size, int) or batch_size <= 0:
                batch_size = BATCH_SIZE
            if elapsed < TIME_FAST:
                next_batch_size = min(max(batch_size * 2, BATCH_SIZE), MAX_BATCH_SIZE)
            elif elapsed < TIME_SLOW:
                next_batch_size = max(batch_size, BATCH_SIZE)
            else:
                next_batch_size = max(batch_size // 2, BATCH_SIZE)
                consecutive_success_count = 0

            processed_now = start_idx + batch_count
            size_batch = push_to_data_schema(file_name, pm.data_dir, DATA_SCHEMA_DIR, processed=processed_now)

            pm.update_progress(
                file_name,
                processed=processed_now,
                last_batch=progress_entry.get("last_batch", 0) + 1,
                last_batch_size=batch_count,
                next_batch_size=next_batch_size,
                retry_count=0,
                last_error_type=None,
                consecutive_success_count=consecutive_success_count,
                last_batch_time=new_last_batch_time,
                size_batch=size_batch,
                cleaned_status="done"
            )
            print(f"[PROGRESS] {file_name}: processed={processed_now}, total={total_items}, elapsed={elapsed:.2f}s, batch_count={batch_count}, next_batch_size={next_batch_size}, cum_batch_time={new_last_batch_time:.4f}, size_batch={size_batch}, cleaned_status=done")
            return True, next_batch_size
        else:
            next_batch_size = max(batch_size // 2, BATCH_SIZE)
            processed_now = progress_entry.get("processed", 0)
            size_batch = push_to_data_schema(file_name, pm.data_dir, DATA_SCHEMA_DIR, processed=processed_now)
            pm.update_progress(
                file_name,
                processed=processed_now,
                last_batch=progress_entry.get("last_batch", 0),
                last_batch_size=progress_entry.get("last_batch_size", 0),
                next_batch_size=next_batch_size,
                retry_count=1,
                last_error_type=error_type,
                consecutive_success_count=0,
                last_batch_time=prev_last_batch_time,
                size_batch=size_batch,
                cleaned_status="failed"
            )
            print(f"[PROGRESS][FAILED] {file_name}: processed={processed_now}, total={total_items}, last_error={error_type}, next_batch_size={next_batch_size}, cum_batch_time={prev_last_batch_time:.4f}, size_batch={size_batch}, cleaned_status=failed")
            return False, next_batch_size
    except Exception as e:
        print(f"[ERROR][EXCEPTION] {file_name} idx {start_idx}-{end_idx} exception: {e}")
        try:
            next_batch_size = max(batch_size // 2, BATCH_SIZE)
            prev_last_batch_time = progress_entry.get("last_batch_time", 0.0) or 0.0
            processed_now = progress_entry.get("processed", 0)
            size_batch = push_to_data_schema(file_name, pm.data_dir, DATA_SCHEMA_DIR, processed=processed_now)
            pm.update_progress(
                file_name,
                processed=processed_now,
                last_batch=progress_entry.get("last_batch", 0),
                last_batch_size=progress_entry.get("last_batch_size", 0),
                next_batch_size=next_batch_size,
                retry_count=1,
                last_error_type="exception",
                consecutive_success_count=0,
                last_batch_time=prev_last_batch_time,
                size_batch=size_batch,
                cleaned_status="failed"
            )
        except Exception as e2:
            print(f"[ERROR] Failed to update progress after exception: {e2}")
        return False, batch_size

def all_files_done(progress, file_info):
    file_names = {info["file"] for info in file_info}
    for fname, entry in progress.items():
        if fname not in file_names:
            continue
        total = entry.get("total", 0)
        processed = entry.get("processed", 0)
        if total > 0 and processed < total:
            return False
    return True

def run_batch_agentic():
    pm = ProgressManager(DATA_DIR)
    pm.sync_progress()
    file_info = get_file_info_from_meta(pm)
    progress = pm.get_all_progress()
    allocations = batch_distributor(file_info, progress)

    finished_files = []
    for fname, entry in progress.items():
        total = entry.get("total", 0)
        processed = entry.get("processed", 0)
        status_batch = entry.get("status_batch", "pending")
        if total > 0 and processed >= total and status_batch == "done":
            finished_files.append(fname)
    if finished_files:
        print("\n[INFO] File berikut SUDAH SELESAI dan tidak ikut batch berikutnya (dipertahankan di progress untuk audit):")
        for fname in finished_files:
            print(f"    - {fname}")

    if not allocations:
        print("[INFO] Tidak ada batch unfinished; semua file selesai.")
        return

    print("\nBatch allocation this trigger (agentic, adaptive, per unfinished file):")
    for fname, start_idx, end_idx, batch_size, reason in allocations:
        print(f"  {fname}: {start_idx}-{end_idx} (batch_size={batch_size}) -> {reason}")

    for fname, start_idx, end_idx, batch_size, reason in allocations:
        entry = progress.get(fname, {})
        ok, next_batch_size = process_file_batch(pm, fname, start_idx, end_idx, batch_size, entry)
        entry = pm.get_file_progress(fname)
        print(f"[INFO] Setelah batch {fname}: {entry}")

    print("[INFO] Batch agentic selesai.")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Agentic Batch Pipeline")
    parser.add_argument("--show-progress", action="store_true", help="Show current progress")
    parser.add_argument("--run-batch", action="store_true", help="Run agentic batch trigger")
    parser.add_argument("--reset", type=str, default=None, help="Reset progress for file")
    parser.add_argument("--remove", type=str, default=None, help="Remove file progress entry")
    parser.add_argument("--sync-progress", action="store_true", help="Sync file_progress.json with meta status_file")
    args = parser.parse_args()

    pm = ProgressManager(DATA_DIR)
    if args.show_progress:
        print(json.dumps(pm.get_all_progress(), indent=2, ensure_ascii=False))
    elif args.run_batch:
        run_batch_agentic()
    elif args.reset:
        pm.reset_progress(args.reset)
        print(f"[INFO] Reset progress for {args.reset}")
    elif args.remove:
        pm.remove_file_progress(args.remove)
        print(f"[INFO] Removed progress entry for {args.remove}")
    elif args.sync_progress:
        pm.sync_progress()
        print("[INFO] Synced file_progress.json with meta status_file.")
    else:
        parser.print_help()

6. monitoring_process.py:

import os
import json
import datetime
import hashlib
import pandas as pd
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from filelock import FileLock
import time

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
SCHEMA_DIR = os.path.join(BASE_DIR, "data_schema")
PROGRESS_FILE = os.path.join(DATA_DIR, "file_progress.json")
META_FILE = os.path.join(DATA_DIR, "other_gdrive_meta.json")

router = APIRouter()

def robust_load_json_with_lock(path, default=None, max_retry=5, sleep_retry=0.1, double_check=True):
    """
    Read a JSON file under filelock, with retry on JSONDecodeError or incomplete write.
    If double_check is True, will read file twice and only return if both results are equal.
    Ensures only valid, fully-written JSON is loaded and stable.
    """
    lock = FileLock(path + ".lock")
    for attempt in range(max_retry):
        with lock:
            try:
                with open(path, "r", encoding="utf-8") as f:
                    data1 = f.read()
                # If file is empty, treat as default
                if not data1.strip():
                    return default if default is not None else {}
                obj1 = json.loads(data1)
                if not double_check:
                    return obj1
                # Second read (stabilize)
                time.sleep(sleep_retry)
                with open(path, "r", encoding="utf-8") as f:
                    data2 = f.read()
                obj2 = json.loads(data2)
                if obj1 == obj2:
                    return obj1
                # If not equal, try again
            except json.JSONDecodeError:
                if attempt == max_retry - 1:
                    raise
                time.sleep(sleep_retry)
            except FileNotFoundError:
                return default if default is not None else {}
            except Exception:
                if attempt == max_retry - 1:
                    raise
                time.sleep(sleep_retry)
    return default if default is not None else {}

def load_json(path, default=None):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default if default is not None else {}

def load_meta(meta_file):
    meta = load_json(meta_file, default={})
    if isinstance(meta, dict):
        return meta.get("files", [])
    if isinstance(meta, list):
        return meta
    return []

def find_meta_by_id(meta_list):
    meta_by_id = {}
    for entry in meta_list:
        file_id = entry.get("id") or entry.get("file_id")
        if file_id:
            meta_by_id[file_id] = entry
    return meta_by_id

def calc_sha256_from_file(path):
    try:
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception:
        return ""

def get_file_sample_df(fpath, ext, n_sample=5):
    try:
        if ext == ".parquet":
            df = pd.read_parquet(fpath, engine="pyarrow")
        else:
            df = pd.read_csv(fpath, nrows=max(n_sample*2, 40))
        if len(df) <= n_sample:
            return df
        samples = []
        n_head = max(2, n_sample // 4)
        samples.append(df.head(n_head))
        n_tail = max(2, n_sample // 4)
        samples.append(df.tail(n_tail))
        mid = len(df) // 2
        n_mid = max(1, n_sample // 4)
        start_mid = max(0, mid - n_mid // 2)
        samples.append(df.iloc[start_mid:start_mid + n_mid])
        rem = n_sample - sum(len(s) for s in samples)
        if rem > 0:
            idx_taken = set()
            for s in samples:
                idx_taken.update(s.index)
            left = list(set(df.index) - idx_taken)
            if left:
                import numpy as np
                rand_idx = np.random.choice(left, min(rem, len(left)), replace=False)
                samples.append(df.loc[rand_idx])
        df_sample = pd.concat(samples).drop_duplicates().reset_index(drop=True)
        return df_sample.iloc[:n_sample]
    except Exception:
        return pd.DataFrame()

def parse_iso_to_local(dt_str):
    if not dt_str or not isinstance(dt_str, str):
        return ""
    try:
        import pandas as pd
        dt_str = dt_str.replace('+00:00Z', 'Z').replace('Z+00:00', 'Z')
        if dt_str.endswith('Z'):
            dt_str = dt_str[:-1] + '+00:00'
        dt_utc = pd.to_datetime(dt_str, utc=True)
        dt_local = dt_utc.tz_convert(None).to_pydatetime().astimezone()
        return dt_local.replace(microsecond=0).isoformat()
    except Exception:
        return ""

def get_schema_row_count(file_name):
    fpath = os.path.join(SCHEMA_DIR, file_name)
    if not os.path.exists(fpath):
        return 0
    try:
        if file_name.lower().endswith('.parquet'):
            return len(pd.read_parquet(fpath))
        elif file_name.lower().endswith('.csv'):
            return len(pd.read_csv(fpath))
        else:
            return 0
    except Exception:
        return 0

def generate_quality_score_explanation(progress_entry, percent_processed):
    explanations = []
    status = progress_entry.get("status_batch", "unknown")
    if status == "pending":
        explanations.append("File status is pending (not processed completely).")
    if percent_processed < 100:
        explanations.append(f"Only {percent_processed}% of the file has been processed.")
    if not explanations:
        explanations.append("No significant quality issues detected in process.")
    return "; ".join(explanations)

def quick_sampling_metrics(processed, total):
    if total == 0 or processed == 0:
        return {
            "completeness_score": 0,
            "confidence_score": 0,
            "confidence_interval": None,
            "was_excluded": True
        }
    percent = processed / total if total else 0
    completeness = round(percent * 100, 2)
    confidence = round(min(max(percent, 0), 1) * 100, 2)
    ci = [max(0, int(completeness-5)), min(100, int(completeness+5))] if completeness else None
    was_excluded = completeness < 50 or confidence < 50
    return {
        "completeness_score": completeness,
        "confidence_score": confidence,
        "confidence_interval": ci,
        "was_excluded": was_excluded
    }

def clean_json(obj):
    try:
        return json.loads(json.dumps(obj, default=str))
    except Exception:
        return obj

def get_file_record_monitoring():
    # Tunggu 1 detik sebelum membaca file_progress.json
    time.sleep(1)
    progress_map = robust_load_json_with_lock(PROGRESS_FILE, {}, max_retry=8, sleep_retry=0.05, double_check=True)
    meta_list = load_meta(META_FILE)
    meta_by_id = find_meta_by_id(meta_list)
    files_monitoring_result = []

    for fname, progress_entry in progress_map.items():
        file_id = progress_entry.get("file_id")
        meta_entry = meta_by_id.get(file_id, {})
        file_name = meta_entry.get("name") or fname
        fpath = os.path.join(DATA_DIR, file_name)
        ext = os.path.splitext(file_name)[-1].lower()

        mimeType = "application/parquet" if ext == ".parquet" else "text/csv"
        size_bytes = os.path.getsize(fpath) if os.path.exists(fpath) else None
        modified_utc = datetime.datetime.utcfromtimestamp(os.path.getmtime(fpath)).isoformat() if os.path.exists(fpath) else None
        sha256 = calc_sha256_from_file(fpath) if os.path.exists(fpath) else ""

        df_sample = get_file_sample_df(fpath, ext, n_sample=5) if os.path.exists(fpath) else pd.DataFrame()
        row_count = len(df_sample)
        column_count = df_sample.shape[1] if not df_sample.empty else 0

        processed_items = progress_entry.get("processed", 0)
        total_items = progress_entry.get("total", row_count)
        percent_processed = round(100 * processed_items / total_items, 2) if total_items else 0.0
        status = progress_entry.get("status_batch", "unknown")
        last_status_change = sha256 if sha256 else modified_utc
        process_start_time_utc = progress_entry.get("process_start_time_utc", None)
        process_end_time_utc = progress_entry.get("process_end_time_utc", None)
        process_start_time_local = parse_iso_to_local(process_start_time_utc) if process_start_time_utc else ""
        process_end_time_local = parse_iso_to_local(process_end_time_utc) if process_end_time_utc else ""
        last_batch_time = progress_entry.get("last_batch_time", 0) or 0
        n_batch = progress_entry.get("last_batch", 1) or 1
        if last_batch_time is not None and n_batch > 0:
            total_processing_time = round(float(last_batch_time) / n_batch, 4)
        else:
            total_processing_time = 0
        is_time_fallback = False
        last_batch = progress_entry.get("last_batch", None)
        last_batch_size = progress_entry.get("last_batch_size", None)
        retry_count = progress_entry.get("retry_count", 0)
        consecutive_success_count = progress_entry.get("consecutive_success_count", 0)
        quality_score_explanation = generate_quality_score_explanation(progress_entry, percent_processed)
        size_batch = progress_entry.get("size_batch", 0)
        total_items_schema = get_schema_row_count(file_name)

        sampling_metrics = quick_sampling_metrics(processed_items, total_items)
        completeness_score = sampling_metrics.get("completeness_score")
        confidence_score = sampling_metrics.get("confidence_score")
        confidence_interval = sampling_metrics.get("confidence_interval")
        was_excluded = sampling_metrics.get("was_excluded")

        files_monitoring_result.append({
            "file_id": file_id,
            "file": file_name,
            "meta": {
                "mimeType": mimeType,
                "size_bytes": size_bytes,
                "modified_utc": modified_utc,
                "sha256": sha256
            },
            "progress": {
                "row_count": row_count,
                "column_count": column_count,
                "percent_processed": percent_processed,
                "processed_items": processed_items,
                "total_items": total_items,
                "status": status,
                "last_status_change": last_status_change,
                "process_time": {
                    "start_utc": process_start_time_utc,
                    "end_utc": process_end_time_utc,
                    "start_local": process_start_time_local,
                    "end_local": process_end_time_local,
                    "total_processing_time": total_processing_time,
                    "is_time_fallback": is_time_fallback
                },
                "last_batch": last_batch,
                "last_batch_size": last_batch_size,
                "retry_count": retry_count,
                "consecutive_success_count": consecutive_success_count,
                "quality_score_explanation": quality_score_explanation,
                "size_batch": size_batch,
                "total_items_schema": total_items_schema
            },
            "quality_metrics": {
                "completeness_score": completeness_score,
                "confidence_score": confidence_score,
                "confidence_interval": confidence_interval,
                "was_excluded": was_excluded
            }
        })
    return files_monitoring_result

@router.get("/monitoring_process")
def monitoring_process_endpoint():
    """
    Endpoint monitoring proses batch data.
    Data dinamis dari file_progress.json, data statis dari other_gdrive_meta.json dan file di disk.
    Output nested.
    Tunggu 1 detik sebelum membaca file_progress.json untuk memastikan file sudah selesai ditulis.
    Menggunakan filelock dan double-read otomatis agar isi file_progress.json benar-benar stabil dan valid.
    """
    return JSONResponse(content=clean_json({
        "files_monitoring_result": get_file_record_monitoring()
    }))

7. add_import_load_meta.py:

import os
import re

IMPORT_LINE = "from divisional_level.utils_gdrive import load_meta\n"

# REVISI: Ganti root_path dengan path target yang diinginkan
ROOT_PATH = r"C:\Users\ASUS\atlas_project\backend-python\divisional_level\data"

def has_import_line(lines):
    return any("from divisional_level.utils_gdrive import load_meta" in l for l in lines)

def is_python(filename):
    return filename.endswith(".py")

def needs_import(filename):
    with open(filename, encoding="utf-8") as f:
        code = f.read()
    return "load_meta(" in code and "from divisional_level.utils_gdrive import load_meta" not in code

def add_import(filename):
    with open(filename, encoding="utf-8") as f:
        lines = f.readlines()
    # cari line import terakhir
    insert_at = 0
    for i, line in enumerate(lines):
        if line.strip().startswith("import ") or line.strip().startswith("from "):
            insert_at = i + 1
    # tambahkan hanya jika belum ada
    if has_import_line(lines):
        return False
    lines.insert(insert_at, IMPORT_LINE)
    with open(filename, "w", encoding="utf-8") as f:
        f.writelines(lines)
    print(f"Added import to {filename}")
    return True

if __name__ == "__main__":
    for root, dirs, files in os.walk(ROOT_PATH):
        for file in files:
            if is_python(file):
                fname = os.path.join(root, file)
                try:
                    if needs_import(fname):
                        add_import(fname)
                except Exception as e:
                    print(f"Skip {fname}: {e}")

8. common_utils.py:

import os
import json
import math
import datetime
import pandas as pd

def safe(val, default=None):
    """
    Returns val unless it's empty (None, NaN, inf, 'null', etc), otherwise returns default.
    """
    if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
        return default
    if isinstance(val, str) and val.lower() in ("none", "null", ""):
        return default
    if val in [None, "", [], {}, "null", "None"]:
        return default
    return val

def clean_json(obj):
    """
    Recursively cleans objects for JSON serialization:
    - Converts NaN/inf to None
    - Converts datetime to isoformat string
    - Converts pd.Timestamp to isoformat string
    """
    if isinstance(obj, dict):
        return {str(k): clean_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_json(v) for v in obj]
    elif isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        return obj
    elif isinstance(obj, (datetime.datetime, datetime.date, pd.Timestamp)):
        return obj.isoformat()
    else:
        return obj

def calc_sha256_from_file(path):
    """
    Calculates SHA256 hash of a file at the given path.
    """
    import hashlib
    try:
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        print(f"[common_utils][ERROR] calc_sha256_from_file failed for path={path}, error={e}")
        return ""

def serialize_for_json(obj):
    """
    Converts datetime and pandas Timestamp objects to isoformat string for JSON serialization.
    """
    if isinstance(obj, (datetime.date, datetime.datetime)):
        return obj.isoformat()
    if isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    return str(obj)

9. config.py:

# config.py
# Centralized batch limit configuration for all modules

START_BATCH_SIZE = 1000
MAX_BATCH_SIZE = 10000000
MIN_BATCH_SIZE = 1000
TIME_FAST = 5
TIME_SLOW = 60
BATCH_SIZE = 1000  # for orchestrator or large batch operations

def get_batch_config():
    return {
        "START_BATCH_SIZE": START_BATCH_SIZE,
        "MAX_BATCH_SIZE": MAX_BATCH_SIZE,
        "MIN_BATCH_SIZE": MIN_BATCH_SIZE,
        "TIME_FAST": TIME_FAST,
        "TIME_SLOW": TIME_SLOW,
        "BATCH_SIZE": BATCH_SIZE,
    }

10. error_handler.py:

import os
import traceback
import datetime
import threading

class ErrorHandler:
    """
    ErrorHandler: Logging error, auto-retry, simpan stacktrace.
    Thread-safe dan bisa dipakai di orchestrator, batch, atau API.
    """
    def __init__(self, log_dir=None):
        try:
            if log_dir is None:
                log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "audit_logs")
            os.makedirs(log_dir, exist_ok=True)
            self.log_dir = log_dir
            self.log_file = os.path.join(log_dir, "error.log")
            self.lock = threading.Lock()
            print(f"[error_handler][DEBUG] ErrorHandler initialized with log_dir={self.log_dir}, log_file={self.log_file}")
        except Exception as e:
            print(f"[error_handler][HYBRID-FALLBACK][ERROR] Failed to initialize ErrorHandler: {e}")
            # Fallback: minimal attributes to avoid crash.
            self.log_dir = "."
            self.log_file = "error.log"
            self.lock = threading.Lock()

    def log_error(self, err, context=None, notify_callback=None):
        """
        Log error dengan stacktrace dan context.
        Optionally, trigger notifikasi via callback jika diberikan.
        """
        now = datetime.datetime.utcnow().isoformat()
        try:
            tb_str = "".join(traceback.format_exception(type(err), err, err.__traceback__))
        except Exception as e:
            tb_str = f"[HYBRID-FALLBACK][ERROR] Failed to format exception: {e}\n"
        log_entry = {
            "timestamp": now,
            "error": str(err),
            "context": context or "",
            "traceback": tb_str
        }
        line = f"{now} | ERROR | {context or ''}\n{tb_str}\n"
        try:
            with self.lock:
                try:
                    with open(self.log_file, "a", encoding="utf-8") as f:
                        f.write(line)
                except Exception as file_err:
                    print(f"[error_handler][HYBRID-FALLBACK][ERROR] Failed to write error log: {file_err}")
        except Exception as e:
            print(f"[error_handler][HYBRID-FALLBACK][ERROR] Locking error on log_error: {e}")
        print(f"[error_handler] Error logged: {err} | Context: {context}")
        print(f"[error_handler][DEBUG] Error log entry:\n{line}")
        # Optional: trigger notification
        if notify_callback:
            try:
                notify_callback(message=line, level="error", context=context)
            except Exception as notif_err:
                print(f"[error_handler][HYBRID-FALLBACK][ERROR] Failed to notify: {notif_err}")

    def log_info(self, msg):
        """Log info ke file dan print."""
        now = datetime.datetime.utcnow().isoformat()
        line = f"{now} | INFO  | {msg}\n"
        try:
            with self.lock:
                try:
                    with open(self.log_file, "a", encoding="utf-8") as f:
                        f.write(line)
                except Exception as file_err:
                    print(f"[error_handler][HYBRID-FALLBACK][ERROR] Failed to write info log: {file_err}")
        except Exception as e:
            print(f"[error_handler][HYBRID-FALLBACK][ERROR] Locking error on log_info: {e}")
        print(f"[error_handler] {msg}")
        print(f"[error_handler][DEBUG] Info log entry:\n{line}")

    def auto_retry(self, func, max_retries=3, context=None, notify_callback=None, *args, **kwargs):
        """
        Eksekusi func dengan auto-retry jika error. Return hasil func jika sukses, None jika gagal semua.
        """
        for attempt in range(1, max_retries + 1):
            try:
                print(f"[error_handler][DEBUG] Attempt {attempt} for {func.__name__}")
                return func(*args, **kwargs)
            except Exception as e:
                self.log_error(e, context=f"{context or func.__name__} [attempt {attempt}]", notify_callback=notify_callback)
                if attempt < max_retries:
                    self.log_info(f"Retrying {func.__name__} (attempt {attempt + 1}/{max_retries})")
        return None

    def get_recent_errors(self, n=20):
        """Ambil n error terakhir dari log."""
        if not os.path.exists(self.log_file):
            print(f"[error_handler][DEBUG] No error log file found: {self.log_file}")
            return []
        try:
            with self.lock:
                try:
                    with open(self.log_file, "r", encoding="utf-8") as f:
                        lines = f.readlines()
                except Exception as file_err:
                    print(f"[error_handler][HYBRID-FALLBACK][ERROR] Failed to read error log: {file_err}")
                    return []
        except Exception as e:
            print(f"[error_handler][HYBRID-FALLBACK][ERROR] Locking error on get_recent_errors: {e}")
            return []
        error_lines = [line for line in lines if "| ERROR |" in line]
        print(f"[error_handler][DEBUG] Found {len(error_lines)} error lines in log, returning last {n}")
        return error_lines[-n:] if error_lines else []

if __name__ == "__main__":
    # Contoh penggunaan
    handler = ErrorHandler()
    try:
        1 / 0
    except Exception as e:
        handler.log_error(e, context="Test ZeroDivisionError")
    handler.log_info("Sample info log")
    print("[error_handler] Recent errors:", handler.get_recent_errors())

11. notification_manager.py:

import os
import smtplib
import threading
from email.message import EmailMessage
import datetime

class NotificationManager:
    """
    NotificationManager: Kirim notifikasi ke email (atau channel lain).
    Bisa diintegrasikan dengan error_handler, orchestrator, dsb.
    """
    def __init__(self, email_config=None):
        """
        email_config: dict, contoh:
        {
            'smtp_host': 'smtp.gmail.com',
            'smtp_port': 587,
            'smtp_user': 'your_email@gmail.com',
            'smtp_pass': 'your_app_password',
            'from_email': 'your_email@gmail.com',
            'to_email': ['recipient1@gmail.com', 'recipient2@gmail.com'],
            'use_tls': True
        }
        """
        self.email_config = email_config or {}
        self.lock = threading.Lock()
        print(f"[notification_manager][DEBUG] NotificationManager initialized with config: {self.email_config}")

    def send_email(self, subject, message, html_message=None):
        """
        Kirim email notifikasi.
        """
        cfg = self.email_config
        print(f"[notification_manager][DEBUG] send_email called with subject: {subject}")
        if not all(k in cfg for k in ['smtp_host', 'smtp_port', 'smtp_user', 'smtp_pass', 'from_email', 'to_email']):
            print("[notification_manager] Email config incomplete, cannot send email.")
            print(f"[notification_manager][DEBUG] Current config: {cfg}")
            return False
        try:
            msg = EmailMessage()
            msg['Subject'] = subject
            msg['From'] = cfg['from_email']
            msg['To'] = ", ".join(cfg['to_email']) if isinstance(cfg['to_email'], list) else cfg['to_email']
            msg.set_content(message)
            if html_message:
                msg.add_alternative(html_message, subtype='html')

            with self.lock:
                print(f"[notification_manager][DEBUG] Sending email via SMTP: {cfg['smtp_host']}:{cfg['smtp_port']}")
                try:
                    with smtplib.SMTP(cfg['smtp_host'], cfg['smtp_port']) as smtp:
                        if cfg.get('use_tls', True):
                            smtp.starttls()
                            print("[notification_manager][DEBUG] TLS started.")
                        smtp.login(cfg['smtp_user'], cfg['smtp_pass'])
                        print("[notification_manager][DEBUG] SMTP login successful.")
                        smtp.send_message(msg)
                    print("[notification_manager] Email sent.")
                    return True
                except Exception as smtp_e:
                    print(f"[notification_manager][HYBRID-FALLBACK][ERROR] SMTP send failed: {smtp_e}")
                    return False
        except Exception as e:
            print(f"[notification_manager][HYBRID-FALLBACK][ERROR] Failed to construct/send email: {e}")
            print(f"[notification_manager][DEBUG] Exception info: {e}")
            return False

    def notify(self, message, level="info", context=None):
        """
        Fungsi notifikasi umum, bisa digunakan oleh error_handler, orchestrator, dsb.
        Extend untuk slack/telegram/notif channel lain jika perlu.
        """
        subject = f"[{level.upper()}] Agentic Batch Notification"
        now = datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
        body = f"{now}\nLevel: {level}\nContext: {context or '-'}\n\n{message}"
        print(f"[notification_manager][DEBUG] notify called: subject={subject}, body={body}")
        try:
            return self.send_email(subject, body)
        except Exception as e:
            print(f"[notification_manager][HYBRID-FALLBACK][ERROR] notify failed: {e}")
            return False

if __name__ == "__main__":
    # Contoh penggunaan
    config = {
        'smtp_host': 'smtp.gmail.com',
        'smtp_port': 587,
        'smtp_user': 'your_email@gmail.com',
        'smtp_pass': 'your_app_password',
        'from_email': 'your_email@gmail.com',
        'to_email': ['recipient1@gmail.com'],
        'use_tls': True
    }
    notif = NotificationManager(email_config=config)
    notif.notify("Test notification from NotificationManager", level="info", context="UnitTest")

12. scan_data_folder_summary.py:

import os
import json
import time
from fastapi import APIRouter
from fastapi.responses import JSONResponse

router = APIRouter()
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FOLDER = os.path.join(BASE_DIR, "data")
PROGRESS_FILE = os.path.join(DATA_FOLDER, "file_progress.json")
META_FILE = os.path.join(DATA_FOLDER, "other_gdrive_meta.json")

# Helper for null-safe value
def safe(val, default=None):
    return val if val not in [None, "", [], {}, "null", "None"] else default

def read_json_file(path, default=None):
    if not os.path.exists(path):
        return default if default is not None else []
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default if default is not None else []

def get_meta_list():
    meta = read_json_file(META_FILE, default=[])
    # Mendukung dict dengan key "files"
    if isinstance(meta, dict) and "files" in meta:
        meta = meta["files"]
    return meta if meta else []

def get_progress_dict():
    progress = read_json_file(PROGRESS_FILE, default={})
    return progress if progress else {}

def extract_metric(*args):
    """Return first non-null, non-empty value from args."""
    for a in args:
        if safe(a) is not None:
            return a
    return None

@router.get("/scan_data_folder_summary")
def scan_data_folder_summary():
    """
    Output matriks summary super fleksibel: 
    Setiap file, summary diisi dari file_progress.json dan/atau other_gdrive_meta.json.
    Ambil metrik dari sumber mana saja yang available, prioritas ke progress, fallback ke meta.
    """
    t0 = time.time()
    summary = []

    progress = get_progress_dict()
    meta_list = get_meta_list()

    # Build meta index by file_name (saved_name), name, or id for fast lookup
    meta_index = {}
    for meta in meta_list:
        if not isinstance(meta, dict):
            continue
        # index by several keys for flexibility
        for k in ["saved_name", "name", "id"]:
            if meta.get(k):
                meta_index[meta[k]] = meta

    # Gabung semua unique file_name dari progress dan meta
    file_keys = set(progress.keys())
    for meta in meta_list:
        if isinstance(meta, dict):
            for k in ["saved_name", "name", "id"]:
                if meta.get(k):
                    file_keys.add(meta.get(k))
    # Cari juga dari value file_name/filename di progress
    for v in progress.values():
        if isinstance(v, dict):
            for k in ["file_name", "filename", "file_id"]:
                if v.get(k):
                    file_keys.add(v[k])

    # Untuk setiap file, ambil metrik dari progress dan meta, prioritaskan progress
    for key in file_keys:
        # Karena key bisa id/nama, lookup meta dan progress dengan beberapa strategi
        # progress: by key, by file_name/filename
        p = progress.get(key)
        if not p:
            # coba cari progress dengan file_name/filename
            for v in progress.values():
                if any(safe(v.get(k)) == key for k in ["file_name", "filename", "file_id"]):
                    p = v
                    break

        # meta: by key
        m = meta_index.get(key)
        if not m:
            # coba cari meta dengan saved_name/name/id
            for meta in meta_list:
                if any(safe(meta.get(k)) == key for k in ["saved_name", "name", "id"]):
                    m = meta
                    break

        file_name = extract_metric(
            p.get("file_name") if p else None,
            p.get("filename") if p else None,
            m.get("saved_name") if m else None,
            m.get("name") if m else None,
            key
        )
        file_id = extract_metric(
            p.get("file_id") if p else None,
            m.get("id") if m else None,
            key
        )
        ext = os.path.splitext(str(file_name))[1].lstrip('.').lower() if file_name else ""
        status = extract_metric(
            p.get("status_batch") if p else None,
            m.get("status") if m else None,
            m.get("status_file") if m else None
        )
        size_bytes = extract_metric(
            p.get("size_bytes") if p else None,
            m.get("size_bytes") if m else None
        )
        row_count = extract_metric(
            p.get("total") if p else None,
            m.get("total_items") if m else None,
            m.get("record_count") if m else None
        )
        processed = extract_metric(
            p.get("processed") if p else None
        )
        percent_processed = (
            round((safe(processed,0) / safe(row_count,1))*100, 2) if safe(row_count) and processed is not None else 0.0
        )
        file_mtime_utc = extract_metric(
            p.get("file_mtime_utc") if p else None,
            m.get("file_mtime_utc") if m else None
        )
        sha256 = extract_metric(
            p.get("sha256") if p else None,
            m.get("sha256") if m else None
        )
        last_batch = extract_metric(
            p.get("last_batch") if p else None,
            m.get("last_batch") if m else None
        )
        last_batch_size = extract_metric(
            p.get("last_batch_size") if p else None,
            m.get("last_batch_size") if m else None
        )
        last_error_type = extract_metric(
            p.get("last_error_type") if p else None,
            m.get("last_error_type") if m else None
        )
        retry_count = extract_metric(
            p.get("retry_count") if p else None,
            m.get("retry_count") if m else None
        )
        consecutive_success_count = extract_metric(
            p.get("consecutive_success_count") if p else None,
            m.get("consecutive_success_count") if m else None
        )

        summary.append({
            "file_name": file_name,
            "file_path": os.path.join(DATA_FOLDER, file_name) if file_name else None,
            "file_type": ext,
            "status": status,
            "size_bytes": size_bytes,
            "row_count": row_count,
            "processed": processed,
            "percent_processed": percent_processed,
            "file_mtime_utc": file_mtime_utc,
            "sha256": sha256,
            "file_id": file_id,
            "last_batch": last_batch,
            "last_batch_size": last_batch_size,
            "last_error_type": last_error_type,
            "retry_count": retry_count,
            "consecutive_success_count": consecutive_success_count
        })

    t1 = time.time()
    by_type = {}
    for item in summary:
        ext = item["file_type"]
        by_type.setdefault(ext, 0)
        by_type[ext] += 1

    total_items = sum(safe(item["row_count"], 0) for item in summary if safe(item["row_count"]) is not None)
    total_files = len(summary)
    total_size_bytes = sum(safe(item["size_bytes"], 0) for item in summary if safe(item["size_bytes"]) is not None)

    return JSONResponse({
        "total_items": total_items,
        "total_files": total_files,
        "total_size_bytes": total_size_bytes,
        "by_type": by_type,
        "files": summary,
        "folder_path": DATA_FOLDER,
        "scan_duration_seconds": round(t1 - t0, 3),
        "last_scan_utc": time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
    })

13. smart_file_loader.py:

import os
import json
import pandas as pd
from divisional_level.utils_gdrive import load_meta  # Import absolute, di luar fungsi

# --- CONFIGURABLE ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FOLDER = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_FOLDER, "other_gdrive_meta.json")
ALLOWED_STATUS = {"active", "new file", "changed", "done", "change", "new"}  # Sinkron dengan status meta baru
EXCLUDE_FILES = {"file_progress.json", "other_gdrive_meta.json"}

def is_parquet(fname):
    fname = str(fname).lower()
    return fname.endswith('.parquet') or fname.endswith('.parquet.gz')

def is_parquet_main(fname):
    """
    Hanya file .parquet utama, BUKAN .parquet.meta.json.
    """
    fname = str(fname).lower()
    return (fname.endswith('.parquet') or fname.endswith('.parquet.gz')) and not fname.endswith('.parquet.meta.json')

def get_valid_meta_files(meta_file=META_FILE, allowed_status=ALLOWED_STATUS):
    """
    Ambil set nama file dari meta master (other_gdrive_meta.json) yang statusnya valid dan hanya file .parquet utama.
    """
    if not os.path.exists(meta_file):
        return set()
    try:
        meta = load_meta(meta_file)
        # Kompatibel dict 'files' atau list
        meta_list = meta.get("files", meta) if isinstance(meta, dict) else meta
        return set(
            entry.get("saved_name") or entry.get("name")
            for entry in meta_list
            if (entry.get("saved_name") or entry.get("name"))
            and (entry.get("status") in allowed_status or entry.get("status_file") in allowed_status)
            and is_parquet_main(entry.get("saved_name") or entry.get("name"))
        )
    except Exception as e:
        print(f"[smart_file_loader][ERROR] Failed to read meta: {e}")
        return set()

def get_all_parquet_files(data_folder=DATA_FOLDER):
    """
    Scan semua file parquet utama di folder data (exclude .meta.json, meta, progress, dst).
    """
    files = set()
    for fname in os.listdir(data_folder):
        if fname in EXCLUDE_FILES:
            continue
        if is_parquet_main(fname):
            files.add(fname)
    return files

def load_all_parquet_tables(
    data_folder=DATA_FOLDER,
    meta_file=META_FILE,
    allowed_status=ALLOWED_STATUS,
    exclude_files=EXCLUDE_FILES
):
    """
    Loader hybrid, meta-centric:
      - File dari meta (status valid) selalu diproses.
      - File baru di folder/rename tetap terdeteksi otomatis.
      - Tidak pernah proses .parquet.meta.json.
    """
    meta_files = get_valid_meta_files(meta_file, allowed_status)
    folder_files = get_all_parquet_files(data_folder)
    all_files = meta_files | folder_files  # union: semua file yang valid di meta dan file baru di folder
    tables = {}
    for fname in sorted(all_files):
        if fname in exclude_files or not is_parquet_main(fname):
            continue
        fpath = os.path.join(data_folder, fname)
        if not os.path.exists(fpath):
            continue
        try:
            if fname.endswith('.parquet.gz'):
                import gzip
                with gzip.open(fpath, 'rb') as gzfile:
                    df = pd.read_parquet(gzfile)
            else:
                df = pd.read_parquet(fpath)
            columns = list(df.columns)
            data = df.fillna('').to_dict(orient='records')
            tables[fname] = {
                "columns": columns,
                "data": data,
                "in_meta": fname in meta_files,
            }
            if fname not in meta_files:
                print(f"[SMART-LOADER][WARNING] File {fname} belum tercatat di meta! (akan diproses, meta perlu update jika ingin full meta-centric)")
        except Exception as e:
            print(f"[SMART-LOADER][ERROR] Gagal load {fname}: {e}")
    print(f"[SMART-LOADER] Total file terbaca: {len(tables)}")
    return tables

def get_first_parquet_file_path(data_folder=DATA_FOLDER, table_name=None):
    """
    Dapatkan path file parquet utama pertama untuk table_name, prioritas .parquet.gz lalu .parquet.
    PATCH: Hanya .parquet utama, bukan .meta.json.
    """
    PRIORITY_EXTS = ['.parquet.gz', '.parquet']
    files = get_all_parquet_files(data_folder)
    if table_name:
        # Toleran terhadap spasi dan case
        norm_table = table_name.strip().lower().replace(" ", "")
        for ext in PRIORITY_EXTS:
            for f in files:
                fname_noext, fext = os.path.splitext(f)
                if fname_noext.strip().lower().replace(" ", "") == norm_table and fext.lower() == ext:
                    fpath = os.path.join(data_folder, f)
                    return fpath, f, get_media_type(f)
    for ext in PRIORITY_EXTS:
        for f in files:
            if f.lower().endswith(ext):
                fpath = os.path.join(data_folder, f)
                return fpath, f, get_media_type(f)
    return None, None, None

def get_media_type(fname):
    fname = fname.lower()
    if fname.endswith('.parquet.gz'):
        return "application/gzip"
    elif fname.endswith('.parquet'):
        return "application/octet-stream"
    else:
        return "application/octet-stream"

class SmartFileLoader:
    def __init__(self, data_folder=DATA_FOLDER):
        self.data_folder = data_folder

    @staticmethod
    def supported_formats():
        return [
            ".parquet", ".parquet.gz"
        ]

    def load_all_parquet_tables(self):
        return load_all_parquet_tables(self.data_folder)

    def get_first_parquet_file_path(self, table_name=None):
        return get_first_parquet_file_path(self.data_folder, table_name)

    def get_media_type(self, fname):
        return get_media_type(fname)

# Contoh penggunaan (standalone):
if __name__ == "__main__":
    tables = load_all_parquet_tables()
    for fname, table in tables.items():
        print(f"{fname}: {len(table['data'])} rows, columns={table['columns']}, in_meta={table['in_meta']}")

14. smart_file_preprocessing.py:

import os
import json
from typing import List, Dict
from divisional_level.utils_gdrive import load_meta  # Absolute import, satu kali saja

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FOLDER = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_FOLDER, "other_gdrive_meta.json")
ALLOWED_STATUS = {"active", "new file", "changed", "change", "done", "new"}  # Sinkron dengan meta master baru

def load_valid_parquet_files_from_meta(meta_file=META_FILE, allowed_status=ALLOWED_STATUS, data_folder=DATA_FOLDER):
    """
    Ambil daftar file Parquet yang statusnya valid dari meta master (other_gdrive_meta.json).
    """
    if not os.path.exists(meta_file):
        print(f"[smart_file_preprocessing] Meta file {meta_file} tidak ditemukan!")
        return []
    try:
        meta = load_meta(meta_file)
        # Bisa jadi meta adalah dict dengan key 'files', atau list
        meta_list = meta.get("files", meta) if isinstance(meta, dict) else meta
        valid_files = []
        for entry in meta_list:
            fname = entry.get("saved_name") or entry.get("name")
            # Status bisa di "status" atau "status_file" tergantung meta generator
            status = entry.get("status", "") or entry.get("status_file", "")
            if fname and status in allowed_status and fname.lower().endswith('.parquet'):
                fpath = os.path.join(data_folder, fname)
                if os.path.exists(fpath):
                    valid_files.append((fname, fpath))
        return valid_files
    except Exception as e:
        print(f"[smart_file_preprocessing][ERROR] Gagal baca meta: {e}")
        return []

def extract_parquet_lines(filepath: str) -> List[str]:
    """
    Membaca semua baris dari file Parquet sebagai string (setiap baris -> JSON string).
    """
    try:
        import pandas as pd
        df = pd.read_parquet(filepath)
        lines = []
        for _, row in df.iterrows():
            line = json.dumps(row.dropna().to_dict(), ensure_ascii=False)
            lines.append(line)
        print(f"[DEBUG] extract_parquet_lines: extracted {len(lines)} lines from {filepath}")
        return lines
    except Exception as e:
        print(f"[smart_file_preprocessing][ERROR] Gagal extract_parquet_lines for {filepath}: {e}")
        return []

def preprocess_all_files(data_folder: str = DATA_FOLDER) -> Dict[str, Dict]:
    """
    Returns a dict: {filename: {"raw_lines": [...], "extension": ext}}
    Hanya proses file Parquet yang ada di meta master dengan status valid.
    """
    print(f"[DEBUG] preprocess_all_files: processing folder {data_folder}")
    data = {}
    files = load_valid_parquet_files_from_meta(data_folder=data_folder)
    for fname, fpath in files:
        ext = os.path.splitext(fname)[-1].lower()
        print(f"[DEBUG] preprocess_all_files: extracting lines from {fname}")
        raw_lines = extract_parquet_lines(fpath)
        data[fname] = {
            "raw_lines": raw_lines,
            "extension": ext
        }
        print(f"[DEBUG] preprocess_all_files: {fname} -> {len(raw_lines)} lines, ext={ext}")
    print(f"[DEBUG] preprocess_all_files: processed {len(data)} files")
    return data

def preprocess_to_flat_table(pre_file_result: Dict[str, Dict]) -> Dict[str, Dict]:
    """
    Mengubah hasil preprocess_all_files menjadi format tabel flat list of dict,
    seragam dengan output CSV loader: {filename: {"columns": [...], "data": [...]}}
    Untuk Parquet: setiap baris dijadikan dict, kolom diambil dari file.
    """
    print("[DEBUG] preprocess_to_flat_table called")
    result = {}
    for fname, item in pre_file_result.items():
        lines = item.get("raw_lines", [])
        ext = item.get("extension", "")
        data = []
        columns = set()
        for line in lines:
            try:
                row = json.loads(line)
                if isinstance(row, dict):
                    data.append(row)
                    columns.update(row.keys())
            except Exception:
                data.append({"text": line})
                columns.add("text")
        columns = sorted(list(columns))
        result[fname] = {
            "columns": columns,
            "data": data
        }
        print(f"[DEBUG] preprocess_to_flat_table: {fname} -> {len(data)} rows, columns={columns}")
    print(f"[DEBUG] preprocess_to_flat_table: processed {len(result)} files")
    return result

# Contoh penggunaan (standalone)
if __name__ == "__main__":
    print("[DEBUG] Standalone smart_file_preprocessing.py start.")
    pre = preprocess_all_files()
    flat = preprocess_to_flat_table(pre)
    for fname, table in flat.items():
        print(f"{fname}: {len(table['data'])} rows, columns={table['columns']}")
    print("[DEBUG] Standalone smart_file_preprocessing.py done.")

15. smart_file_scanner.py:

import os
import hashlib
import time
import json
from divisional_level.utils_gdrive import load_meta

# Only Parquet supported via meta after data pipeline migration
SUPPORTED_EXTS = [
    '.parquet', '.parquet.gz'
]
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_DIR, "other_gdrive_meta.json")
ALLOWED_STATUS = {"active", "new file", "changed", "change", "done", "new"}

def calc_sha256_from_file(path, block_size=65536):
    """Hitung SHA256 file, efisien untuk file besar."""
    sha256 = hashlib.sha256()
    try:
        print(f"[DEBUG] calc_sha256_from_file: {path}")
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(block_size), b""):
                sha256.update(chunk)
        sha = sha256.hexdigest()
        print(f"[DEBUG] calc_sha256_from_file: {path} sha256={sha}")
        return sha
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] calc_sha256_from_file failed for {path}: {e}")
        return ""

def load_valid_parquet_meta(meta_file=META_FILE, allowed_status=ALLOWED_STATUS, data_dir=DATA_DIR):
    """
    Ambil daftar file Parquet valid dari meta master beserta metadata status.
    """
    if not os.path.exists(meta_file):
        print(f"[smart_file_scanner][ERROR] Meta file {meta_file} tidak ditemukan!")
        return []
    try:
        meta = load_meta(meta_file)
        meta_list = meta.get("files", meta) if isinstance(meta, dict) else meta
        result = []
        for entry in meta_list:
            fname = entry.get("saved_name") or entry.get("name")
            # Status bisa di "status" atau "status_file"
            status = entry.get("status", "") or entry.get("status_file", "")
            if fname and status in allowed_status and fname.lower().endswith('.parquet'):
                fpath = os.path.join(data_dir, fname)
                if os.path.exists(fpath):
                    info = {
                        'name': fname,
                        'path': fpath,
                        'ext': os.path.splitext(fname)[-1].lower(),
                        'meta_status': status,
                        'size_bytes': None,
                        'modified_time': None,
                        'sha256': None,
                        'meta': entry  # simpan meta asli untuk downstream
                    }
                    try:
                        info['size_bytes'] = os.path.getsize(fpath)
                        info['modified_time'] = os.path.getmtime(fpath)
                        info['sha256'] = calc_sha256_from_file(fpath)
                    except Exception as e:
                        print(f"[smart_file_scanner][ERROR] Failed to stat {fname}: {e}")
                    result.append(info)
        return result
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] Gagal baca meta: {e}")
        return []

def scan_data_folder(data_dir=DATA_DIR, exts=SUPPORTED_EXTS, include_hidden=False, pm=None, only_incomplete=False):
    """
    Scan folder data, tapi hanya mengembalikan file Parquet yang ada di meta master (status valid).
    Return: list of dict:
        [{
            'name': 'namafile.parquet',
            'path': '/full/path/namafile.parquet',
            'ext': '.parquet',
            'size_bytes': 12345,
            'modified_time': 1685420000.123,
            'sha256': '...',
            'meta_status': 'active',
            'meta': { ... },
            'progress': {...},
            'percent_processed': ..
        }, ...]
    """
    print(f"[DEBUG] scan_data_folder [META MODE]: data_dir={data_dir}, exts={exts}, include_hidden={include_hidden}")
    files = load_valid_parquet_meta(data_dir=data_dir)
    filtered = []
    for info in files:
        fname = info['name']
        if not include_hidden and fname.startswith('.'):
            print(f"[DEBUG] scan_data_folder: skip hidden {fname}")
            continue
        ext = info['ext']
        if ext not in exts:
            print(f"[DEBUG] scan_data_folder: skip ext {fname} ({ext})")
            continue
        # PATCH: Tambahkan info progres dan persen processed jika pm diberikan
        progress = None
        percent_processed = None
        if pm is not None and hasattr(pm, "get_file_progress"):
            progress = pm.get_file_progress(fname)
            if progress:
                info['progress'] = progress
                processed = progress.get('processed', 0)
                total = progress.get('total', None)
                if total and total > 0:
                    percent_processed = round((processed / total) * 100, 2)
                    info['percent_processed'] = percent_processed
                else:
                    info['percent_processed'] = None
        filtered.append(info)
        print(f"[DEBUG] scan_data_folder: found {info}")
    # PATCH: filter only_incomplete jika diinginkan (dan pm ada)
    if only_incomplete and pm is not None:
        before_filter = len(filtered)
        filtered = [
            f for f in filtered
            if f.get("progress") and f["progress"].get("processed", 0) < f["progress"].get("total", 0)
        ]
        print(f"[DEBUG] scan_data_folder: only_incomplete filter: {before_filter} -> {len(filtered)} files")
    print(f"[DEBUG] scan_data_folder: total files found: {len(filtered)}")
    return filtered

def detect_new_and_changed_files(data_dir, prev_snapshot, pm=None, only_incomplete=False):
    """
    Bandingkan snapshot scan terbaru dengan snapshot sebelumnya (list of dict).
    Return: (list_new, list_changed, list_deleted)
    PATCH: mendukung parameter pm & only_incomplete untuk filter file-file active/incomplete.
    """
    print(f"[DEBUG] detect_new_and_changed_files: data_dir={data_dir}")
    try:
        curr_files = scan_data_folder(data_dir=data_dir, pm=pm, only_incomplete=only_incomplete)
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] scan_data_folder error in detect_new_and_changed_files: {e}")
        curr_files = []
    prev_map = {f['name']: f for f in prev_snapshot}
    curr_map = {f['name']: f for f in curr_files}

    new_files = [f for f in curr_files if f['name'] not in prev_map]
    changed_files = [
        f for f in curr_files
        if f['name'] in prev_map and (
            f['sha256'] != prev_map[f['name']]['sha256'] or
            f['modified_time'] != prev_map[f['name']]['modified_time']
        )
    ]
    deleted_files = [f for f in prev_snapshot if f['name'] not in curr_map]

    print(f"[DEBUG] detect_new_and_changed_files: new_files={len(new_files)}, changed_files={len(changed_files)}, deleted_files={len(deleted_files)}")
    return new_files, changed_files, deleted_files

def snapshot_to_dict(snapshot):
    """Convert snapshot list to dict {name: fileinfo}."""
    try:
        d = {f['name']: f for f in snapshot}
        print(f"[DEBUG] snapshot_to_dict: keys={list(d.keys())}")
        return d
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] snapshot_to_dict failed: {e}")
        return {}

if __name__ == "__main__":
    try:
        scan = scan_data_folder()
        print("[smart_file_scanner] Files scanned (META MODE):")
        for info in scan:
            print(info)
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] main scan failed: {e}")

16. upload_frontend_data.py:

import os
import json
from fastapi import APIRouter, Request

# Ganti: dari batch_agentic, bukan sync_progress
from .batch_agentic import ProgressManager

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

router = APIRouter()

@router.post("/upload_frontend_data")
async def upload_frontend_data(request: Request):
    print("[DEBUG] upload_frontend_data: called")
    data = await request.json()
    print(f"[DEBUG] upload_frontend_data: received data type={type(data)}, keys={list(data.keys()) if isinstance(data, dict) else 'not dict'}")
    filepath = os.path.join(DATA_DIR, "frontend_data.json")
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"[DEBUG] upload_frontend_data: data saved to {filepath}")

    # Sync progress/meta setelah upload data (hybrid fallback)
    pm = ProgressManager(DATA_DIR)
    try:
        pm.sync_progress()
        print("[DEBUG] upload_frontend_data: progress/meta sync selesai (agentic)")
    except Exception as e:
        print(f"[upload_frontend_data][HYBRID-FALLBACK] sync_progress error: {e}")

    return {"status": "ok", "saved_to": filepath}

Modul EDA

1. eda_prefect_flow.py:

from prefect import flow, task
import os
import polars as pl
import json
import time
import hashlib
from datetime import datetime, date
import math

from .my_ge_agentic_utils import apply_dynamic_expectations
from .advanced_evaluator import generate_advanced_review
from .eda_logger import save_eda_log, save_eda_summary
from .eda_rule_flexible import EDAReasonerFlexible

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
EDA_LOG_DIR = os.path.join(BASE_DIR, "data_log")
EDA_RESULT_DIR = os.path.join(BASE_DIR, "eda_result")

# REVISED: Do NOT create folder. Raise error if missing.
if not os.path.exists(EDA_LOG_DIR):
    raise FileNotFoundError(f"Target folder does not exist: {EDA_LOG_DIR}")
if not os.path.exists(EDA_RESULT_DIR):
    raise FileNotFoundError(f"Target folder does not exist: {EDA_RESULT_DIR}")

TEMPLATE_PATH = os.path.join(
    BASE_DIR, "eda_result", "templates", "eda_pdf_template.html"
)

CONFIG_PATH = os.path.join(BASE_DIR, "config.json")
if os.path.exists(CONFIG_PATH):
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        CONFIG = json.load(f)
else:
    CONFIG = {}

def get_conf(*keys, default=None):
    d = CONFIG
    for k in keys:
        if k in d:
            d = d[k]
        else:
            return default
    return d

def json_serial(obj):
    if isinstance(obj, (datetime, date)):
        return obj.isoformat()
    return str(obj)

def clean_json(obj):
    if isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        else:
            return obj
    elif isinstance(obj, (datetime, date)):
        return obj.isoformat()
    elif isinstance(obj, dict):
        return {k: clean_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_json(v) for v in obj]
    else:
        return obj

def get_file_hash(filepath):
    hash_sha256 = hashlib.sha256()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(65536), b""):
            hash_sha256.update(chunk)
    return hash_sha256.hexdigest()

def get_meta_file_entries(meta_path, data_dir):
    if not os.path.exists(meta_path):
        return []
    with open(meta_path, "r", encoding="utf-8") as f:
        meta = json.load(f)
    meta_files = meta.get("files", []) if isinstance(meta, dict) else meta
    file_entries = []
    for entry in meta_files:
        fname = entry.get("saved_name") or entry.get("name")
        file_id = entry.get("id")
        if fname and fname.endswith(".parquet") and file_id:
            fpath = os.path.join(data_dir, fname)
            if os.path.exists(fpath):
                file_entries.append({
                    "id": file_id,
                    "file": fname,
                    "path": fpath,
                })
    return file_entries

def get_existing_eda_hash(file_id):
    json_file = os.path.join(EDA_RESULT_DIR, f"{file_id}_eda_final.json")
    if not os.path.exists(json_file):
        return None
    try:
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("data_hash")
    except Exception as e:
        print(f"[ERROR] get_existing_eda_hash: {e}")
        return None

def remove_orphan_eda_results(data_dir, meta_path):
    valid_file_ids = set()
    if os.path.exists(meta_path):
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
        meta_files = meta.get("files", []) if isinstance(meta, dict) else meta
        for entry in meta_files:
            file_id = entry.get("id")
            fname = entry.get("saved_name") or entry.get("name")
            if file_id and fname and fname.endswith(".parquet"):
                fpath = os.path.join(data_dir, fname)
                if os.path.exists(fpath):
                    valid_file_ids.add(file_id)
    for fname in os.listdir(EDA_RESULT_DIR):
        if not fname.endswith("_eda_final.json"):
            continue
        parts = fname.split("_eda_final.")
        if len(parts) != 2:
            continue
        file_id = parts[0]
        if file_id not in valid_file_ids:
            try:
                os.remove(os.path.join(EDA_RESULT_DIR, fname))
                print(f"[CLEANUP] Removed orphan EDA result: {fname}")
            except Exception as e:
                print(f"[CLEANUP] Failed to remove {fname}: {e}")

def log_task_status(status_monitor, step, status, error=None, extra=None):
    log = {"step": step, "status": status}
    if error:
        log["error"] = str(error)
    if extra:
        log.update(extra)
    status_monitor.append(log)
    print(f"[TASK_LOG] {log}")

@task
def load_and_analyze_file(filepath, file_id=None, meta_path=None, n_sample=None, frac=None, stratify_col=None, weight_col=None):
    from .sampling import get_file_sample_df
    from .eda import advanced_eda
    from .schemas import EDAResult
    from .pdf_report_generator import generate_pdf_report

    warnings = []
    sampled = df = None
    status_monitor = []

    basename = os.path.splitext(os.path.basename(filepath))[0]
    output_json_path = os.path.join(EDA_RESULT_DIR, f"{file_id}_eda_final.json")
    output_pdf_path = os.path.join(EDA_RESULT_DIR, f"{file_id}_eda_final.pdf")
    output_html_path = os.path.join(EDA_RESULT_DIR, f"{file_id}_eda_final.html")
    template_path = TEMPLATE_PATH

    data_hash = get_file_hash(filepath)

    try:
        log_task_status(status_monitor, "sampling", "started")
        sample_conf = CONFIG.get('sampling', {})
        sample_df, sampling_info = get_file_sample_df(filepath)
        df = pl.read_parquet(filepath)
        log_task_status(status_monitor, "sampling", "success", extra={"rows": df.height, "cols": list(df.columns)})
    except Exception as e:
        log_task_status(status_monitor, "sampling", "error", error=e)
        return {
            "file": filepath,
            "error": f"Load/Sampling error: {e}",
            "status_monitor": status_monitor
        }

    profile_dict = None
    ydata_profile_summary = None
    try:
        log_task_status(status_monitor, "profiling", "started")
        import pandas as pd
        from ydata_profiling import ProfileReport
        profile = ProfileReport(sample_df.to_pandas(), minimal=False, explorative=True)
        if hasattr(profile, "to_dict"):
            profile_dict = profile.to_dict()
            ydata_profile_summary = {
                "table": profile_dict.get("table", {}),
                "variables": {k: v for k, v in list(profile_dict.get("variables", {}).items())[:10]} # limit 10 var
            }
        elif hasattr(profile, "get_description"):
            profile_dict = profile.get_description()
            ydata_profile_summary = profile_dict
        elif hasattr(profile, "to_json"):
            profile_dict = json.loads(profile.to_json())
            ydata_profile_summary = profile_dict
        log_task_status(status_monitor, "profiling", "success")
    except Exception as e:
        log_task_status(status_monitor, "profiling", "error", error=e)
        warnings.append(f"Profiling error: {e}")

    ge_summary = None
    validation_result = None
    try:
        log_task_status(status_monitor, "great_expectations", "started")
        import pandas as pd
        import great_expectations as ge
        from great_expectations.core.batch import RuntimeBatchRequest
        context = ge.get_context()
        batch_request = RuntimeBatchRequest(
            datasource_name="my_pandas_datasource",
            data_connector_name="default_runtime_data_connector_name",
            data_asset_name="my_pandas_asset",
            runtime_parameters={"batch_data": sample_df.to_pandas()},
            batch_identifiers={"default_identifier_name": "default_id"},
        )
        datasources = [ds["name"] for ds in context.list_datasources()]
        if "my_pandas_datasource" not in datasources:
            context.add_datasource(
                "my_pandas_datasource",
                class_name="Datasource",
                execution_engine={"class_name": "PandasExecutionEngine"},
                data_connectors={
                    "default_runtime_data_connector_name": {
                        "class_name": "RuntimeDataConnector",
                        "batch_identifiers": ["default_identifier_name"],
                    }
                }
            )
        # PATCH: Use the revised apply_dynamic_expectations API (returns (validator, reasoning_log))
        validator, ge_reasoning_log = apply_dynamic_expectations(validator=context.get_validator(batch_request=batch_request), sampled=sample_df)
        validation_result = validator.validate()
        if isinstance(validation_result, dict):
            ge_summary = {
                "success": validation_result.get("success"),
                "statistics": validation_result.get("statistics"),
                "expectations_failed": validation_result.get("unsuccessful_expectations"),
                "reasoning_log": ge_reasoning_log
            }
        else:
            ge_summary = {
                "success": getattr(validation_result, "success", None),
                "statistics": getattr(validation_result, "statistics", None),
                "expectations_failed": getattr(validation_result, "unsuccessful_expectations", None),
                "reasoning_log": ge_reasoning_log
            }
        log_task_status(status_monitor, "great_expectations", "success")
    except Exception as e:
        log_task_status(status_monitor, "great_expectations", "error", error=e)
        warnings.append(f"Great Expectations error: {e}")

    anomaly_alerts = []
    anomaly_rate = 0.0

    eda_insight = None
    context = None
    try:
        log_task_status(status_monitor, "advanced_eda", "started")
        # --- PATCH: Use EDAReasonerFlexible for advanced insight & context ---
        analyzer = EDAReasonerFlexible(sample_df)
        eda_insight = analyzer.get_insight()
        context = eda_insight.get("context")
        # advanced_eda may still be used for additional statistics, but eda_insight is now from EDAReasonerFlexible
        eda_dict = advanced_eda(
            sample_df,
            file=filepath,
            meta=None,
            progress={"status_monitor": status_monitor},  # PATCH: pass status_monitor for audit_testing
            sampling_info=sampling_info,
            profiling_dict=profile_dict,
            ge_result=ge_summary,
            kolaborasi_summary=None,
            file_id=file_id
        )
        # PATCH: Inject context and insight into eda_dict['eda_insight']
        eda_dict['eda_insight'] = eda_insight
        eda_dict['context'] = context
        log_task_status(status_monitor, "advanced_eda", "success")
    except Exception as e:
        log_task_status(status_monitor, "advanced_eda", "error", error=e)
        return {
            "file": filepath,
            "error": f"Advanced EDA error: {e}",
            "status_monitor": status_monitor
        }

    eda_dict['anomaly'] = {
        "alerts": anomaly_alerts,
        "rate": anomaly_rate
    }
    eda_dict['warnings'] = warnings

    # Save DETAIL LOG to data_log (log lengkap seluruh proses)
    try:
        log_task_status(status_monitor, "eda_log", "started")
        log_content = {
            "file": filepath,
            "file_id": file_id,
            "basename": basename,
            "run_time": datetime.utcnow().isoformat(),
            "eda_result": clean_json(eda_dict),
            "warnings": warnings,
            "status_monitor": status_monitor
        }
        save_eda_log(
            EDA_LOG_DIR,
            (file_id, filepath),  # tuple agar nama log unik
            log_content,
            log_type="eda"
        )
        log_task_status(status_monitor, "eda_log", "success")
    except Exception as e:
        log_task_status(status_monitor, "eda_log", "error", error=e)
        warnings.append(f"EDA log save error: {e}")

    # Save SUMMARY to eda_result (hanya ringkasan 3 komponen!)
    try:
        log_task_status(status_monitor, "eda_summary", "started")
        save_eda_summary(
            eda_result_dir=EDA_RESULT_DIR,
            file_id=file_id,
            file_name=filepath,
            ydata_summary=ydata_profile_summary,
            ge_summary=ge_summary,
            eda_insight=eda_dict.get("eda_insight")
        )
        log_task_status(status_monitor, "eda_summary", "success")
    except Exception as e:
        log_task_status(status_monitor, "eda_summary", "error", error=e)

    if meta_path and file_id:
        try:
            log_task_status(status_monitor, "update_meta_status", "skipped")
        except Exception as e:
            log_task_status(status_monitor, "update_meta_status", "error", error=e)

    return {
        "file": filepath,
        "file_id": file_id,
        "data_hash": data_hash,
        "context": context,
        "success": True,
        "warnings": warnings,
        "eda_summary_path": os.path.join(EDA_RESULT_DIR, f"{file_id}_{basename}_eda_final.json"),
        "eda_log_path": os.path.join(EDA_LOG_DIR, f"{file_id}_{basename}_eda_log.json"),
        "status_monitor": status_monitor
    }

@flow
def eda_analyze_all_files(
    data_dir: str = None,
    n_sample: int = None,
    frac: float = None,
    stratify_col: str = None,
    weight_col: str = None,
    recommendations: list = None,
    meta_path: str = None,
):
    if data_dir is None:
        data_dir = os.path.join(BASE_DIR, "data")
    if meta_path is None:
        meta_path = os.path.join(data_dir, "other_gdrive_meta.json")

    sample_conf = CONFIG.get('sampling', {})
    start_time = time.time()
    results = []
    errors = 0
    anomaly_sum = 0.0
    num_files = 0

    file_entries = get_meta_file_entries(meta_path, data_dir)
    files_to_process = []
    for entry in file_entries:
        file_id = entry["id"]
        fpath = entry["path"]
        current_hash = get_file_hash(fpath)
        eda_hash = get_existing_eda_hash(file_id)
        if eda_hash != current_hash:
            files_to_process.append({
                "id": file_id,
                "file": entry["file"],
                "path": fpath
            })

    if not files_to_process:
        print("[EDA] Tidak ada file baru/berubah untuk diproses.")
    else:
        print(f"[EDA] File untuk diproses (baru/berubah): {[f['file'] for f in files_to_process]}")

    for entry in files_to_process:
        fpath = entry["path"]
        fname = entry["file"]
        file_id = entry["id"]
        if not os.path.exists(fpath):
            continue
        num_files += 1
        res = load_and_analyze_file.submit(
            fpath,
            file_id=file_id,
            meta_path=meta_path
        )
        results.append(res)
    final_results = [r.result() for r in results]

    for r in final_results:
        if not isinstance(r, dict):
            print("[WARNING] Skipping non-dict result in final_results:", r)
            continue
        if r.get("error"):
            errors += 1
        anomaly_sum += r.get("eda_result", {}).get("anomaly", {}).get("rate", 0) if "eda_result" in r else 0
    runtime = time.time() - start_time
    anomaly_rate = anomaly_sum / num_files if num_files else 0

    if recommendations is None:
        recommendations = []
        for r in final_results:
            if not isinstance(r, dict):
                print("[WARNING] Skipping non-dict result in recommendations loop:", r)
                continue
            recs = r.get("eda_result", {}).get("eda_insight", {}).get("recommendations", [])
            if recs:
                recommendations.extend(recs)
        recommendations = list(dict.fromkeys(recommendations)) if recommendations else [
            "Tidak ada rekomendasi dinamis dari EDA insight."
        ]

    remove_orphan_eda_results(data_dir, meta_path)

    all_results = []
    for entry in file_entries:
        file_id = entry["id"]
        json_file = os.path.join(EDA_RESULT_DIR, f"{file_id}_eda_final.json")
        if os.path.exists(json_file):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    all_results.append(json.load(f))
            except Exception as e:
                print(f"[ERROR] Failed reading {json_file}: {e}")

    # -------- PATCH: Extract and aggregate metrics per file -----------
    status_data = []
    audit_scores, conf_scores, comp_scores, ci_lowers, ci_uppers = [], [], [], [], []
    for entry in all_results:
        audit = entry.get("audit_testing", {})
        audit_score = audit.get("score") if isinstance(audit, dict) else None
        conf_score = entry.get("confidence_score")
        comp_score = entry.get("completeness_score")
        ci = entry.get("confidence_interval")
        ci_low, ci_up = (ci[0], ci[1]) if isinstance(ci, list) and len(ci) == 2 else (None, None)
        status_data.append({
            "file_id": entry.get("file_id"),
            "file": entry.get("file"),
            "audit_testing": audit_score,
            "confidence_score": conf_score,
            "completeness_score": comp_score,
            "confidence_interval": ci,
        })
        if audit_score is not None: audit_scores.append(audit_score)
        if conf_score is not None: conf_scores.append(conf_score)
        if comp_score is not None: comp_scores.append(comp_score)
        if ci_low is not None: ci_lowers.append(ci_low)
        if ci_up is not None: ci_uppers.append(ci_up)

    def avg(xs):
        xs = [x for x in xs if x is not None]
        return sum(xs) / len(xs) if xs else None

    avg_audit = avg(audit_scores)
    avg_conf = avg(conf_scores)
    avg_comp = avg(comp_scores)
    avg_ci = [avg(ci_lowers), avg(ci_uppers)] if ci_lowers and ci_uppers else None

    summary = {
        "timestamp": datetime.utcnow().isoformat(),
        "data_dir": data_dir,
        "num_files": num_files,
        "runtime": runtime,
        "errors": errors,
        "anomaly_rate": anomaly_rate,
        "success_rate": (num_files - errors) / num_files if num_files else 0,
        "details": all_results,
        "recommendations": recommendations,
        "status": status_data,  
        "completeness_score": avg_comp,
        "confidence_score": avg_conf,
        "confidence_interval": avg_ci,
        "audit_testing": avg_audit,
    }
    print(f"[SUMMARY_LOG] {json.dumps(summary, indent=2, ensure_ascii=False)}")
    return clean_json(summary)

if __name__ == "__main__":
    eda_analyze_all_files()

2. sampling.py:

import polars as pl
import numpy as np
from typing import Optional, Tuple, Dict, Any, List
import os
import json

DATA_LOG_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data_log")
# PATCH: Tidak ada auto-create folder, hanya cek eksistensi
if not os.path.exists(DATA_LOG_DIR):
    raise FileNotFoundError(f"Target folder does not exist: {DATA_LOG_DIR}")

# Load config.json (agentic sampling config)
CONFIG_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "config.json")
def load_sampling_config() -> dict:
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, "r") as f:
            config = json.load(f)
            return config.get("sampling", {})
    return {}

SAMPLING_CONFIG = load_sampling_config()

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.cluster import KMeans

try:
    from imblearn.under_sampling import RandomUnderSampler
    IMBLEARN_AVAILABLE = True
except ImportError:
    IMBLEARN_AVAILABLE = False

try:
    from evidently.test_suite import TestSuite
    from evidently.tests import TestColumnDistribution
    EVIDENTLY_AVAILABLE = True
except ImportError:
    EVIDENTLY_AVAILABLE = False

from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False

##############################
# Numeric Type Detection Helper
##############################
def is_integer_polars_dtype(dtype):
    return dtype in [
        pl.Int8, pl.Int16, pl.Int32, pl.Int64,
        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64
    ]
def is_numeric_polars_dtype(dtype):
    return is_integer_polars_dtype(dtype) or dtype in [pl.Float32, pl.Float64]

##############################
# Auto Feature Selector (Super Cerdas)
##############################
def auto_feature_selector(
    df: pl.DataFrame, 
    target: str, 
    task: str = 'auto', 
    top_n: int = 10, 
    random_state: int = 42,
    use_shap: bool = True
):
    import pandas as pd
    # Convert polars to pandas for sklearn compatibility
    df_pd = df.to_pandas()
    X = df_pd.drop(columns=[target]).select_dtypes(include=['number', 'category', 'object', 'bool']).copy()
    y = df_pd[target]

    if task == 'auto':
        if y.nunique() <= 10 and y.dtype in ['int', 'category', 'object']:
            task = 'classification'
        else:
            task = 'regression'

    for col in X.select_dtypes(include=['object', 'category']):
        X[col] = X[col].astype('category').cat.codes

    info = {}
    selected_features = []

    try:
        if task == 'classification':
            mi = mutual_info_classif(X, y, discrete_features='auto', random_state=random_state)
        else:
            mi = mutual_info_regression(X, y, discrete_features='auto', random_state=random_state)
        mi_scores = pd.Series(mi, index=X.columns)
        info['mutual_info'] = mi_scores.sort_values(ascending=False).to_dict()
    except Exception as e:
        info['mutual_info_error'] = str(e)
        mi_scores = pd.Series(0, index=X.columns)

    try:
        if task == 'classification':
            rf = RandomForestClassifier(n_estimators=100, random_state=random_state)
        else:
            rf = RandomForestRegressor(n_estimators=100, random_state=random_state)
        rf.fit(X, y)
        fi_scores = pd.Series(rf.feature_importances_, index=X.columns)
        info['feature_importance'] = fi_scores.sort_values(ascending=False).to_dict()
    except Exception as e:
        info['feature_importance_error'] = str(e)
        fi_scores = pd.Series(0, index=X.columns)

    if use_shap and SHAP_AVAILABLE:
        try:
            explainer = shap.TreeExplainer(rf)
            shap_values = explainer.shap_values(X)
            if isinstance(shap_values, list):
                mean_abs_shap = pd.DataFrame(shap_values).abs().mean(axis=(0,1))
            else:
                mean_abs_shap = pd.DataFrame(shap_values).abs().mean(axis=0)
            shap_scores = pd.Series(mean_abs_shap, index=X.columns)
            info['shap_importance'] = shap_scores.sort_values(ascending=False).to_dict()
        except Exception as e:
            info['shap_importance_error'] = str(e)
            shap_scores = pd.Series(0, index=X.columns)
    else:
        shap_scores = pd.Series(0, index=X.columns)

    mean_rank = (
        mi_scores.rank(ascending=False, method="min")
        + fi_scores.rank(ascending=False, method="min")
        + shap_scores.rank(ascending=False, method="min")
    ) / 3
    selected_features = mean_rank.sort_values().head(top_n).index.tolist()
    info['selected_features'] = selected_features

    return selected_features, info

##############################
# Smart Dynamic Sample Size (Super Cerdas, Hanya diatur di sini!)
##############################
def smart_dynamic_sample_size(
    n_rows: int,
    min_sample: Optional[int] = None,
    max_sample: Optional[int] = None,
    min_frac: Optional[float] = None,
    max_frac: Optional[float] = None
) -> int:
    """
    Menentukan jumlah sample representatif secara dinamis.
    HANYA fungsi ini yang menentukan sample size, abaikan input n_sample dari luar!
    """
    # Load defaults from config if not provided
    min_sample = min_sample if min_sample is not None else SAMPLING_CONFIG.get("min_sample", 100)
    max_sample = max_sample if max_sample is not None else SAMPLING_CONFIG.get("max_sample", 3000)
    min_frac = min_frac if min_frac is not None else SAMPLING_CONFIG.get("min_frac", 0.005)
    max_frac = max_frac if max_frac is not None else SAMPLING_CONFIG.get("max_frac", 0.1)

    # Hitung sample berbasis fraksi dan batas
    sample_size = int(n_rows * 0.03)
    sample_size = max(sample_size, int(n_rows * min_frac), min_sample)
    sample_size = min(sample_size, int(n_rows * max_frac), max_sample, n_rows)
    print(f"[SmartSample] n_rows={n_rows}, sample_size={sample_size}, min={min_sample}, max={max_sample}, min_frac={min_frac}, max_frac={max_frac}")
    return sample_size

##############################
# Smart Sampling with sklearn/imbalanced-learn
##############################
def detect_stratify_column(df: pl.DataFrame, target: Optional[str] = None, max_unique: int = 20) -> Optional[str]:
    # Use polars to detect best stratify column
    candidates = []
    for col in df.columns:
        if col == target:
            continue
        ser = df[col]
        # PATCH: gunakan cara polars yang benar untuk deteksi tipe
        if (
            ser.dtype == pl.String
            or ser.dtype == pl.Categorical
            or ser.dtype == pl.Boolean
            or is_integer_polars_dtype(ser.dtype)
        ):
            nunique = ser.n_unique()
            if 1 < nunique <= max_unique:
                candidates.append(col)
    return candidates[0] if candidates else None

def smart_sample(
    df: pl.DataFrame,
    n_sample: int,
    stratify_col: Optional[str] = None,
    balanced_col: Optional[str] = None,
    random_state: int = 42,
    cluster_cols: Optional[list] = None,
    n_clusters: int = 10
) -> Tuple[pl.DataFrame, Dict[str, Any]]:
    info = {}
    n_rows = df.height

    # 1. Stratified sampling
    if stratify_col and stratify_col in df.columns:
        info['strategy'] = 'stratified'
        # Convert to pandas for stratified sampling with sklearn
        df_pd = df.to_pandas()
        from sklearn.model_selection import StratifiedShuffleSplit
        split = StratifiedShuffleSplit(n_splits=1, train_size=min(n_sample/n_rows, 1.0), random_state=random_state)
        for _, idx in split.split(df_pd, df_pd[stratify_col]):
            sample = df_pd.iloc[idx]
        info['stratify_col'] = stratify_col
        import pandas as pd
        return pl.from_pandas(sample.reset_index(drop=True)), info

    # 2. Balanced sampling (using imbalanced-learn)
    if balanced_col and balanced_col in df.columns and IMBLEARN_AVAILABLE:
        info['strategy'] = 'balanced'
        df_pd = df.to_pandas()
        X = df_pd.drop(columns=[balanced_col])
        y = df_pd[balanced_col]
        sampler = RandomUnderSampler(random_state=random_state)
        X_res, y_res = sampler.fit_resample(X, y)
        import pandas as pd
        sample = pd.concat([X_res, y_res], axis=1)
        if len(sample) > n_sample:
            sample = sample.sample(n=n_sample, random_state=random_state)
        info['balanced_col'] = balanced_col
        return pl.from_pandas(sample.reset_index(drop=True)), info

    # 3. Cluster sampling (for numeric data)
    if cluster_cols and all(col in df.columns for col in cluster_cols):
        info['strategy'] = 'cluster'
        df_pd = df.to_pandas()
        X = df_pd[cluster_cols].fillna(df_pd[cluster_cols].mean())
        from sklearn.cluster import KMeans
        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
        clusters = kmeans.fit_predict(X)
        df_pd["_cluster"] = clusters
        sample = df_pd.groupby("_cluster", group_keys=False).apply(
            lambda x: x.sample(n=min(len(x), max(1, int(n_sample / n_clusters))), random_state=random_state)
        )
        import pandas as pd
        info['n_clusters'] = n_clusters
        return pl.from_pandas(sample.drop(columns=["_cluster"]).reset_index(drop=True)), info

    # 4. Random sampling
    info['strategy'] = 'random'
    if n_sample < n_rows:
        # Use polars sample
        sample = df.sample(n_sample, seed=random_state)
    else:
        sample = df.clone()
    return sample, info

########################
# REPRESENTATIVITY VALIDATION (Evidently)
########################

def validate_sample_evidently(
    df_full: pl.DataFrame,
    df_sample: pl.DataFrame,
    columns: Optional[list] = None
) -> Dict[str, Any]:
    if not EVIDENTLY_AVAILABLE:
        return {"evidently": False, "msg": "Evidently not installed."}
    df_full_pd = df_full.to_pandas()
    df_sample_pd = df_sample.to_pandas()
    columns = columns or [col for col in df_full_pd.columns if df_full_pd[col].dtype in ["O", "object"] or np.issubdtype(df_full_pd[col].dtype, np.number)]
    tests = [TestColumnDistribution(column_name=col) for col in columns if col in df_sample_pd.columns]
    suite = TestSuite(tests=tests)
    suite.run(reference_data=df_full_pd, current_data=df_sample_pd)
    result = suite.as_dict()
    summary = {}
    for test in result['tests']:
        summary[test['name']] = test['status']
    return {"evidently": True, "summary": summary, "success": all(v=="SUCCESS" for v in summary.values())}

##############################
# get_file_sample_df (ENTRY POINT. ABAIKAN n_sample DARI LUAR)
##############################
def get_file_sample_df(
    fpath: str,
    stratify_col: Optional[str] = None,
    balanced_col: Optional[str] = None,
    cluster_cols: Optional[list] = None,
    n_clusters: int = 10,
    random_state: int = 42,
    validate: bool = True,
    min_sample: Optional[int] = None,
    max_sample: Optional[int] = None,
    min_frac: Optional[float] = None,
    max_frac: Optional[float] = None
) -> Tuple[pl.DataFrame, dict]:
    """
    Fungsi utama: hanya sampling.py yang menentukan jumlah sample.
    Semua parameter sampling size dari kode lain diabaikan.
    Akan membaca default sampling config dari config.json jika parameter None.
    """
    df = pl.read_parquet(fpath)
    n_rows = df.height

    # HANYA sampling.py yang menentukan n_sample!
    n_sample = smart_dynamic_sample_size(
        n_rows,
        min_sample=min_sample,
        max_sample=max_sample,
        min_frac=min_frac,
        max_frac=max_frac
    )

    # Deteksi stratify_col otomatis jika belum diset
    stratify_auto = detect_stratify_column(df)
    if stratify_col is None and stratify_auto:
        stratify_col = stratify_auto

    # Sampling
    sample_df, info = smart_sample(
        df,
        n_sample=n_sample,
        stratify_col=stratify_col,
        balanced_col=balanced_col,
        random_state=random_state,
        cluster_cols=cluster_cols,
        n_clusters=n_clusters
    )

    # Validasi representativitas sample menggunakan Evidently
    if validate and EVIDENTLY_AVAILABLE:
        columns_to_check = sample_df.columns[:10]
        eval_res = validate_sample_evidently(df, sample_df, columns=columns_to_check)
        info['evidently'] = eval_res
        if eval_res.get("success") is False:
            info['warning'] = "Sample mungkin kurang representatif pada beberapa kolom!"

    info.update({
        "input_file": fpath,
        "input_rows": n_rows,
        "sample_rows": len(sample_df),
        "n_sample_used": n_sample,
        "stratify_col_used": stratify_col
    })
    # Warning jika sample terlalu kecil (<0.5% data besar)
    warn = ""
    if len(sample_df) / n_rows < 0.005 and n_rows > 2000:
        warn = f"WARNING: Sample hanya {len(sample_df)} dari {n_rows} baris (<0.5%). Hasil EDA mungkin tidak representatif!"
        info['sampling_warning'] = warn
    return sample_df, info

##############################
# Unit Test & Example
##############################
def test_auto_feature_selector():
    import pandas as pd
    df = pl.DataFrame({
        "x1": np.random.randn(100),
        "x2": np.random.rand(100),
        "cat": np.random.choice(['a', 'b', 'c'], 100),
        "target": np.random.randint(0, 2, 100)
    })
    feats, info = auto_feature_selector(df, target='target', task='classification', top_n=2)
    print("Selected features:", feats)
    print("Feature info:", info)

def test_get_file_sample_df():
    import pandas as pd
    df = pl.DataFrame({
        "cat": ["a", "a", "b", "b", "c"]*20,
        "num": np.arange(100),
        "target": np.random.randint(0, 2, 100)
    })
    fpath = "test_data.parquet"
    df.write_parquet(fpath)
    sample, info = get_file_sample_df(fpath)
    print("Sample shape:", sample.shape)
    print("Sampling info:", info)
    os.remove(fpath)

if __name__ == "__main__":
    test_auto_feature_selector()
    test_get_file_sample_df()
    print("sampling.py tests OK")

3. my_ge_agentic_utils.py:

import os
import polars as pl
import numpy as np
import datetime
import json
import warnings

from .eda_logger import save_eda_log  # Use best practice logger

# --- REVISED: All log/metadata JSON files go to data_log ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_LOG_DIR = os.path.join(BASE_DIR, "data_log")
# PATCH: Tidak ada auto-create folder, hanya cek eksistensi
if not os.path.exists(DATA_LOG_DIR):
    raise FileNotFoundError(f"Target folder does not exist: {DATA_LOG_DIR}")

def safe(val, default=None):
    try:
        if isinstance(val, float) and (np.isnan(val) or np.isinf(val)):
            return default
        return val
    except Exception:
        return default

def get_reference_histogram(ser, bins=10):
    arr = np.array(ser.drop_nulls().to_list())
    counts, bin_edges = np.histogram(arr, bins=bins)
    weights = counts / counts.sum() if counts.sum() else np.zeros_like(counts, dtype=float)
    return {"bins": bin_edges.tolist(), "weights": weights.tolist()}

def auto_detect_datetime_col(df: pl.DataFrame):
    dt_cols = []
    for col in df.columns:
        ser = df[col]
        # PATCH: SAFE Polars datetime detection
        if ser.dtype in [pl.Date, pl.Datetime]:
            dt_cols.append(col)
        elif ser.dtype == pl.String:
            arr = ser.drop_nulls().to_list()
            import pandas as pd
            try:
                # Coba konversi 10 sample pertama ke datetime
                pd.to_datetime(arr[:min(10, len(arr))])
                dt_cols.append(col)
            except Exception:
                continue
    return dt_cols

def get_reference_set(ref_df: pl.DataFrame, col):
    try:
        if col in ref_df.columns:
            return set(ref_df[col].drop_nulls().unique().to_list())
        return None
    except Exception:
        return None

def explain_expectation(col, expectation, params, evidence=None):
    reason = f"Expectation '{expectation}' applied on '{col}' with params {params}."
    if evidence:
        reason += f" Evidence: {evidence}"
    return reason

def is_numeric_polars_dtype(dtype):
    # PATCH: Use this everywhere for numeric detection
    return dtype in [
        pl.Int8, pl.Int16, pl.Int32, pl.Int64,
        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
        pl.Float32, pl.Float64
    ]

def apply_dynamic_expectations(validator, sampled: pl.DataFrame, reference_data: dict = None, time_series_check=True):
    """
    Apply super-advance, agentic, robust, multi-functional, and reasoning-rich GE expectations to a DataFrame.
    Parameters:
        validator: GE validator object (already loaded with sampled df)
        sampled: pl.DataFrame (sampled data)
        reference_data: dict, optional (for cross-table/column checks or reference histograms)
        time_series_check: bool, auto-detect and apply special handling if time-series column found
    Returns:
        validator (with all expectations applied), reasoning_log (list)
    """
    reasoning_log = []
    try:
        # Table-level expectations (volume, basic completeness)
        validator.expect_table_row_count_to_be_between(
            min_value=10, max_value=int(1.2 * len(sampled))
        )
        validator.expect_table_column_count_to_equal(len(sampled.columns))
        reasoning_log.append({
            "scope": "table",
            "expectation": "row/col count",
            "params": {"min_rows": 10, "max_rows": int(1.2 * len(sampled)), "n_cols": len(sampled.columns)},
            "reason": "Basic table volume and structure check"
        })

        # Detect time-series columns
        dt_cols = auto_detect_datetime_col(sampled)
        ref_sets = reference_data.get("foreign_keys", {}) if reference_data else {}

        for col in sampled.columns:
            ser = sampled[col]
            col_type = str(ser.dtype)
            arr = ser.drop_nulls().to_list()
            null_rate = (ser.null_count() / sampled.height) if sampled.height else 0.0
            nunique = ser.n_unique()
            params = {}

            # Completeness
            if null_rate < 0.05:
                validator.expect_column_values_to_not_be_null(col)
                reasoning_log.append({
                    "col": col, "expectation": "expect_column_values_to_not_be_null",
                    "params": {}, "reason": explain_expectation(col, "not null", {}, f"null_rate={null_rate:.2%}")
                })
            else:
                validator.expect_column_values_to_not_be_null(col, mostly=1-null_rate)
                reasoning_log.append({
                    "col": col, "expectation": "expect_column_values_to_not_be_null (mostly)",
                    "params": {"mostly": 1-null_rate}, "reason": explain_expectation(col, "not null mostly", {"mostly": 1-null_rate}, f"null_rate={null_rate:.2%}")
                })

            # Uniqueness
            if nunique == sampled.height:
                validator.expect_column_values_to_be_unique(col)
                reasoning_log.append({
                    "col": col, "expectation": "expect_column_values_to_be_unique",
                    "params": {}, "reason": explain_expectation(col, "unique", {}, "All values unique")
                })
            # Foreign key (cross-table)
            if col in ref_sets:
                validator.expect_column_values_to_be_in_set(col, ref_sets[col])
                reasoning_log.append({
                    "col": col, "expectation": "expect_column_values_to_be_in_set",
                    "params": {"set_size": len(ref_sets[col])}, "reason": explain_expectation(col, "fk in set", {"set_size": len(ref_sets[col])})
                })

            # Schema/type/length
            if is_numeric_polars_dtype(ser.dtype):
                minval, maxval = float(safe(ser.min(), 0)), float(safe(ser.max(), 1))
                validator.expect_column_values_to_be_between(col, min_value=minval, max_value=maxval)
                validator.expect_column_min_to_be_between(col, min_value=minval, max_value=maxval)
                validator.expect_column_max_to_be_between(col, min_value=maxval, max_value=maxval)
                reasoning_log.append({
                    "col": col, "expectation": "numeric range",
                    "params": {"min": minval, "max": maxval}, "reason": explain_expectation(col, "between", {"min": minval, "max": maxval})
                })
                # Distribution / Outlier
                try:
                    hist = get_reference_histogram(ser)
                    validator.expect_column_kl_divergence_to_be_less_than(col, partition_object=hist, threshold=0.2)
                    reasoning_log.append({
                        "col": col, "expectation": "expect_column_kl_divergence_to_be_less_than",
                        "params": {"partition_object": "auto-histogram", "threshold": 0.2},
                        "reason": explain_expectation(col, "kl divergence", {"threshold": 0.2})
                    })
                except Exception as e:
                    warnings.warn(f"KL divergence failed for {col}: {e}")

                # Advance stats (mean, std, median)
                mean = float(safe(ser.mean()))
                std = float(safe(ser.std()))
                median = float(safe(ser.median()))
                validator.expect_column_mean_to_be_between(col, min_value=mean-std, max_value=mean+std)
                validator.expect_column_median_to_be_between(col, min_value=median-std, max_value=median+std)
                validator.expect_column_stdev_to_be_between(col, min_value=0, max_value=std*2)
                reasoning_log.append({
                    "col": col, "expectation": "mean/median/stdev range",
                    "params": {"mean": mean, "std": std, "median": median}, "reason": explain_expectation(col, "mean/median/stdev", {"mean": mean, "std": std, "median": median})
                })

            elif ser.dtype == pl.String:
                maxlen = int(np.max([len(str(x)) for x in arr])) if arr else 30
                validator.expect_column_value_lengths_to_be_between(col, min_value=1, max_value=maxlen)
                reasoning_log.append({
                    "col": col, "expectation": "expect_column_value_lengths_to_be_between",
                    "params": {"min": 1, "max": maxlen}, "reason": explain_expectation(col, "length between", {"min": 1, "max": maxlen})
                })
                # Regex detection
                if "email" in col.lower():
                    validator.expect_column_values_to_match_regex(col, r"^[\w\.-]+@[\w\.-]+\.\w+$")
                    reasoning_log.append({
                        "col": col, "expectation": "expect_column_values_to_match_regex",
                        "params": {"regex": "email"}, "reason": explain_expectation(col, "regex email", {"regex": "email"})
                    })
                if "phone" in col.lower():
                    validator.expect_column_values_to_match_regex(col, r"^\+?\d{8,15}$")
                    reasoning_log.append({
                        "col": col, "expectation": "expect_column_values_to_match_regex",
                        "params": {"regex": "phone"}, "reason": explain_expectation(col, "regex phone", {"regex": "phone"})
                    })
                if "status" in col.lower():
                    status_set = ["PAID", "UNPAID", "CANCELLED", "SUCCESS", "FAILED"]
                    validator.expect_column_values_to_be_in_set(col, status_set)
                    reasoning_log.append({
                        "col": col, "expectation": "expect_column_values_to_be_in_set",
                        "params": {"set": status_set}, "reason": explain_expectation(col, "status set", {"set": status_set})
                    })

            # Time-series
            if time_series_check and col in dt_cols:
                validator.expect_column_values_to_be_increasing(col)
                reasoning_log.append({
                    "col": col, "expectation": "expect_column_values_to_be_increasing",
                    "params": {}, "reason": explain_expectation(col, "increasing", {}, "Time series monotonicity")
                })
                try:
                    import pandas as pd
                    arr_sorted = sorted(arr)
                    diff = pd.to_datetime(arr_sorted).diff().dt.total_seconds().dropna()
                    max_gap = float(diff.max()) if len(diff) > 0 else 0
                    if max_gap > 3 * 24 * 3600:
                        validator.expect_column_values_to_be_between(col, min_value=min(arr_sorted), max_value=max(arr_sorted))
                        reasoning_log.append({
                            "col": col, "expectation": "expect_column_values_to_be_between (time gap)",
                            "params": {"min": str(min(arr_sorted)), "max": str(max(arr_sorted))},
                            "reason": explain_expectation(col, "time gap", {"min": str(min(arr_sorted)), "max": str(max(arr_sorted))}, f"max_gap={max_gap}")
                        })
                except Exception:
                    pass

        # Cross-column (example: totalsales > qty, if both present)
        if set(["totalsales", "qty"]).issubset(set(sampled.columns)):
            validator.expect_column_pair_values_A_to_be_greater_than_B("totalsales", "qty")
            reasoning_log.append({
                "cols": ["totalsales", "qty"],
                "expectation": "expect_column_pair_values_A_to_be_greater_than_B",
                "params": {}, "reason": explain_expectation("totalsales, qty", "A>B", {}, "Business logic totalsales > qty")
            })

        # Cross-table (foreign key, if reference_data given)
        if reference_data and "foreign_keys" in reference_data:
            for col, ref_set in reference_data["foreign_keys"].items():
                if col in sampled.columns:
                    validator.expect_column_values_to_be_in_set(col, ref_set)
                    reasoning_log.append({
                        "col": col, "expectation": "expect_column_values_to_be_in_set (FK)",
                        "params": {"fk_size": len(ref_set)},
                        "reason": explain_expectation(col, "fk in set", {"fk_size": len(ref_set)})
                    })

    except Exception as e:
        warnings.warn(f"Agentic GE expectation failed: {e}")
        reasoning_log.append({"error": str(e)})

    # PATCH: DO NOT log agentic reasoning to a separate file!
    # Instead, return the reasoning_log so caller can store it in the main per-file EDA log
    return validator, reasoning_log

4. advanced_evaluator.py:

import polars as pl
import json
import os
from dotenv import load_dotenv

from .eda_rule_flexible import EDAReasonerFlexible

# Muat variabel lingkungan dari file .env
load_dotenv()

def rule_based_insight(df: pl.DataFrame):
    """
    Insight sederhana berbasis rule (opsional, sebagai fallback jika EDAReasonerFlexible gagal).
    """
    insights = []
    null_counts = [df[col].null_count() for col in df.columns]
    if any(null_counts):
        insights.append("Terdapat nilai kosong (missing value) pada beberapa kolom, lakukan imputasi atau hapus baris terkait.")
    n_rows = df.height
    for col in df.columns:
        n_unique = df[col].n_unique()
        if n_unique > 0.9 * n_rows:
            insights.append(f"Kolom '{col}' memiliki kardinalitas tinggi (high cardinality), sebaiknya tidak digunakan sebagai fitur prediktor.")
    return insights

def generate_advanced_review(df: pl.DataFrame, profiling_json, ge_summary, gemini_api_key=None):
    """
    Menghasilkan advanced review JSON dengan insight otomatis dari EDAReasonerFlexible.
    Jika output EDAReasonerFlexible gagal, fallback ke insight sederhana rule-based.
    Parameter gemini_api_key diabaikan (tidak relevan lagi).
    """
    try:
        analyzer = EDAReasonerFlexible(df)
        enrichment = analyzer.get_insight()
        if enrichment and ("insight" in enrichment or "recommendations" in enrichment):
            return enrichment
        else:
            return {
                "error": "EDAReasonerFlexible tidak menghasilkan output yang diharapkan.",
                "rule_based_insight": rule_based_insight(df)
            }
    except Exception as e:
        return {
            "error": f"Terjadi kesalahan: {str(e)}",
            "rule_based_insight": rule_based_insight(df)
        }

##########################
# Integrasi dengan sampling.py
##########################

def get_sampled_review(
    fpath,
    profiling_json,
    ge_summary,
    n_sample=None,
    frac=None,
    stratify_col=None,
    balanced_col=None,
    cluster_cols=None,
    n_clusters=10,
    random_state=42,
    gemini_api_key=None,
    sampling_validate=True
):
    """
    Wrapper untuk mengambil sample representatif (menggunakan sampling.py),
    lalu menjalankan advanced review EDAReasonerFlexible berbasis sample.
    """
    from .sampling import get_file_sample_df

    # Ambil sample representatif
    sample_df, sampling_info = get_file_sample_df(
        fpath=fpath,
        stratify_col=stratify_col,
        balanced_col=balanced_col,
        cluster_cols=cluster_cols,
        n_clusters=n_clusters,
        random_state=random_state,
        validate=sampling_validate
    )

    # Dapatkan advanced review dari sample
    review = generate_advanced_review(
        sample_df,
        profiling_json=profiling_json,
        ge_summary=ge_summary,
        gemini_api_key=gemini_api_key
    )

    # Sertakan info sampling untuk audit trail dan interpretasi insight
    review["sampling_info"] = sampling_info
    return review

# --- Unit Test ---
def test_advanced_evaluator():
    df = pl.DataFrame({
        "A": [1, 2, 3, None, 5, 6, 7, 8, 9, 10],
        "B": ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j"]
    })
    profiling_json = {"summary": "Dummy profiling"}
    ge_summary = {"checks": "Dummy GE"}
    insight = generate_advanced_review(df, profiling_json, ge_summary)
    print("Advanced review output:", insight)

if __name__ == "__main__":
    test_advanced_evaluator()
    print("advanced_evaluator.py tests OK")

5. eda.py:

import numpy as np
import polars as pl
import datetime
from typing import Dict, Any, List, Optional
from .utils import safe
import os
import json
from scipy import stats

from .eda_logger import save_eda_log, save_eda_summary
from .eda_rule_flexible import EDAReasonerFlexible
from .audit_testing import audit_testing_auto

DATA_LOG_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data_log")
EDA_RESULT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "eda_result")

if not os.path.exists(DATA_LOG_DIR):
    raise FileNotFoundError(f"Target folder does not exist: {DATA_LOG_DIR}")
if not os.path.exists(EDA_RESULT_DIR):
    raise FileNotFoundError(f"Target folder does not exist: {EDA_RESULT_DIR}")

CONFIG_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "config.json")
if os.path.exists(CONFIG_PATH):
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        CONFIG = json.load(f)
else:
    CONFIG = {}

def get_conf(*keys, default=None):
    d = CONFIG
    for k in keys:
        if k in d:
            d = d[k]
        else:
            return default
    return d

def clean_json(obj):
    import math
    if isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        else:
            return obj
    elif isinstance(obj, (datetime.datetime, datetime.date)):
        return obj.isoformat()
    elif isinstance(obj, dict):
        return {k: clean_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_json(v) for v in obj]
    else:
        return obj

def ensure_all_string_columns(df: pl.DataFrame) -> pl.DataFrame:
    """
    Convert all columns that look like string/object (or mixed) to pl.String,
    so Polars string namespace is always available and .str.length() never errors.
    """
    new_cols = []
    for col in df.columns:
        ser = df[col]
        if (
            ser.dtype in (pl.Object, pl.Utf8, pl.String, pl.Null)
            or str(ser.dtype).lower() in ("object", "string", "str", "utf8")
        ):
            try:
                new_cols.append(ser.cast(pl.String).alias(col))
            except Exception:
                try:
                    arr = np.array(ser.to_list(), dtype=str)
                    new_cols.append(pl.Series(col, arr, dtype=pl.String))
                except Exception:
                    new_cols.append(ser)
        else:
            new_cols.append(ser)
    return pl.DataFrame(new_cols)

def ensure_all_numeric_columns(df: pl.DataFrame) -> pl.DataFrame:
    """
    Enterprise-grade: try to auto-cast columns with numeric-looking values to Float (if possible).
    Solves cases where numeric columns are loaded as string/object due to bad Parquet/Pandas/DuckDB conversions.
    """
    new_cols = []
    for col in df.columns:
        ser = df[col]
        # skip columns that are already numeric
        if ser.dtype in [
            pl.Int8, pl.Int16, pl.Int32, pl.Int64,
            pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
            pl.Float32, pl.Float64
        ]:
            new_cols.append(ser)
            continue
        # Try to cast string/object columns to float if possible
        if ser.dtype in (pl.String, pl.Object, pl.Utf8, pl.Null) or str(ser.dtype).lower() in ("object", "string", "str", "utf8"):
            try:
                arr = ser.to_list()
                arr_numeric = []
                valid_count = 0
                for x in arr:
                    try:
                        num = float(x)
                        arr_numeric.append(num)
                        valid_count += 1
                    except Exception:
                        arr_numeric.append(np.nan)
                if len(arr_numeric) > 0 and valid_count / len(arr_numeric) > 0.9:
                    new_cols.append(pl.Series(col, arr_numeric, dtype=pl.Float64))
                else:
                    new_cols.append(ser)
            except Exception:
                new_cols.append(ser)
        else:
            new_cols.append(ser)
    return pl.DataFrame(new_cols)

def agentic_cast_to_string(ser):
    if ser.dtype == pl.String:
        return ser
    try:
        return ser.cast(pl.String)
    except Exception:
        try:
            ser_np = np.array(ser.to_list(), dtype=str)
            return pl.Series("", ser_np).cast(pl.String)
        except Exception as e:
            print(f"[AGENTIC][ERROR] agentic_cast_to_string failed: {e}")
            return pl.Series("", [], dtype=pl.String)

def agentic_string_metrics(ser, col):
    try:
        ser_str = agentic_cast_to_string(ser)
        nonnull = ser_str.drop_nulls()
        if len(nonnull) == 0:
            print(f"[AGENTIC][INFO] Kolom '{col}' seluruhnya NULL/kosong. max_length=0, n_unique=0.")
            return (0, 0, [])
        # Fallback native Python jika .str.length() error
        try:
            lengths = nonnull.str.length()
            max_length = int(lengths.max())
        except Exception as e:
            dtype = ser_str.dtype
            example_values = nonnull[:5].to_list()
            print(f"[AGENTIC][WARNING] .str.length() failed after cast for col={col}: {e}; dtype={dtype}; contoh={example_values}")
            values = [str(x) for x in nonnull.to_list()]
            max_length = max([len(x) for x in values]) if values else 0
        try:
            n_unique = int(nonnull.n_unique())
        except Exception:
            n_unique = len(set(str(x) for x in nonnull.to_list()))
        try:
            sample_values = list(dict.fromkeys([str(x) for x in nonnull.to_list()]))[:5]
        except Exception:
            sample_values = []
        return (max_length, n_unique, sample_values)
    except Exception as e:
        print(f"[AGENTIC][ERROR] agentic_string_metrics failed for col={col}: {e}")
        return (0, 0, [])

def suggest_schema_expectation(df: pl.DataFrame) -> Dict[str, Any]:
    result = {}
    for col in df.columns:
        dtype = str(df[col].dtype)
        suggestion = {"type": dtype}
        ser = df[col]
        if ser.dtype in [
            pl.Int8, pl.Int16, pl.Int32, pl.Int64,
            pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
            pl.Float32, pl.Float64,
        ]:
            suggestion['min'] = float(safe(ser.min(), 0))
            suggestion['max'] = float(safe(ser.max(), 0))
            suggestion['n_unique'] = int(ser.n_unique())
        elif ser.dtype == pl.String or ser.dtype == pl.Null or ser.dtype == pl.Object:
            max_length, n_unique, sample_values = agentic_string_metrics(ser, col)
            suggestion['max_length'] = max_length
            suggestion['n_unique'] = n_unique
            suggestion['sample_values'] = sample_values
        elif ser.dtype in (pl.Date, pl.Datetime):
            suggestion['min'] = str(safe(ser.min()))
            suggestion['max'] = str(safe(ser.max()))
        else:
            try:
                suggestion['n_unique'] = int(ser.n_unique())
                suggestion['sample_values'] = ser.unique()[:5].to_list()
            except Exception:
                suggestion['n_unique'] = 0
                suggestion['sample_values'] = []
        result[col] = suggestion
    return result

def explain_expectation_applied(col, col_type, rule, params, evidence=None):
    reason = f"Expectation '{rule}' applied to column '{col}' (type: {col_type}) with params {params}."
    if evidence:
        reason += f" Evidence: {evidence}"
    return reason

def capture_data_reference(df: pl.DataFrame, cols=None, n=10):
    bins = get_conf('eda', 'hist_bins', default=10)
    ref = {}
    if cols is None:
        cols = df.columns
    for col in cols:
        ser = df[col]
        if ser.dtype in [
            pl.Int8, pl.Int16, pl.Int32, pl.Int64,
            pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
            pl.Float32, pl.Float64,
        ]:
            try:
                arr = np.array(ser.drop_nulls().to_list())
                hist = np.histogram(arr, bins=bins) if len(arr) else ([], [])
                ref[col] = {
                    "min": float(ser.min()) if len(arr) else None,
                    "max": float(ser.max()) if len(arr) else None,
                    "sample": ser.drop_nulls().sample(n=min(n, len(arr))).to_list() if len(arr) else [],
                    "hist_bins": hist[1].tolist() if len(hist) > 1 else [],
                    "hist_counts": hist[0].tolist() if len(hist) > 0 else [],
                }
            except Exception:
                ref[col] = {}
        else:
            try:
                ser_str = agentic_cast_to_string(ser)
                ref[col] = {
                    "value_sample": ser_str.drop_nulls().unique()[:n].to_list()
                }
            except Exception:
                ref[col] = {
                    "value_sample": []
                }
    return ref

def compute_confidence_interval(data, confidence=0.95):
    arr = np.array(data.drop_nulls().to_list()) if hasattr(data, "drop_nulls") else np.array(data.dropna())
    n = len(arr)
    if n == 0:
        return (None, None)
    if n < 30:
        boots = [np.mean(np.random.choice(arr, size=n, replace=True)) for _ in range(1000)]
        lower = np.percentile(boots, (1-confidence)/2*100)
        upper = np.percentile(boots, (1-(1-confidence)/2)*100)
        return (float(lower), float(upper))
    else:
        m = np.mean(arr)
        se = stats.sem(arr) if n > 1 else 0.0
        h = se * stats.t.ppf((1 + confidence) / 2., n-1) if n > 1 else 0.0
        return (float(m - h), float(m + h))

def compute_completeness_score(df: pl.DataFrame, weights: Optional[dict] = None) -> float:
    if weights is None:
        weights = {col: 1.0 for col in df.columns}
    total_weight = sum(weights.values())
    completeness = {col: 1 - (df[col].null_count()/len(df)) if len(df) else 0.0 for col in df.columns}
    score = sum(completeness[col] * weights[col] for col in df.columns) / total_weight if total_weight else 0.0
    if score < 0 or score > 1:
        print("Warning: completeness_score out of bound, check data or logic")
        score = max(0, min(1, score))
    return round(score * 100, 2)

def compute_confidence_score(df: pl.DataFrame, numeric_stats: Dict[str, Any]) -> float:
    if not numeric_stats:
        return 0.0
    scores = []
    for col, stat in numeric_stats.items():
        mean = stat.get("mean")
        if mean is None:
            continue
        arr = np.array(df[col].drop_nulls().to_list())
        n = len(arr)
        if n < 2:
            scores.append(0.0)
            continue
        boots = [np.mean(np.random.choice(arr, size=n, replace=True)) for _ in range(500)]
        std_boot = np.std(boots)
        conf_score = 1 / (1 + std_boot) if std_boot != 0 else 1.0
        conf_score = max(0, min(1, conf_score))
        scores.append(conf_score)
    return round(np.mean(scores) * 100, 2) if scores else 0.0

class NumericAnalyzer:
    @staticmethod
    def analyze(df: pl.DataFrame, confidence=0.95) -> Dict[str, Any]:
        numeric_cols = [col for col in df.columns if df[col].dtype in [
            pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Float32, pl.Float64
        ]]
        bins = get_conf('eda', 'hist_bins', default=10)
        result = {}
        for col in numeric_cols:
            s = df[col].drop_nulls()
            arr = np.array(s.to_list())
            if len(arr) == 0:
                continue
            desc = {
                "count": float(len(arr)),
                "mean": float(np.mean(arr)),
                "std": float(np.std(arr, ddof=1)) if len(arr) > 1 else 0.0,
                "min": float(np.min(arr)),
                "25%": float(np.percentile(arr, 25)),
                "50%": float(np.percentile(arr, 50)),
                "75%": float(np.percentile(arr, 75)),
                "max": float(np.max(arr))
            }
            q25 = desc.get('25%')
            q75 = desc.get('75%')
            iqr = q75 - q25 if (q75 is not None and q25 is not None) else None
            hist = np.histogram(arr, bins=bins) if len(arr) else ([], [])
            outlier_count = int(((arr < (q25 - 1.5 * iqr)) | (arr > (q75 + 1.5 * iqr))).sum()) if iqr else 0
            skew = float(stats.skew(arr)) if len(arr) > 2 else 0
            kurt = float(stats.kurtosis(arr)) if len(arr) > 2 else 0
            zeros = int((arr == 0).sum())
            neg = int((arr < 0).sum())
            pos = int((arr > 0).sum())
            ci_low, ci_high = compute_confidence_interval(s, confidence)
            result[col] = {
                "count": float(safe(desc.get("count"), 0)),
                "mean": float(safe(desc.get("mean"), 0)),
                "std": float(safe(desc.get("std"), 0)),
                "min": float(safe(desc.get("min"), 0)),
                "q25": float(safe(desc.get("25%"), 0)),
                "q50": float(safe(desc.get("50%"), 0)),
                "q75": float(safe(desc.get("75%"), 0)),
                "max": float(safe(desc.get("max"), 0)),
                "unique": int(len(np.unique(arr))),
                "outlier_count": outlier_count,
                "hist_bins": hist[1].tolist() if len(hist) > 1 else [],
                "hist_counts": hist[0].tolist() if len(hist) > 0 else [],
                "skewness": skew,
                "kurtosis": kurt,
                "zero_ratio": zeros / len(arr) if len(arr) else 0,
                "neg_ratio": neg / len(arr) if len(arr) else 0,
                "pos_ratio": pos / len(arr) if len(arr) else 0,
                "mean_confidence_interval": [ci_low, ci_high]
            }
        return result

class CategoricalAnalyzer:
    @staticmethod
    def analyze(df: pl.DataFrame, max_freq: int = None) -> Dict[str, Any]:
        cat_cols = [col for col in df.columns if df[col].dtype == pl.String or df[col].dtype == pl.Categorical]
        if max_freq is None:
            max_freq = get_conf('eda', 'max_freq', default=5)
        result = {}
        for col in cat_cols:
            try:
                s = agentic_cast_to_string(df[col])
                s = s.drop_nulls().to_list()
                if not s:
                    continue
                vals, counts = np.unique(s, return_counts=True)
                freq = dict(sorted(zip(vals, counts), key=lambda x: -x[1])[:max_freq])
                value_counts = counts / np.sum(counts)
                entropy = float(np.sum([-p * np.log2(p) if p > 0 else 0 for p in value_counts]))
                rare = [v for v, c in zip(vals, counts) if c == 1]
                rare_count = len(rare)
                rare_pct = rare_count / len(s) * 100 if len(s) else 0
                result[col] = {
                    "unique": int(len(vals)),
                    "top_freq": freq,
                    "entropy": entropy,
                    "rare_count": rare_count,
                    "rare_pct": rare_pct
                }
            except Exception as e:
                result[col] = {
                    "unique": 0,
                    "top_freq": {},
                    "entropy": 0,
                    "rare_count": 0,
                    "rare_pct": 0,
                    "error": str(e)
                }
        return result

class QualityScorer:
    @staticmethod
    def score(df: pl.DataFrame, numeric_stats: Dict[str, Any]):
        completeness_score = compute_completeness_score(df)
        confidence_score = compute_confidence_score(df, numeric_stats)
        return completeness_score, confidence_score

def patch_metrics(result: dict):
    """
    Patch to guarantee core metrics are never null or None.
    Call this before saving result/summary/log.
    """
    # Guarantee status always a list
    if "status" not in result or result["status"] is None:
        result["status"] = []
    # Guarantee completeness_score, confidence_score always float
    for k in ["completeness_score", "confidence_score"]:
        if result.get(k) is None or (isinstance(result.get(k), float) and np.isnan(result.get(k))):
            result[k] = 0.0
    # Guarantee confidence_interval always a 2-float list
    ci = result.get("confidence_interval")
    if ci is None or not isinstance(ci, list) or len(ci) != 2 or any(
        x is None or (isinstance(x, float) and np.isnan(x)) for x in ci
    ):
        result["confidence_interval"] = [0.0, 0.0]
    # Guarantee audit_testing always dict with score
    audit = result.get("audit_testing")
    if audit is None or not isinstance(audit, dict):
        result["audit_testing"] = {"score": 0.0, "evidence": {}, "comment": ""}
    else:
        if audit.get("score") is None or (isinstance(audit.get("score"), float) and np.isnan(audit.get("score"))):
            result["audit_testing"]["score"] = 0.0
        if "evidence" not in audit or audit["evidence"] is None:
            result["audit_testing"]["evidence"] = {}
        if "comment" not in audit or audit["comment"] is None:
            result["audit_testing"]["comment"] = ""
    return result

def advanced_eda(
    df: pl.DataFrame,
    file: str,
    meta: Optional[dict] = None,
    progress: Optional[dict] = None,
    sampling_info: Optional[dict] = None,
    profiling_dict: Optional[dict] = None,
    ge_result: Optional[dict] = None,
    kolaborasi_summary: Optional[dict] = None,
    file_id: Optional[str] = None
) -> dict:
    # PATCH ENTERPRISE: ensure all string and numeric columns are cast in advance
    df = ensure_all_string_columns(df)
    df = ensure_all_numeric_columns(df)
    n = len(df)
    # --- DEBUG PATCH: print numeric columns and numeric_stats if null ---
    numeric_cols = [col for col in df.columns if df[col].dtype in [
        pl.Int8, pl.Int16, pl.Int32, pl.Int64,
        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
        pl.Float32, pl.Float64
    ]]
    print("DEBUG - Numeric columns:", numeric_cols)
    auto_schema = suggest_schema_expectation(df)
    data_reference = capture_data_reference(df)
    numeric_stats = NumericAnalyzer.analyze(df)
    print("DEBUG - numeric_stats:", numeric_stats)
    categorical_stats = CategoricalAnalyzer.analyze(df)
    completeness_score, confidence_score = QualityScorer.score(df, numeric_stats)

    confidence_interval_global = None
    if numeric_stats:
        all_means = []
        for stat in numeric_stats.values():
            mean = stat.get("mean")
            if mean is not None:
                all_means.append(mean)
        if all_means:
            arr = np.array(all_means)
            ci_low, ci_high = compute_confidence_interval(pl.Series("", arr))
            confidence_interval_global = [ci_low, ci_high]

    sampling_warning = ""
    if sampling_info and "reasoning" in sampling_info and isinstance(sampling_info["reasoning"], dict):
        if "evidently" in sampling_info["reasoning"]:
            sampling_info["representativity_check"] = sampling_info["reasoning"]["evidently"]
            if sampling_info["reasoning"]["evidently"].get("success") is False:
                sampling_warning = "Sample tidak representatif untuk beberapa kolom (lihat reasoning.evidently.summary)."
        if "sampling_warning" in sampling_info["reasoning"]:
            sampling_warning += "\n" + sampling_info["reasoning"]["sampling_warning"]

    status_monitor = []
    if progress and isinstance(progress, dict) and "status_monitor" in progress:
        status_monitor = progress["status_monitor"]

    df_pd = df.to_pandas()

    result = {
        "file": file,
        "file_id": file_id,
        "total_rows": n,
        "columns": list(df.columns),
        "columns_count": len(df.columns),
        "duplicate_rows": int(df_pd.duplicated().sum()),
        "duplicate_pct": round(df_pd.duplicated().mean() * 100, 2),
        "missing_per_col": df_pd.isnull().sum().to_dict(),
        "missing_pct_per_col": (df_pd.isnull().mean() * 100).round(2).to_dict(),
        "numeric": numeric_stats,
        "categorical": categorical_stats,
        "completeness_score": completeness_score,
        "confidence_score": confidence_score,
        "confidence_interval": confidence_interval_global,
        "auto_schema_expectation": auto_schema,
        "data_reference": data_reference,
        "sampling": sampling_info or {},
        "sampling_warning": sampling_warning.strip(),
        "meta": meta,
        "progress": progress,
        "status_monitor": status_monitor,
        "generated_at": datetime.datetime.now().isoformat()
    }

    # --- ADVANCED ENRICHMENT ---
    try:
        from .data_drift import compute_stats, load_baseline_stats, detect_drift, save_baseline_stats
        baseline_path = os.path.join(DATA_LOG_DIR, "baseline_stats.json")
        current_stats = compute_stats(df_pd)
        baseline = load_baseline_stats(baseline_path=baseline_path)
        drift_threshold = get_conf('eda', 'drift_threshold', default=0.15)
        if baseline:
            drift_alerts = detect_drift(current_stats, baseline, threshold=drift_threshold)
        else:
            save_baseline_stats(current_stats, baseline_path=baseline_path)
            drift_alerts = []
        result['drift_alerts'] = drift_alerts
    except Exception as e:
        result['drift_alerts'] = [f"Drift detection error: {e}"]

    try:
        from .feature_interaction import analyze_feature_interaction
        corr_threshold = get_conf('eda', 'corr_threshold', default=0.7)
        vif_threshold = get_conf('eda', 'vif_threshold', default=5.0)
        feature_interaction = analyze_feature_interaction(df_pd)
        feature_interaction['high_correlation'] = [
            p for p in feature_interaction.get('high_correlation', [])
            if p['corr'] > corr_threshold
        ]
        feature_interaction['high_vif'] = [
            v for v in feature_interaction.get('high_vif', [])
            if v['vif'] > vif_threshold
        ]
        result['feature_interaction'] = feature_interaction
    except Exception as e:
        result['feature_interaction'] = {"error": str(e)}

    try:
        from .timeseries_profile import analyze_timeseries
        result['timeseries_profile'] = analyze_timeseries(df_pd)
    except Exception as e:
        result['timeseries_profile'] = {"error": str(e)}

    try:
        from .plugins.plugin_registry import run_plugins
        result['plugin_results'] = run_plugins(df_pd)
    except Exception as e:
        result['plugin_results'] = {"error": str(e)}

    try:
        ge_suite_export = get_conf('eda', 'export_ge_suite', default=False)
        if ge_suite_export:
            from .ge_suite_tools import suggest_schema_to_ge_suite, save_ge_suite
            suite = suggest_schema_to_ge_suite(auto_schema)
            suite_path = os.path.join(DATA_LOG_DIR, "auto_suite.json")
            save_ge_suite(suite, suite_path)
            result['ge_suite_path'] = suite_path
    except Exception as e:
        result['ge_suite_path'] = f"GE Suite Export error: {e}"

    if profiling_dict is not None:
        result["profile"] = profiling_dict
    if ge_result is not None:
        result["ge"] = ge_result
    if kolaborasi_summary is not None:
        result["rangkuman_kolaborasi"] = kolaborasi_summary

    try:
        analyzer = EDAReasonerFlexible(df_pd)
        eda_insight = analyzer.get_insight()
        result["eda_insight"] = eda_insight
        if isinstance(eda_insight, dict) and "reasoning" in eda_insight:
            result["reasoning_trace"] = eda_insight["reasoning"]
    except Exception as e:
        result["eda_insight"] = {
            "insight": f"EDA insight enrichment error: {e}",
            "recommendations": []
        }

    if sampling_info and isinstance(sampling_info.get("reasoning"), (dict, list)):
        result["sampling_reasoning"] = sampling_info["reasoning"]

    try:
        audit_result = audit_testing_auto(status_monitor, result)
        result["audit_testing"] = audit_result
    except Exception as e:
        result["audit_testing"] = {
            "score": 0,
            "evidence": {"error": str(e)},
            "comment": "Audit testing gagal dijalankan."
        }

    # PATCH: Guarantee core metrics never null/None before save/return
    result = patch_metrics(result)

    try:
        basename = os.path.splitext(os.path.basename(file))[0]
        log_file_id = file_id or "unknownid"
        log_filename = f"{log_file_id}_{basename}_eda_log.json"
        fpath = os.path.join(DATA_LOG_DIR, log_filename)
        log_content = {
            "file": file,
            "file_id": file_id,
            "run_time": datetime.datetime.utcnow().isoformat(),
            "eda_result": clean_json(result),
            "completeness_score": result["completeness_score"],
            "confidence_score": result["confidence_score"],
            "confidence_interval": result["confidence_interval"],
            "duplicate_pct": result["duplicate_pct"],
            "anomaly": {
                "outlier_frac": [
                    stat.get("outlier_count", 0) / n if n else 0
                    for stat in numeric_stats.values()
                ] if numeric_stats else []
            },
            "scores": {
                "completeness": result["completeness_score"],
                "confidence": result["confidence_score"],
                "confidence_interval": result["confidence_interval"]
            },
            "auto_schema": auto_schema,
            "profile": profiling_dict,
            "ge": ge_result,
            "insight": result.get("eda_insight"),
            "reasoning_trace": result.get("reasoning_trace"),
            "sampling_reasoning": result.get("sampling_reasoning"),
            "drift_alerts": result.get("drift_alerts"),
            "feature_interaction": result.get("feature_interaction"),
            "timeseries_profile": result.get("timeseries_profile"),
            "plugin_results": result.get("plugin_results"),
            "sampling_info": sampling_info,
            "audit_testing": result.get("audit_testing"),
            "status_monitor": status_monitor,
            "status": result.get("status")
        }
        save_eda_log(DATA_LOG_DIR, (file_id, file), log_content, log_type="eda")
    except Exception as e:
        print(f"[EDA_LOG_ERROR] {e}")

    try:
        eda_insight = result.get("eda_insight")
        save_eda_summary(
            eda_result_dir=EDA_RESULT_DIR,
            file_id=file_id,
            file_name=file,
            ydata_summary=profiling_dict,
            ge_summary=ge_result,
            eda_insight=eda_insight
        )
    except Exception as e:
        print(f"[EDA_SUMMARY_ERROR] {e}")

    return result

def batch_eda_on_folder(
    data_dir: str,
    enrich_meta: bool = True,
    n_sample: int = None,
    frac: float = None,
    stratify_col: str = None,
    weight_col: str = None
) -> List[dict]:
    from .utils import get_all_parquet_files, enrich_files_with_metadata
    from .sampling import get_file_sample_df

    files = get_all_parquet_files(data_dir)
    if enrich_meta:
        files = enrich_files_with_metadata(files)
    all_eda = []
    for f in files:
        try:
            fname = f["name"]
            meta = f.get("meta", {})
            progress = f.get("progress", {})
            sample_df, reasoning = get_file_sample_df(
                f["path"],
                n_sample=n_sample,
                frac=frac
            )
            if not isinstance(sample_df, pl.DataFrame):
                sample_df = pl.from_pandas(sample_df)
            sampling_info = {
                "sample_shape": sample_df.shape,
                "reasoning": reasoning,
                "stratify_col": stratify_col,
                "weight_col": weight_col,
                "frac": frac
            }
            eda = advanced_eda(
                sample_df,
                file=fname,
                meta=meta,
                progress=progress,
                sampling_info=sampling_info,
                file_id=f.get("id")
            )
            all_eda.append(eda)
        except Exception as e:
            err_content = {
                "error": str(e),
                "file": f.get("name"),
                "run_time": datetime.datetime.utcnow().isoformat()
            }
    return all_eda

def test_advanced_eda():
    df = pl.DataFrame({
        "cat": ["a", "a", "b", "b", "c"]*20,
        "num": np.arange(100),
        "dt": [datetime.datetime(2020, 1, 1) + datetime.timedelta(days=i) for i in range(100)],
        "mix": [None, "foo", 123, None, "bar"] * 20,
        "allnull": [None] * 100
    })
    res = advanced_eda(df, "dummy.parquet", file_id="testid123")
    assert isinstance(res, dict)
    assert "completeness_score" in res
    assert "confidence_score" in res
    assert "confidence_interval" in res
    assert "auto_schema_expectation" in res
    assert "data_reference" in res
    assert "mean_confidence_interval" in res["numeric"]["num"]
    assert "audit_testing" in res
    assert "status_monitor" in res
    print("advanced_eda() output:", res)

if __name__ == "__main__":
    test_advanced_eda()
    print("eda.py agentic patch tests OK")

6. eda_rule_flexible.py:

import polars as pl
import numpy as np
import re

# --- Tambahan library cerdas ---
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import statsmodels.api as sm
import scipy.stats as stats
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans
import shap
import xgboost as xgb
import lightgbm as lgb
from pyod.models.iforest import IForest
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
import pmdarima
import tsfresh
# import pandas_profiling  # Sudah dihapus sesuai permintaan

# Kamus keyword konteks bisnis
CONTEXT_KEYWORDS = {
    "hr": ["salary", "employee", "hire", "resign", "payroll", "dept", "position", "gender", "attrition", "leave", "absenteeism"],
    "finance": ["balance", "debt", "credit", "debit", "invoice", "cashflow", "revenue", "expense", "margin", "profit", "loss", "cost"],
    "inventory": ["stock", "inventory", "item", "warehouse", "restock", "sku", "quantity", "supply", "demand", "outofstock"],
    "project": ["project", "deadline", "milestone", "task", "progress", "owner", "completion", "overdue", "plan"],
    "sales": ["sales", "totalsales", "order", "customer", "region", "amount", "churn", "conversion", "retention"],
    "operation": ["shift", "operation", "machine", "downtime", "maintenance", "utilization", "output", "production"],
    "strategy": ["target", "kpi", "goal", "plan", "initiative", "objective", "achievement"],
    # Tambahkan lainnya sesuai kebutuhan
}

def is_numeric_polars_dtype(dtype):
    return dtype in [
        pl.Int8, pl.Int16, pl.Int32, pl.Int64,
        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
        pl.Float32, pl.Float64
    ]

def detect_context(df: pl.DataFrame):
    context_score = {k: 0 for k in CONTEXT_KEYWORDS}
    for col in df.columns:
        for ctx, keywords in CONTEXT_KEYWORDS.items():
            if any(kw in col.lower() for kw in keywords):
                context_score[ctx] += 1
    best_context = max(context_score.items(), key=lambda x: x[1])[0]
    return best_context, context_score

class EDAReasonerFlexible:
    def __init__(self, df: pl.DataFrame, feedback_path: str = "eda_feedback.txt"):
        self.df = df
        self.insights = []
        self.recommendations = []
        self.feedback_path = feedback_path
        self.feedback_rules = self.load_feedback()
        self.context, self.context_score = detect_context(df)

    def load_feedback(self):
        try:
            with open(self.feedback_path, "r", encoding="utf-8") as f:
                return [line.strip() for line in f if line.strip() and not line.strip().startswith("#")]
        except FileNotFoundError:
            return []

    def is_context(self, key):
        return self.context == key or self.context_score.get(key, 0) > 0

    def missing_ratio(self, col):
        return self.df[col].null_count() / self.df.height if self.df.height else 0

    def outlier_ratio(self, col):
        ser = np.array(self.df[col].drop_nulls().to_list())
        if len(ser) < 5:
            return 0
        q1 = np.percentile(ser, 25)
        q3 = np.percentile(ser, 75)
        iqr = q3 - q1
        lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr
        return float(((ser < lower) | (ser > upper)).mean())

    def kolom_imbalance(self, col):
        ser = self.df[col].drop_nulls().to_list()
        if not ser:
            return False
        vals, counts = np.unique(ser, return_counts=True)
        if len(counts) == 0:
            return False
        return (counts.max() / sum(counts)) > 0.9

    def max_value(self, col):
        ser = self.df[col].drop_nulls()
        return float(ser.max()) if ser.len() > 0 else None

    def min_value(self, col):
        ser = self.df[col].drop_nulls()
        return float(ser.min()) if ser.len() > 0 else None

    def num_unique(self, col):
        ser = self.df[col]
        return int(ser.n_unique())

    def corr(self, col1, col2):
        try:
            arr1 = np.array(self.df[col1].drop_nulls().to_list())
            arr2 = np.array(self.df[col2].drop_nulls().to_list())
            mask = ~np.isnan(arr1) & ~np.isnan(arr2)
            arr1, arr2 = arr1[mask], arr2[mask]
            if len(arr1) < 2:
                return 0
            return float(abs(np.corrcoef(arr1, arr2)[0, 1]))
        except Exception:
            return 0

    def regression_coef(self, target, col):
        try:
            df_nonull = self.df.select([col, target]).drop_nulls()
            X = np.array(df_nonull[col].to_list()).reshape(-1, 1)
            y = np.array(df_nonull[target].to_list())
            if len(X) < 2:
                return 0
            model = LinearRegression().fit(X, y)
            return float(model.coef_[0])
        except Exception:
            return 0

    def get_numeric_cols(self):
        return [col for col in self.df.columns if is_numeric_polars_dtype(self.df[col].dtype)]

    def get_categorical_cols(self):
        return [col for col in self.df.columns if self.df[col].dtype == pl.String]

    def get_possible_target(self):
        candidates = ["churn", "target", "label", "attrition", "sales", "revenue"]
        for c in candidates:
            for col in self.df.columns:
                if c in col.lower():
                    return col
        return None

    def parse_natural_language_rule(self, line):
        rule = line
        if "lebih dari 50% missing" in line:
            rule = "IF missing_ratio('{col}') > 0.5 THEN recommend(\"Drop kolom '{col}' karena lebih dari 50% data hilang.\")"
        elif "proporsi satu kelas di target lebih dari 80%" in line or "sangat imbalance" in line or "sangat timpang" in line:
            rule = "IF kolom_imbalance('{col}') THEN insight(\"Kolom '{col}' sangat imbalance, perlu balancing.\")"
        elif "semua nilainya unik" in line:
            rule = "IF num_unique('{col}') == self.df.height THEN recommend(\"Kolom '{col}' kemungkinan ID unik, tidak relevan untuk modelling.\")"
        elif "banyak outlier" in line:
            rule = "IF outlier_ratio('{col}') > 0.1 THEN recommend(\"Kolom '{col}' mengandung banyak outlier.\")"
        elif "dua fitur sangat berkorelasi" in line or "fitur sangat berkorelasi" in line:
            rule = "IF corr('{col1}', '{col2}') > 0.9 THEN recommend(\"Cek dengan user bisnis untuk '{col1}' dan '{col2}' yang sangat berkorelasi.\")"
        elif "fitur sangat mempengaruhi target" in line:
            tgt = self.get_possible_target() or "target"
            rule = f"IF abs(regression_coef('{tgt}', '{{col}}')) > 1.0 THEN recommend(\"Fitur '{{col}}' sangat mempengaruhi target {tgt}.\")"
        elif "salary di bawah minimum regional" in line:
            rule = "IF min_value('salary') < 2000000 THEN recommend(\"Salary di bawah UMR, cek kebijakan HR.\")"
        elif "stock negatif" in line:
            rule = "IF min_value('stock') < 0 THEN insight(\"Ada stok negatif, cek integritas data inventory.\")"
        elif "balance negatif" in line:
            rule = "IF min_value('balance') < 0 THEN insight(\"Balance negatif ditemukan, cek transaksi dan integritas keuangan.\")"
        elif "deadline project lewat" in line:
            rule = "IF min_value('progress') < 100 and min_value('deadline') < 0 THEN recommend(\"Ada project lewat deadline dengan progress < 100%, cek manajemen project.\")"
        return rule

    def _apply_feedback_rules(self):
        rules = []
        for l in self.feedback_rules:
            rules.append(self.parse_natural_language_rule(l))
        self.feedback_rules = rules

        context = {
            "missing_ratio": self.missing_ratio,
            "outlier_ratio": self.outlier_ratio,
            "kolom_imbalance": self.kolom_imbalance,
            "max_value": self.max_value,
            "min_value": self.min_value,
            "corr": self.corr,
            "regression_coef": self.regression_coef,
            "num_unique": self.num_unique,
            "self": self,
            "len": len,
        }
        for rule in self.feedback_rules:
            m = re.match(r"IF (.+) THEN (.+)", rule, re.IGNORECASE)
            if m:
                cond, action = m.group(1), m.group(2)
                if "{col}" in rule:
                    for col in self.df.columns:
                        cond_eval = cond.replace("{col}", f"'{col}'")
                        try:
                            if eval(cond_eval, {}, context):
                                self._do_action(action, col=col)
                        except Exception:
                            pass
                elif "{col1}" in rule and "{col2}" in rule:
                    for col1 in self.df.columns:
                        for col2 in self.df.columns:
                            if col1 == col2: continue
                            cond_eval = cond.replace("{col1}", f"'{col1}'").replace("{col2}", f"'{col2}'")
                            try:
                                if eval(cond_eval, {}, context):
                                    self._do_action(action, col1=col1, col2=col2)
                            except Exception:
                                pass
                else:
                    try:
                        if eval(cond, {}, context):
                            self._do_action(action)
                    except Exception:
                        pass
            else:
                self._do_action(rule)

    def _do_action(self, action, **kwargs):
        if action.startswith("recommend("):
            msg = action[len("recommend("):-1].format(**kwargs)
            if msg not in self.recommendations:
                self.recommendations.append(msg)
        elif action.startswith("insight("):
            msg = action[len("insight("):-1].format(**kwargs)
            if msg not in self.insights:
                self.insights.append(msg)
        elif action.startswith("ignore_outlier("):
            col = action[len("ignore_outlier("):-1].replace("'", "")
            self.insights = [i for i in self.insights if not (col in i and "outlier" in i.lower())]
            self.recommendations = [r for r in self.recommendations if not (col in r and "outlier" in r.lower())]

    # --- Advanced plug-in for super-insightful EDA ---
    def advanced_stat_ml_plugins(self):
        # Outlier detection (IsolationForest)
        for col in self.get_numeric_cols():
            arr = np.array(self.df[col].drop_nulls().to_list())
            if len(arr) > 10:
                try:
                    iso = IsolationForest(contamination=0.1, random_state=0)
                    vals = arr.reshape(-1, 1)
                    preds = iso.fit_predict(vals)
                    ratio = (preds == -1).mean()
                    if ratio > 0.05:
                        self.insights.append(f"Deteksi multivariate outlier pada '{col}' (IsolationForest, {ratio:.1%} outlier).")
                        self.recommendations.append(f"Investigasi baris outlier pada '{col}'.")
                except Exception:
                    pass

        # Feature importance - XGBoost atau LightGBM (jika target numerik dan cukup data)
        target_col = self.get_possible_target()
        if target_col and is_numeric_polars_dtype(self.df[target_col].dtype) and self.df.height > 30:
            try:
                num_cols = [col for col in self.get_numeric_cols() if col != target_col]
                X = np.array(self.df.select(num_cols).drop_nulls().to_numpy())
                y = np.array(self.df[target_col].drop_nulls().to_list())
                # Sinkronisasi index X dan y
                if len(y) != len(X):
                    minlen = min(len(y), len(X))
                    X = X[:minlen]
                    y = y[:minlen]
                model = xgb.XGBRegressor(n_estimators=50, max_depth=3, verbosity=0)
                model.fit(X, y)
                importances = model.feature_importances_
                idx = np.argsort(importances)[::-1]
                for i in idx[:3]:
                    col = num_cols[i]
                    score = importances[i]
                    self.insights.append(f"XGBoost: '{col}' fitur paling pengaruh ke '{target_col}' (FI={score:.2f}).")
                    self.recommendations.append(f"Pastikan kualitas data '{col}', karena sangat mempengaruhi prediksi {target_col}.")
            except Exception:
                pass

        # Cluster detection (KMeans)
        try:
            num_cols = self.get_numeric_cols()
            arr = np.array(self.df.select(num_cols).drop_nulls().to_numpy())
            if arr.shape[1] > 1 and arr.shape[0] > 10:
                scaler = StandardScaler()
                Xs = scaler.fit_transform(arr)
                inertia = []
                for k in range(2, 5):
                    km = KMeans(n_clusters=k, random_state=0, n_init=10)
                    km.fit(Xs)
                    inertia.append(km.inertia_)
                if inertia and (inertia[0] - inertia[-1]) / inertia[0] > 0.2:
                    self.insights.append("Teridentifikasi potensi klaster pada data, lakukan segmentasi lanjut untuk strategi bisnis.")
                    self.recommendations.append("Gunakan hasil cluster untuk rekomendasi program segmented.")
        except Exception:
            pass

        # Visualisasi missing value (optional, tidak output gambar, hanya insight)
        miss_cols = [col for col in self.df.columns if self.missing_ratio(col) > 0.2]
        if miss_cols:
            self.insights.append(f"Visualisasi missingno: Kolom dengan missing value tinggi: {miss_cols}")

        # Time series: deteksi tren musiman (pmdarima, tsfresh, hanya insight, tidak prediksi)
        for col in self.get_numeric_cols():
            if "date" in col.lower() or "period" in col.lower() or "month" in col.lower():
                date_col = col
                for num_col in self.get_numeric_cols():
                    if num_col == date_col:
                        continue
                    try:
                        ts_df = self.df.select([date_col, num_col]).drop_nulls().sort(date_col)
                        ts_np = np.array(ts_df[num_col].to_list())
                        if len(ts_np) > 20:
                            from statsmodels.tsa.seasonal import STL
                            res = STL(ts_np, period=12, robust=True).fit()
                            if (res.seasonal.std() > 0.05 * ts_np.std()):
                                self.insights.append(f"Deteksi pola seasonality pada '{num_col}' berdasarkan waktu '{date_col}'.")
                                self.recommendations.append(f"Gunakan model time series musiman untuk '{num_col}'.")
                    except Exception:
                        pass

    def get_insight(self):
        self.insights = []
        self.recommendations = []

        # --- EDA default, context-aware, presisi ---
        for col in self.df.columns:
            ratio = self.missing_ratio(col)
            if ratio > 0.2:
                self.insights.append(f"Kolom '{col}' memiliki {ratio:.1%} missing value.")
                self.recommendations.append(f"Imputasi atau investigasi missing value di '{col}'.")

            # Numeric: outlier
            if is_numeric_polars_dtype(self.df[col].dtype) and self.outlier_ratio(col) > 0.05:
                outlier_r = self.outlier_ratio(col)
                self.insights.append(f"Kolom '{col}' memiliki {outlier_r:.1%} outlier (IQR).")
                self.recommendations.append(f"Cek dan tangani outlier di '{col}'.")

            # Context: HR - salary
            if self.is_context("hr") and "salary" in col.lower():
                if self.min_value(col) is not None and self.min_value(col) < 2000000:
                    self.insights.append(f"Salary di bawah UMR terdeteksi di '{col}'.")
                    self.recommendations.append("Audit kebijakan HR untuk salary di bawah UMR.")

            # Context: inventory
            if self.is_context("inventory") and "stock" in col.lower():
                if self.min_value(col) is not None and self.min_value(col) < 0:
                    self.insights.append(f"Ada stok negatif di '{col}'.")
                    self.recommendations.append("Audit data dan SOP inventory.")

            # Context: finance
            if self.is_context("finance") and "balance" in col.lower():
                if self.min_value(col) is not None and self.min_value(col) < 0:
                    self.insights.append(f"Balance negatif ditemukan di '{col}'.")
                    self.recommendations.append("Audit transaksi keuangan dan integritas data.")

            # Imbalance
            if self.kolom_imbalance(col):
                self.insights.append(f"Kolom '{col}' sangat imbalance (dominan satu kelas/kategori).")
                self.recommendations.append(f"Pertimbangkan balancing atau pengelompokan ulang pada '{col}'.")

            # Unik
            if self.num_unique(col) == self.df.height:
                self.insights.append(f"Kolom '{col}' kemungkinan ID unik, tidak relevan untuk modeling.")
                self.recommendations.append(f"Drop '{col}' dari feature engineering.")

        # Cross-feature: correlation
        num_cols = self.get_numeric_cols()
        for i, col1 in enumerate(num_cols):
            for col2 in num_cols[i+1:]:
                corr_val = self.corr(col1, col2)
                if corr_val > 0.9:
                    self.insights.append(f"'{col1}' dan '{col2}' sangat berkorelasi (corr={corr_val:.2f}).")
                    self.recommendations.append(f"Pertimbangkan drop salah satu: '{col1}' atau '{col2}', konsultasi domain expert.")

        # Fitur penting ke target
        target_col = self.get_possible_target()
        if target_col:
            for col in num_cols:
                if col == target_col:
                    continue
                coef = self.regression_coef(target_col, col)
                if abs(coef) > 1.0:
                    self.insights.append(f"Fitur '{col}' sangat mempengaruhi target '{target_col}' (koefisien={coef:.2f}).")
                    self.recommendations.append(f"Prioritaskan kualitas data '{col}' dan cek potensi leakage.")

        # Jalankan bank pengetahuan (feedback rules)
        self._apply_feedback_rules()

        # Jalankan advanced ML/statistik plug-in
        self.advanced_stat_ml_plugins()

        # Dedup, sort by relevansi (panjang kalimat/unik), batasi 10 per kategori
        self.insights = sorted(list(set(self.insights)), key=lambda x: -len(x))[:10]
        self.recommendations = sorted(list(set(self.recommendations)), key=lambda x: -len(x))[:10]

        if not self.insights:
            self.insights = ["Tidak ditemukan masalah kualitas data yang signifikan."]
        if not self.recommendations:
            self.recommendations = ["Data siap untuk tahap selanjutnya."]
        return {
            "context": self.context,
            "insight": self.insights,
            "recommendations": self.recommendations
        }

# Contoh penggunaan
if __name__ == "__main__":
    df = pl.DataFrame({
        "salary": [1_500_000, 2_500_000, 3_000_000, 1_800_000, 2_800_000, 3_200_000],
        "stock": [10, 0, 5, -2, 7, 8],
        "sales": [1000, 2000, 1900, 1850, 1780, 2100],
        "employee": ["A", "B", "C", "D", "E", "F"]
    })
    analyzer = EDAReasonerFlexible(df, feedback_path="eda_feedback.txt")
    result = analyzer.get_insight()
    import json
    print(json.dumps(result, indent=2, ensure_ascii=False))

7. eda_utils.py:

import os
import datetime
import json
import polars as pl

# --- REVISED: Remove auto-create of data_eda directory ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_EDA_DIR = os.path.join(BASE_DIR, "data_eda")
# os.makedirs(DATA_EDA_DIR, exist_ok=True)  # DIHAPUS agar tidak auto create folder

LOG_DIR = DATA_EDA_DIR  # All logs go to data_eda

def log_reasoning(event: dict, log_name: str = "eda_reasoning_log.json"):
    """
    Log reasoning events as JSON lines in the data_eda folder.
    """
    log_path = os.path.join(LOG_DIR, log_name)
    event['timestamp'] = datetime.datetime.utcnow().isoformat()
    if os.path.exists(log_path):
        try:
            with open(log_path, "r", encoding="utf-8") as f:
                logs = json.load(f)
            if not isinstance(logs, list):
                logs = []
        except Exception:
            logs = []
    else:
        logs = []
    logs.append(event)
    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(logs, f, indent=2, ensure_ascii=False)

def capture_data_reference(df, cols=None, n=10):
    """
    Ambil data referensi (contoh value, histogram, dsb) untuk kebutuhan agentic GE.
    Kompatibel untuk polars DataFrame.
    """
    ref = {}
    if cols is None:
        cols = df.columns
    for col in cols:
        ser = df[col]
        # Numeric
        if ser.dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Float32, pl.Float64]:
            arr = ser.drop_nulls().to_list()
            ref[col] = {
                "min": float(ser.min()) if arr else None,
                "max": float(ser.max()) if arr else None,
                "sample": ser.drop_nulls().sample(n=min(n, len(arr))).to_list() if arr else [],
            }
        else:
            # String/category/other
            ref[col] = {
                "value_sample": ser.drop_nulls().unique()[:n].to_list()
            }
    return ref

def explain_expectation_applied(col, col_type, rule, params, evidence=None):
    """
    Generate reasoning string for why a rule is applied.
    """
    reason = f"Expectation '{rule}' applied to column '{col}' (type: {col_type}) with params {params}."
    if evidence:
        reason += f" Evidence: {evidence}"
    return reason

8. feature_interaction.py:

import polars as pl
import numpy as np

def correlation_report(df: pl.DataFrame, threshold=0.7):
    """
    Mendapatkan pasangan kolom dengan korelasi Pearson absolut di atas threshold.
    """
    # Hanya kolom numerik
    num_cols = [col for col in df.columns if pl.datatypes.is_numeric(df[col].dtype)]
    if not num_cols or len(num_cols) < 2:
        return []
    corr_df = df.select(num_cols).corr(method="pearson").abs()
    pairs = []
    # corr_df adalah polars DataFrame, ubah ke numpy array untuk efisiensi
    corr_np = corr_df.to_numpy()
    for i, col1 in enumerate(num_cols):
        for j in range(i + 1, len(num_cols)):
            col2 = num_cols[j]
            val = corr_np[i, j]
            if val > threshold:
                pairs.append({"pair": (col1, col2), "corr": float(val)})
    return pairs

def vif_report(df: pl.DataFrame, thresh=5.0):
    """
    Mendapatkan kolom dengan VIF (Variance Inflation Factor) di atas threshold.
    """
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    num_cols = [col for col in df.columns if pl.datatypes.is_numeric(df[col].dtype)]
    X = df.select(num_cols).drop_nulls()
    vifs = []
    if X.width > 1:
        X_np = np.array(X.to_numpy())
        for i, col in enumerate(num_cols):
            try:
                vif = variance_inflation_factor(X_np, i)
                if vif > thresh:
                    vifs.append({"column": col, "vif": float(vif)})
            except Exception:
                continue
    return vifs

def analyze_feature_interaction(df: pl.DataFrame, sample_if_large=True, sample_size=1000):
    """
    Analisis interaksi fitur (korelasi tinggi dan VIF) pada DataFrame.
    Akan otomatis sampling jika data sangat besar dan sample_if_large=True.
    """
    # --- Integrasi dengan sampling.py: sampling otomatis jika data besar ---
    if sample_if_large and df.height > sample_size:
        try:
            from .sampling import get_file_sample_df
            # Sampling hanya pada data numerik (agar efisien)
            num_cols = [col for col in df.columns if pl.datatypes.is_numeric(df[col].dtype)]
            if num_cols:
                tmp_df = df.select(num_cols)
                # Simulasi sampling dengan polars: random sampling pada numeric
                # Sampling, tanpa stratifikasi (karena hanya numerik)
                # get_file_sample_df expects fpath, but for in-memory we just sample
                sampled_df = tmp_df.sample(n=sample_size, seed=42)
                df = sampled_df
        except Exception:
            # Jika sampling.py tidak tersedia, fallback: sample biasa jika data besar
            df = df.sample(n=sample_size, seed=42)
    return {
        "high_correlation": correlation_report(df),
        "high_vif": vif_report(df)
    }

9. timeseries_profile.py:

import os
import polars as pl
import numpy as np

def is_numeric_polars_dtype(dtype):
    return dtype in [
        pl.Int8, pl.Int16, pl.Int32, pl.Int64,
        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
        pl.Float32, pl.Float64
    ]

def is_datetime_polars_dtype(dtype):
    return dtype in [pl.Date, pl.Datetime]

def find_timeseries_columns(df: pl.DataFrame):
    """
    Temukan kolom bertipe datetime di DataFrame.
    """
    dt_cols = [col for col in df.columns if is_datetime_polars_dtype(df[col].dtype)]
    return dt_cols

def seasonal_decompose_report(df: pl.DataFrame, col: str, period: int = 12):
    """
    Laporan dekomposisi musiman pada satu kolom time series.
    """
    from statsmodels.tsa.seasonal import seasonal_decompose
    results = {}
    # Cari kolom numerik selain datetime
    num_cols = [c for c in df.columns if is_numeric_polars_dtype(df[c].dtype) and c != col]
    if not num_cols:
        return {"error": "Tidak ada kolom numerik untuk dekomposisi musiman"}
    target_col = num_cols[0]
    df_sorted = df.sort(col)
    dates = df_sorted[col].to_numpy()
    values = df_sorted[target_col].to_numpy()
    # Buat pandas Series
    import pandas as pd
    ser = pd.Series(values, index=pd.to_datetime(dates))
    ser = ser.dropna().sort_index()
    if len(ser) < period * 2:
        return {"error": "Terlalu sedikit data untuk dekomposisi musiman"}
    try:
        result = seasonal_decompose(ser, period=period, model='additive')
        results["trend"] = result.trend.describe().to_dict() if hasattr(result.trend, "describe") else {}
        results["seasonal_strength"] = float(np.std(result.seasonal)) / np.std(ser)
        results["resid_std"] = float(np.std(result.resid))
    except Exception as e:
        results["error"] = str(e)
    return results

def autocorrelation_report(df: pl.DataFrame, col: str, lags=20):
    """
    Laporan autokorelasi (ACF) pada kolom time series.
    """
    from statsmodels.tsa.stattools import acf
    # Cek kolom numerik selain datetime
    num_cols = [c for c in df.columns if is_numeric_polars_dtype(df[c].dtype) and c != col]
    if not num_cols:
        return {"error": "Tidak ada kolom numerik untuk ACF"}
    target_col = num_cols[0]
    import pandas as pd
    values = df.sort(col)[target_col].to_numpy()
    ser = pd.Series(values)
    ser = ser.dropna()
    if len(ser) < lags+1:
        return {"error": "Data terlalu pendek untuk ACF"}
    acfs = acf(ser, nlags=lags)
    return {"acf": acfs.tolist()}

def gap_analysis(df: pl.DataFrame, dt_col: str):
    """
    Analisis gap antar waktu pada kolom datetime.
    """
    import pandas as pd
    ser = pd.to_datetime(df[dt_col].to_numpy())
    ser = pd.Series(ser).dropna().sort_values()
    gaps = ser.diff().dt.total_seconds().dropna()
    large_gaps = gaps[gaps > gaps.median() * 2]
    return {
        "gap_median_sec": float(gaps.median()) if len(gaps) > 0 else 0.0,
        "num_large_gaps": int((gaps > gaps.median() * 2).sum()) if len(gaps) > 0 else 0,
        "large_gaps": large_gaps.tolist()[:10] if len(large_gaps) > 0 else []
    }

##############################
# Integrasi sampling.py
##############################
def analyze_timeseries(
    df: pl.DataFrame,
    sample_if_large: bool = True,
    sample_size: int = 500,
    use_sampling_py: bool = True
):
    """
    Analisis timeseries pada DataFrame. Jika data besar, otomatis sampling menggunakan sampling.py jika tersedia.
    """
    # --- Sampling jika data besar ---
    if sample_if_large and df.height > sample_size:
        try:
            if use_sampling_py:
                from .sampling import get_file_sample_df
                # Sampling tanpa stratifikasi (karena ini timeseries)
                sampled_df, _ = get_file_sample_df(
                    fpath=None,
                    n_sample=sample_size,
                    frac=None,
                    stratify_col=None,
                    balanced_col=None,
                    cluster_cols=None,
                    random_state=42,
                    validate=False
                )
                df = sampled_df
            else:
                df = df.sample(sample_size, seed=42)
        except Exception:
            df = df.sample(sample_size, seed=42)
    dt_cols = find_timeseries_columns(df)
    reports = {}
    for col in dt_cols:
        reports[col] = {
            "gap_analysis": gap_analysis(df, col),
            "autocorrelation": autocorrelation_report(df, col),
            "seasonal_decompose": seasonal_decompose_report(df, col)
        }
    return reports

##############################
# Unit Test
##############################
def test_timeseries_profile():
    import pandas as pd
    # Buat data dummy dengan datetime dan tren
    dates = pd.date_range("2022-01-01", periods=120, freq="M")
    vals = np.sin(np.arange(120) / 12 * 2 * np.pi) + np.random.normal(0, 0.1, 120)
    df = pl.DataFrame({"ts": dates, "y": vals})
    res = analyze_timeseries(df)
    print("Timeseries profile:", res)
    assert isinstance(res, dict)
    print("timeseries_profile.py tests OK")

if __name__ == "__main__":
    test_timeseries_profile()

10. data_drift.py:

import os
import json
import polars as pl
import numpy as np

# --- REVISED: Save/load baseline_stats.json in data_log ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_LOG_DIR = os.path.join(BASE_DIR, "data_log")
# PATCH: Tidak ada auto-create folder, hanya cek eksistensi
if not os.path.exists(DATA_LOG_DIR):
    raise FileNotFoundError(f"Target folder does not exist: {DATA_LOG_DIR}")
BASELINE_PATH = os.path.join(DATA_LOG_DIR, "baseline_stats.json")

def is_numeric_polars_dtype(dtype):
    return dtype in [
        pl.Int8, pl.Int16, pl.Int32, pl.Int64,
        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
        pl.Float32, pl.Float64
    ]

def save_baseline_stats(stats: dict, baseline_path: str = BASELINE_PATH):
    """
    Simpan statistik baseline ke file JSON.
    """
    with open(baseline_path, "w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)

def load_baseline_stats(baseline_path: str = BASELINE_PATH):
    """
    Load statistik baseline dari file JSON.
    """
    if not os.path.exists(baseline_path):
        return None
    with open(baseline_path, "r", encoding="utf-8") as f:
        return json.load(f)

def compute_stats(df: pl.DataFrame):
    """
    Hitung statistik kolom numerik utama untuk deteksi drift.
    """
    stats = {}
    for col in df.columns:
        ser = df[col]
        if is_numeric_polars_dtype(ser.dtype):
            arr = np.array(ser.drop_nulls().to_list())
            if len(arr) == 0:
                continue
            mean = float(np.mean(arr))
            std = float(np.std(arr))
            minv = float(np.min(arr))
            maxv = float(np.max(arr))
            # Outlier = nilai lebih dari 3 std dari mean
            outlier_rate = float(((np.abs(arr - mean) > 3 * std).sum()) / len(arr)) if std > 0 else 0.0
            stats[col] = {
                "mean": mean,
                "std": std,
                "min": minv,
                "max": maxv,
                "outlier_rate": outlier_rate
            }
    return stats

def detect_drift(current_stats, baseline_stats, threshold=0.15):
    """
    Deteksi drift pada statistik kolom numerik berdasarkan baseline.
    threshold default: 15% perubahan relatif.
    """
    alerts = []
    for col in current_stats:
        if col in baseline_stats:
            for k in ["mean", "std", "outlier_rate"]:
                base = baseline_stats[col].get(k)
                curr = current_stats[col].get(k)
                if base is None or curr is None or base == 0:
                    continue
                delta = abs(curr - base) / abs(base)
                if delta > threshold:
                    alerts.append(
                        f"Drift pada kolom '{col}' ({k}): baseline={base:.3f}, sekarang={curr:.3f}, delta={delta:.2%}"
                    )
    return alerts

##############################
# Integrasi dengan sampling.py (pipeline EDA)
##############################
def compute_and_check_drift(
    df: pl.DataFrame,
    threshold=0.15,
    baseline_path: str = BASELINE_PATH,
    save_if_none=True
):
    """
    Hitung statistik data, cek drift terhadap baseline (jika ada).
    Jika baseline belum ada dan save_if_none=True, baseline akan dibuat dari data ini.
    Return: tuple (drift_alerts, current_stats, baseline_stats)
    """
    current_stats = compute_stats(df)
    baseline_stats = load_baseline_stats(baseline_path)
    if baseline_stats:
        drift_alerts = detect_drift(current_stats, baseline_stats, threshold=threshold)
    else:
        drift_alerts = []
        if save_if_none:
            save_baseline_stats(current_stats, baseline_path)
    return drift_alerts, current_stats, baseline_stats

# Example usage in EDA pipeline:
# drift_alerts, current_stats, baseline = compute_and_check_drift(df)

##############################
# Unit Test
##############################
def test_data_drift():
    df = pl.DataFrame({
        "A": np.random.normal(0, 1, 1000),
        "B": np.random.normal(10, 5, 1000)
    })
    # Save baseline
    stats = compute_stats(df)
    save_baseline_stats(stats)
    # Modify data for drift detection
    df2 = df.with_columns([
        (pl.col("A") + 1.5).alias("A")
    ])
    drift_alerts, cur_stats, base_stats = compute_and_check_drift(df2)
    print("Drift alerts:", drift_alerts)
    assert isinstance(drift_alerts, list)
    assert "Drift pada kolom 'A' (mean)" in " ".join(drift_alerts) or len(drift_alerts) > 0
    print("data_drift.py tests OK")

if __name__ == "__main__":
    test_data_drift()

11. schemas.py:

from pydantic import BaseModel
from typing import List, Dict, Optional, Any, Union

# --- Numeric Column Statistics ---
class NumericColStat(BaseModel):
    count: float
    mean: float
    std: float
    min: float
    q25: float
    q50: float
    q75: float
    max: float
    unique: int
    outlier_count: int
    hist_bins: List[float]
    hist_counts: List[int]
    skewness: float
    kurtosis: float
    zero_ratio: float
    neg_ratio: float
    pos_ratio: float
    mean_confidence_interval: Optional[List[float]] = None

# --- Categorical Column Statistics ---
class CategoricalColStat(BaseModel):
    unique: int
    top_freq: Dict[str, int]
    entropy: float
    rare_count: int
    rare_pct: float

# --- Schema/Expectations Auto-Suggestion ---
class ColumnSchemaExpectation(BaseModel):
    type: str
    min: Optional[Union[float, str]] = None
    max: Optional[Union[float, str]] = None
    n_unique: Optional[int] = None
    max_length: Optional[int] = None
    sample_values: Optional[List[Any]] = None

class AutoSchemaExpectation(BaseModel):
    columns: Dict[str, ColumnSchemaExpectation]

# --- Data Reference Example ---
class DataReference(BaseModel):
    min: Optional[float]
    max: Optional[float]
    sample: Optional[List[Any]]
    hist_bins: Optional[List[float]]
    hist_counts: Optional[List[int]]
    value_sample: Optional[List[Any]]

# --- Gemini Enrichment ---
class GeminiEnrichment(BaseModel):
    insight: Optional[Union[str, List[str]]] = None
    recommendations: Optional[List[str]] = None

# --- Audit Testing Result ---
class AuditTestingResult(BaseModel):
    score: float
    evidence: Optional[Dict[str, Any]] = None
    comment: Optional[str] = None

# --- Main EDA Result Schema ---
class EDAResult(BaseModel):
    file: str
    total_rows: int
    columns: List[str]
    columns_count: int
    duplicate_rows: int
    duplicate_pct: float
    missing_per_col: Dict[str, int]
    missing_pct_per_col: Dict[str, float]
    numeric: Dict[str, NumericColStat]
    categorical: Dict[str, CategoricalColStat]
    completeness_score: float
    confidence_score: float
    confidence_interval: Optional[List[float]] = None
    auto_schema_expectation: Optional[Dict[str, Any]] = None
    data_reference: Optional[Dict[str, Any]] = None
    sampling: Dict[str, Any]
    sampling_warning: Optional[str] = None
    generated_at: str
    meta: Optional[Dict[str, Any]] = None
    progress: Optional[Dict[str, Any]] = None

    # --- Optional: For advanced/LLM/GE/anomaly results ---
    profile: Optional[Any] = None
    ge: Optional[Any] = None
    anomaly: Optional[Any] = None
    warnings: Optional[Any] = None
    drift_alerts: Optional[Any] = None
    feature_interaction: Optional[Any] = None
    timeseries_profile: Optional[Any] = None
    plugin_results: Optional[Any] = None
    ge_suite_path: Optional[str] = None
    rangkuman_kolaborasi: Optional[Any] = None

    audit_testing: Optional[AuditTestingResult] = None

    # --- PATCH: EDA Reasoner Flexible (Context & Insight/Recommendation) ---
    context: Optional[str] = None
    eda_insight: Optional[Dict[str, Any]] = None

13. utils.py:

import os
import json
import hashlib
import math
import datetime
from typing import Any, Dict, Optional, List
import polars as pl
import numpy as np

from .eda_logger import save_eda_log  # Use best practice logger

# Define base and data directories (adjust as needed)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
META_FILE = os.path.join(DATA_DIR, "other_gdrive_meta.json")
PROGRESS_FILE = os.path.join(DATA_DIR, "file_progress.json")
EXCLUDED_FILES = {"other_gdrive_meta.json", "file_progress.json"}

# --- REVISED: data_eda/data_log directory is NOT auto-created anymore ---
DATA_LOG_DIR = os.path.join(BASE_DIR, "data_eda", "data_log")
# os.makedirs(DATA_LOG_DIR, exist_ok=True)  # <- DIHAPUS agar tidak auto create folder

def safe(val, default=None):
    """Return default if val is NaN, inf, None, empty, or null-like."""
    # --- Patch: handle numpy array (DeprecationWarning fix) ---
    if isinstance(val, np.ndarray):
        if val.size == 0:
            return default
        # For non-empty arrays, treat as value if needed
        if val.size == 1:
            return safe(val.item(), default)
        return val
    if isinstance(val, float) and (math.isnan(val) or math.isinf(val)):
        return default
    if isinstance(val, (np.floating, )):
        if np.isnan(val) or np.isinf(val):
            return default
    # Patch for polars nulls
    try:
        import pandas as pd
        if 'pd' in locals() and pd.isnull(val):
            return default
    except Exception:
        pass
    # polars null-like
    if val is None or val in ["", [], {}, "null", "None"]:
        return default
    return val

def load_json(path: str, default=None) -> Any:
    """Load JSON from file, return default on error."""
    if not os.path.exists(path):
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, path, {"error": f"load_json error {e}", "file": path}, log_type="utils_error")
        return default

def save_json(path: str, obj: Any):
    """Save object as JSON to file."""
    def clean_json(obj):
        if isinstance(obj, dict):
            return {str(k): clean_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [clean_json(v) for v in obj]
        elif isinstance(obj, float):
            if math.isnan(obj) or math.isinf(obj):
                return None
            return obj
        elif isinstance(obj, (np.integer, )):
            return int(obj)
        elif isinstance(obj, (np.floating, )):
            return float(obj)
        elif isinstance(obj, (datetime.datetime, datetime.date)):
            return str(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(clean_json(obj), f, indent=2, ensure_ascii=False)
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, path, {"error": f"save_json error {e}", "file": path}, log_type="utils_error")

def calc_sha256_from_file(path: str) -> str:
    """Calculate SHA256 hash of a file."""
    try:
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, path, {"error": f"calc_sha256_from_file error {e}", "file": path}, log_type="utils_error")
        return ""

def clean_json(obj):
    """Recursively clean object for JSON serialization."""
    if isinstance(obj, dict):
        return {str(k): clean_json(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_json(v) for v in obj]
    elif isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return None
        return obj
    elif isinstance(obj, (np.integer, )):
        return int(obj)
    elif isinstance(obj, (np.floating, )):
        return float(obj)
    elif isinstance(obj, (datetime.datetime, datetime.date)):
        return str(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    else:
        return obj

def parse_iso_to_local(dt_str: str) -> str:
    """Convert ISO string to local time ISO string."""
    if not dt_str or not isinstance(dt_str, str):
        return ""
    dt_str = dt_str.replace("+00:00Z", "Z").replace("Z+00:00", "Z")
    try:
        import pandas as pd
        if dt_str.endswith("Z"):
            dt_str = dt_str[:-1] + "+00:00"
        dt_utc = pd.to_datetime(dt_str, utc=True)
        dt_local = dt_utc.tz_convert(None).to_pydatetime().astimezone()
        return dt_local.replace(microsecond=0).isoformat()
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, "parse_iso_to_local", {"error": f"parse_iso_to_local error {e}", "value": dt_str}, log_type="utils_error")
        return ""

def parse_iso_to_utc(dt_str: str):
    """Convert ISO string to pandas UTC datetime."""
    if not dt_str or not isinstance(dt_str, str):
        return None
    dt_str = dt_str.replace("+00:00Z", "Z").replace("Z+00:00", "Z")
    try:
        import pandas as pd
        if dt_str.endswith("Z"):
            dt_str = dt_str[:-1] + "+00:00"
        return pd.to_datetime(dt_str, utc=True)
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, "parse_iso_to_utc", {"error": f"parse_iso_to_utc error {e}", "value": dt_str}, log_type="utils_error")
        return None

def get_all_parquet_files(data_dir=DATA_DIR) -> List[Dict]:
    """List all parquet files in data directory except excluded files."""
    files = []
    if not os.path.isdir(data_dir):
        return files
    for fname in os.listdir(data_dir):
        if fname in EXCLUDED_FILES or fname.startswith(".") or not fname.lower().endswith(".parquet"):
            continue
        fpath = os.path.join(data_dir, fname)
        if os.path.isfile(fpath):
            files.append({
                "name": fname,
                "path": fpath,
                "size": os.path.getsize(fpath),
                "modified": datetime.datetime.fromtimestamp(os.path.getmtime(fpath)).isoformat()
            })
    return files

def get_file_metadata():
    """Load and combine file metadata from meta and progress JSON files."""
    meta = load_json(META_FILE, default={})
    progress = load_json(PROGRESS_FILE, default={})
    meta_map = {}
    if isinstance(meta, dict) and "files" in meta:
        for f in meta["files"]:
            meta_map[f.get("name")] = f
    elif isinstance(meta, list):
        for f in meta:
            meta_map[f.get("name")] = f
    progress_map = {}
    if isinstance(progress, dict) and "progress" in progress:
        for f in progress["progress"]:
            progress_map[f.get("name")] = f
    elif isinstance(progress, list):
        for f in progress:
            progress_map[f.get("name")] = f
    return meta_map, progress_map

def enrich_files_with_metadata(files: List[Dict]):
    """Add meta and progress info to file dicts."""
    meta_map, progress_map = get_file_metadata()
    for f in files:
        fname = f["name"]
        f["meta"] = meta_map.get(fname, {})
        f["progress"] = progress_map.get(fname, {})
    return files

def get_df_metadata(df: pl.DataFrame) -> Dict[str, Any]:
    """Get dataframe metadata: dtype, min, max, null, unique, sample."""
    meta = {}
    for col in df.columns:
        ser = df[col]
        # --- PATCH Polars: gunakan cara yang benar untuk deteksi tipe numerik ---
        if ser.dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Float32, pl.Float64]:
            arr = ser.drop_nulls().to_list()
            minv = float(ser.min()) if arr else None
            maxv = float(ser.max()) if arr else None
        else:
            minv = None
            maxv = None
        meta[col] = {
            "type": str(ser.dtype),
            "min": minv,
            "max": maxv,
            "null_count": int(ser.null_count()),
            "nunique": int(ser.n_unique()),
            "sample": ser.drop_nulls().unique()[:5].to_list() if ser.drop_nulls().len() > 0 else [],
        }
    return meta

##############################
# Integrasi dengan sampling.py
##############################
def get_representative_sample(
    fpath: str,
    n_sample: int = 100,
    frac: float = None,
    stratify_col: str = None,
    balanced_col: str = None,
    cluster_cols: List[str] = None,
    n_clusters: int = 10,
    random_state: int = 42,
    validate: bool = True
):
    """
    Wrapper util untuk mengambil sample representatif dari file parquet menggunakan sampling.py.
    """
    try:
        from sampling import get_file_sample_df
        sample_df, info = get_file_sample_df(
            fpath=fpath,
            n_sample=n_sample,
            frac=frac,
            stratify_col=stratify_col,
            balanced_col=balanced_col,
            cluster_cols=cluster_cols,
            n_clusters=n_clusters,
            random_state=random_state,
            validate=validate
        )
        # Pastikan sample_df polars
        if not isinstance(sample_df, pl.DataFrame):
            import pandas as pd
            sample_df = pl.from_pandas(sample_df)
        return sample_df, info
    except Exception as e:
        save_eda_log(DATA_LOG_DIR, fpath, {"error": f"get_representative_sample error {e}", "file": fpath}, log_type="utils_error")
        return None, {"error": str(e)}

# --- Unit Tests (Run python utils.py for basic check) ---
def test_safe():
    assert safe(None, 0) == 0
    assert safe(float('nan'), 1) == 1
    assert safe("", 9) == 9
    assert safe(7, 9) == 7
    assert safe(np.array([]), 42) == 42
    assert safe(np.array([1.0]), 0) == 1.0

def test_load_json_and_save_json(tmp_path):
    test_path = tmp_path / "test.json"
    save_json(str(test_path), {"a": 1, "b": np.int64(5)})
    d = load_json(str(test_path))
    assert d == {"a": 1, "b": 5}

def test_get_df_metadata():
    df = pl.DataFrame({
        "num": [1, 2, 3, 4, 5, None],
        "cat": ["a", "b", "b", "c", "c", "c"]
    })
    meta = get_df_metadata(df)
    assert "num" in meta and "cat" in meta
    assert "num" in meta
    assert meta["num"]["type"].startswith("Int") or meta["num"]["type"].startswith("Float")
    assert "cat" in meta

if __name__ == "__main__":
    test_safe()
    import tempfile
    from pathlib import Path
    p = tempfile.TemporaryDirectory()
    test_load_json_and_save_json(Path(p.name))
    test_get_df_metadata()
    print("utils.py tests OK")

14. eda_logger.py:

import os
import json
import datetime

def get_eda_log_path(
    log_dir, 
    file_data, 
    log_type="eda", 
    suffix=None, 
    with_time=False
):
    """
    Generate log file path with clear naming:
    <file_id>_<basename>_eda_log.json   (if file_data is a tuple (file_id, filename))
    eda_log_<basename>_<YYYYMMDD>[__<suffix>].json (if file_data is string filename)
    If with_time=True, use eda_log_<basename>_<YYYYMMDD_%H%M%S>[__<suffix>].json
    For batch log (file_data = "batch_..."), the basename is batch_...
    """
    # Support file_data as (file_id, filename) tuple or just filename string
    if isinstance(file_data, tuple) and len(file_data) == 2:
        file_id, filename = file_data
        base = os.path.splitext(os.path.basename(filename))[0]
        fname = f"{file_id}_{base}_eda_log.json"
        return os.path.join(log_dir, fname)
    else:
        base = os.path.splitext(os.path.basename(file_data))[0]
        if with_time:
            date = datetime.datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        else:
            date = datetime.datetime.utcnow().strftime("%Y%m%d")
        suffix_part = f"__{suffix}" if suffix else ""
        fname = f"{log_type}_log_{base}_{date}{suffix_part}.json"
        return os.path.join(log_dir, fname)

def ensure_json_serializable(obj):
    """
    Ensure obj is JSON serializable, converting datetime/date/etc if needed.
    """
    def default(o):
        if isinstance(o, (datetime.datetime, datetime.date)):
            return o.isoformat()
        return str(o)
    try:
        # Try to dump and load to enforce JSON-compatibility
        return json.loads(json.dumps(obj, ensure_ascii=False, default=default))
    except Exception as e:
        return {"error": f"Object not serializable: {str(e)}", "original": str(obj)}

def save_eda_log(
    log_dir, 
    file_data, 
    log_content, 
    log_type="eda", 
    suffix=None, 
    with_time=False
):
    """
    Save EDA log or metadata log with best practice:
    - For per-file log: use file_data as (file_id, filename) for unique log per file.
    - For batch/error logs: use file_data as string, with with_time=True to avoid overwrite within a day.
    - Overwrites file per run for file log, or creates new file with timestamp for batch/error log.
    - REVISED: Do NOT create folder, only allow if folder already exists.
    """
    if not os.path.exists(log_dir):
        raise FileNotFoundError(f"Target folder does not exist: {log_dir}")
    log_path = get_eda_log_path(
        log_dir, file_data, log_type, suffix, with_time=with_time
    )
    serializable_log_content = ensure_json_serializable(log_content)
    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(serializable_log_content, f, indent=2, ensure_ascii=False)
    print(f"[LOG] Saved {log_type} log: {log_path}")
    return log_path

def save_eda_summary(
    eda_result_dir,
    file_id,
    file_name,
    ydata_summary,
    ge_summary,
    eda_insight
):
    """
    Save summary JSON to eda_result with file_id+file_name as filename.
    Only summary of ydata profiling, GE, and EDAReasonerFlexible (insight).
    - REVISED: Do NOT create folder, only allow if folder already exists.
    """
    if not os.path.exists(eda_result_dir):
        raise FileNotFoundError(f"Target folder does not exist: {eda_result_dir}")
    base = os.path.splitext(os.path.basename(file_name))[0]
    fname = f"{file_id}_{base}_eda_final.json"
    fpath = os.path.join(eda_result_dir, fname)
    summary = {
        "file_id": file_id,
        "file": file_name,
        "ydata_profile": ydata_summary,
        "ge": ge_summary,
        "eda_insight": eda_insight
    }
    serializable_log_content = ensure_json_serializable(summary)
    with open(fpath, "w", encoding="utf-8") as f:
        json.dump(serializable_log_content, f, indent=2, ensure_ascii=False)
    print(f"[EDA_RESULT] Saved summary to {fpath}")
    return fpath

# Example usage for save_eda_log and save_eda_summary:
# save_eda_log("/path/to/data_eda/data_log", ("abc123def456", "table_orders.parquet"), log_content, log_type="eda")
# save_eda_summary("/path/to/data_eda/eda_result", "abc123def456", "table_orders.parquet", ydata_summary, ge_summary, eda_insight)

15. eda_utils.py:

import os
import datetime
import json
import polars as pl

# --- REVISED: Remove auto-create of data_eda directory ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_EDA_DIR = os.path.join(BASE_DIR, "data_eda")
# os.makedirs(DATA_EDA_DIR, exist_ok=True)  # DIHAPUS agar tidak auto create folder

LOG_DIR = DATA_EDA_DIR  # All logs go to data_eda

def log_reasoning(event: dict, log_name: str = "eda_reasoning_log.json"):
    """
    Log reasoning events as JSON lines in the data_eda folder.
    """
    log_path = os.path.join(LOG_DIR, log_name)
    event['timestamp'] = datetime.datetime.utcnow().isoformat()
    if os.path.exists(log_path):
        try:
            with open(log_path, "r", encoding="utf-8") as f:
                logs = json.load(f)
            if not isinstance(logs, list):
                logs = []
        except Exception:
            logs = []
    else:
        logs = []
    logs.append(event)
    with open(log_path, "w", encoding="utf-8") as f:
        json.dump(logs, f, indent=2, ensure_ascii=False)

def capture_data_reference(df, cols=None, n=10):
    """
    Ambil data referensi (contoh value, histogram, dsb) untuk kebutuhan agentic GE.
    Kompatibel untuk polars DataFrame.
    """
    ref = {}
    if cols is None:
        cols = df.columns
    for col in cols:
        ser = df[col]
        # Numeric
        if ser.dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64, pl.Float32, pl.Float64]:
            arr = ser.drop_nulls().to_list()
            ref[col] = {
                "min": float(ser.min()) if arr else None,
                "max": float(ser.max()) if arr else None,
                "sample": ser.drop_nulls().sample(n=min(n, len(arr))).to_list() if arr else [],
            }
        else:
            # String/category/other
            ref[col] = {
                "value_sample": ser.drop_nulls().unique()[:n].to_list()
            }
    return ref

def explain_expectation_applied(col, col_type, rule, params, evidence=None):
    """
    Generate reasoning string for why a rule is applied.
    """
    reason = f"Expectation '{rule}' applied to column '{col}' (type: {col_type}) with params {params}."
    if evidence:
        reason += f" Evidence: {evidence}"
    return reason

16. audit_testing.py:

import os

def audit_testing_auto(status_monitor: list, eda_dict: dict, config: dict = None) -> dict:
    # Configurable via config/env
    min_score = float(os.environ.get("AUDIT_MIN_SCORE", 0))
    max_score = float(os.environ.get("AUDIT_MAX_SCORE", 100))
    error_penalty = float(os.environ.get("AUDIT_ERROR_PENALTY", 30))
    warning_penalty = float(os.environ.get("AUDIT_WARNING_PENALTY", 10))
    anomaly_penalty = float(os.environ.get("AUDIT_ANOMALY_PENALTY", 10))
    coverage_bonus = float(os.environ.get("AUDIT_COVERAGE_BONUS", 5))

    # Robust default
    status_monitor = status_monitor or []
    eda_dict = eda_dict or {}

    # OTOMATIS: scan semua step, warning, error, anomaly, coverage, validasi
    steps = set(s.get("step") for s in status_monitor if s.get("step"))
    errors = [s for s in status_monitor if s.get("status") == "error"]
    warnings = eda_dict.get("warnings", []) or []
    anomaly = eda_dict.get("anomaly", {}) or {}
    anomaly_rate = float(anomaly.get("rate", 0.0) or 0.0)
    coverage = eda_dict.get("coverage", None)
    if coverage is not None:
        try:
            coverage = float(coverage)
        except Exception:
            coverage = None
    validation = eda_dict.get("validation", {}).get("success") if eda_dict.get("validation") else None

    # Dinamis: penalti tergantung jumlah error/warning/anomaly, tanpa hardcode step
    score = max_score
    score -= error_penalty * len(errors)
    score -= warning_penalty * len(warnings)
    score -= anomaly_penalty * anomaly_rate

    # Cerdas: bonus jika ada full coverage/unit test
    if coverage is not None and coverage > 0.9:
        score += coverage_bonus
    if validation is False:
        score -= 10  # penalti validasi gagal

    score = max(min_score, min(max_score, score))

    evidence = {
        "step_count": len(steps),
        "errors": errors,
        "warnings": warnings,
        "anomaly_rate": anomaly_rate,
        "coverage": coverage,
        "validation_success": validation,
    }
    comment = (
        "Semua proses EDA lolos tanpa error/warning/anomaly."
        if score == max_score
        else "Cek evidence audit, ada warning/error/anomaly."
    )
    return {
        "score": score,
        "evidence": evidence,
        "comment": comment
    }

17. pdf_report_generator.py:

import matplotlib.pyplot as plt
from jinja2 import Environment, FileSystemLoader, select_autoescape
import pdfkit
import os

def generate_plots(df, out_dir):
    plot_paths = {}
    if 'totalsales' in df.columns:
        plt.figure(figsize=(8,5))
        df['totalsales'].plot(kind='hist', bins=20, color='#2196f3', edgecolor='black')
        plt.title('Distribusi Total Penjualan')
        plt.xlabel('Total Penjualan')
        plt.ylabel('Frekuensi')
        plot_sales = os.path.join(out_dir, 'sales_hist.png')
        plt.savefig(plot_sales, bbox_inches='tight')
        plt.close()
        plot_paths['sales_hist'] = plot_sales
    if 'qty' in df.columns:
        plt.figure(figsize=(8,5))
        df['qty'].plot(kind='hist', bins=20, color='#4caf50', edgecolor='black')
        plt.title('Distribusi Jumlah Barang (Qty)')
        plt.xlabel('Qty')
        plt.ylabel('Frekuensi')
        plot_qty = os.path.join(out_dir, 'qty_hist.png')
        plt.savefig(plot_qty, bbox_inches='tight')
        plt.close()
        plot_paths['qty_hist'] = plot_qty
    if 'kategori' in df.columns:
        plt.figure(figsize=(7,7))
        df['kategori'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=plt.cm.Pastel1.colors)
        plt.title('Proporsi Kategori')
        plot_kat = os.path.join(out_dir, 'kategori_pie.png')
        plt.savefig(plot_kat, bbox_inches='tight')
        plt.close()
        plot_paths['kategori_pie'] = plot_kat
    return plot_paths

def generate_pdf_report(
    df, 
    advanced_review, 
    pdf_path,
    profiling=None,
    ge=None,
    eda_insight=None,   # <--- Parameter insight baru
    kolaborasi=None,
    wkhtmltopdf_path=None,
    template_path=None
):
    """
    Membuat PDF report EDA berbasis insight otomatis (EDAReasonerFlexible).
    Parameter gemini dihapus, diganti eda_insight.
    """
    out_dir = os.path.dirname(pdf_path)
    # PATCH: Tidak ada auto-create folder, hanya cek eksistensi
    if not os.path.exists(out_dir):
        raise FileNotFoundError(f"Target folder does not exist: {out_dir}")
    plot_paths = generate_plots(df, out_dir)
    
    # Template logic
    if template_path is None:
        template_dir = out_dir
        template_filename = 'eda_pdf_template.html'
        template_path = os.path.join(template_dir, template_filename)
    else:
        template_dir = os.path.dirname(template_path)
        template_filename = os.path.basename(template_path)

    if not os.path.exists(template_path):
        raise FileNotFoundError(f"Template {template_path} tidak ditemukan. Pastikan template tersedia di {template_dir}")

    env = Environment(
        loader=FileSystemLoader(template_dir),
        autoescape=select_autoescape(['html', 'xml'])
    )
    template = env.get_template(template_filename)
    html_out = template.render(
        review=advanced_review,
        plot_sales=plot_paths.get('sales_hist'),
        plot_qty=plot_paths.get('qty_hist'),
        plot_kategori=plot_paths.get('kategori_pie'),
        profiling=profiling,
        ge=ge,
        eda_insight=eda_insight,   # <--- Tambahkan ke template context
        kolaborasi=kolaborasi
    )
    # Simpan HTML di folder eda_result
    html_out_path = os.path.splitext(pdf_path)[0] + ".html"
    with open(html_out_path, "w", encoding="utf-8") as f:
        f.write(html_out)
    # Convert HTML ke PDF dengan konfigurasi wkhtmltopdf
    if wkhtmltopdf_path is None:
        wkhtmltopdf_path = r"C:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe"
    if not os.path.exists(wkhtmltopdf_path):
        raise FileNotFoundError(f"wkhtmltopdf.exe tidak ditemukan di {wkhtmltopdf_path}. Pastikan sudah terinstall.")
    config = pdfkit.configuration(wkhtmltopdf=wkhtmltopdf_path)
    try:
        pdfkit.from_string(html_out, pdf_path, configuration=config)
    except Exception as e:
        print(f"ERROR saat generate PDF: {e}")
        raise

if __name__ == "__main__":
    import pandas as pd
    df = pd.DataFrame({
        "totalsales": [100, 200, 150, 300, 350, 400, 120, 180, 170, 260],
        "qty": [1, 2, 2, 3, 1, 2, 2, 1, 3, 2],
        "kategori": ["A", "B", "A", "C", "B", "B", "C", "A", "A", "C"]
    })
    advanced_review = {
        "file": "dummy.parquet",
        "confidence_score": 98.7,
        "completeness_score": 95.4,
        "columns": ["totalsales", "qty", "kategori"],
        "numeric": {
            "totalsales": {
                "mean": 223.0, "std": 95.7, "min": 100, "max": 400
            }
        },
        "categorical": {
            "kategori": {
                "unique": 3,
                "top_freq": {"A": 4, "B": 3, "C": 3}
            }
        },
        "sampling": {
            "sample_shape": (10, 3),
            "reasoning": {"strategy": "random"}
        }
    }
    profiling = {"summary": "Ini hasil profiling dari YData Profiling."}
    ge = {"checks": "Validasi GE lolos semua."}
    eda_insight = {
        "insight": [
            "Kolom 'qty' memiliki distribusi miring ke kanan.",
            "Tidak ditemukan outlier pada kolom 'totalsales'."
        ],
        "recommendations": [
            "Lakukan normalisasi pada kolom 'qty' sebelum modeling."
        ]
    }
    kolaborasi = {"ringkasan": "Ringkasan kolaborasi tim."}

    out_dir = "data_eda/eda_result"
    # PATCH: Tidak ada auto-create folder, hanya cek eksistensi
    if not os.path.exists(out_dir):
        raise FileNotFoundError(f"Target folder does not exist: {out_dir}")
    pdf_path = os.path.join(out_dir, "dummy_eda_report.pdf")
    generate_pdf_report(df, advanced_review, pdf_path, profiling, ge, eda_insight, kolaborasi)
    print(f"PDF report generated: {pdf_path}")

18. ge_suite_tools.py:

import json
import os

def suggest_schema_to_ge_suite(suggest_schema: dict, name="auto_suite"):
    """
    Konversi hasil auto_schema_expectation (dict) menjadi GE suite dict.
    """
    suite = {
        "expectation_suite_name": name,
        "expectations": [],
        "meta": {}
    }
    for col, spec in suggest_schema.items():
        typ = spec.get("type")
        # Numeric: range
        if typ and (typ.startswith("float") or typ.startswith("int")):
            suite["expectations"].append({
                "expectation_type": "expect_column_values_to_be_between",
                "kwargs": {
                    "column": col,
                    "min_value": spec.get("min"),
                    "max_value": spec.get("max")
                }
            })
            # Unique
            n_unique = spec.get("n_unique")
            if n_unique is not None:
                suite["expectations"].append({
                    "expectation_type": "expect_column_unique_value_count_to_be_between",
                    "kwargs": {
                        "column": col,
                        "min_value": 1,
                        "max_value": n_unique
                    }
                })
        # String: length
        elif "max_length" in spec:
            suite["expectations"].append({
                "expectation_type": "expect_column_value_lengths_to_be_between",
                "kwargs": {
                    "column": col,
                    "min_value": 1,
                    "max_value": spec["max_length"]
                }
            })
            # Unique
            n_unique = spec.get("n_unique")
            if n_unique is not None:
                suite["expectations"].append({
                    "expectation_type": "expect_column_unique_value_count_to_be_between",
                    "kwargs": {
                        "column": col,
                        "min_value": 1,
                        "max_value": n_unique
                    }
                })
            # Sample values as domain expectation
            if "sample_values" in spec and isinstance(spec["sample_values"], list):
                suite["expectations"].append({
                    "expectation_type": "expect_column_values_to_be_in_set",
                    "kwargs": {
                        "column": col,
                        "value_set": spec["sample_values"]
                    }
                })
        # Domain constraint for categorical
        elif "sample_values" in spec and isinstance(spec["sample_values"], list):
            suite["expectations"].append({
                "expectation_type": "expect_column_values_to_be_in_set",
                "kwargs": {
                    "column": col,
                    "value_set": spec["sample_values"]
                }
            })
    return suite

def save_ge_suite(suite: dict, out_path: str):
    """
    Simpan GE suite dict ke file JSON.
    """
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(suite, f, indent=2, ensure_ascii=False)

def load_ge_suite(path: str):
    """
    Load GE suite dict dari file JSON.
    """
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

##############################
# Integrasi dengan sampling.py
##############################

def generate_ge_suite_from_sample(
    sample_df,
    auto_schema: dict,
    out_dir: str,
    suite_name: str = "auto_suite"
):
    """
    Generate dan simpan GE suite dari sample representatif (hasil sampling.py) dan hasil auto_schema.
    Return path suite JSON.
    """
    # PATCH: Tidak ada auto-create folder, hanya cek eksistensi
    if not os.path.exists(out_dir):
        raise FileNotFoundError(f"Target folder does not exist: {out_dir}")
    suite = suggest_schema_to_ge_suite(auto_schema, name=suite_name)
    out_path = os.path.join(out_dir, f"{suite_name}.json")
    save_ge_suite(suite, out_path)
    return out_path

# --- Unit Test ---
def test_ge_suite_tools():
    auto_schema = {
        "price": {"type": "float64", "min": 0, "max": 1000, "n_unique": 70},
        "category": {"type": "object", "max_length": 10, "sample_values": ["A", "B", "C"], "n_unique": 3}
    }
    suite = suggest_schema_to_ge_suite(auto_schema, name="test_suite")
    assert isinstance(suite, dict)
    assert "expectations" in suite
    tmp_dir = "tmp_suite_test"
    # PATCH: Tidak ada auto-create folder, hanya cek eksistensi
    if not os.path.exists(tmp_dir):
        raise FileNotFoundError(f"Target folder does not exist: {tmp_dir}")
    path = os.path.join(tmp_dir, "test_suite.json")
    save_ge_suite(suite, path)
    loaded = load_ge_suite(path)
    assert loaded["expectation_suite_name"] == "test_suite"
    print("ge_suite_tools.py tests OK")

if __name__ == "__main__":
    # Pastikan tmp_dir sudah ada sebelum menjalankan test
    # Jika ingin menjalankan test, buat folder tmp_suite_test terlebih dahulu secara manual
    test_ge_suite_tools()

19. plugin_registry.py:

import os
import importlib.util

# Integrasi dengan sampling.py: plugin dapat menerima sample_df hasil sampling representatif
PLUGIN_DIR = os.path.join(os.path.dirname(__file__), "plugins")

def discover_plugins():
    """
    Menemukan semua plugin Python di direktori plugins, kecuali __init__.py.
    """
    plugins = []
    if not os.path.isdir(PLUGIN_DIR):
        return plugins
    for fname in os.listdir(PLUGIN_DIR):
        if fname.endswith(".py") and fname != "__init__.py":
            path = os.path.join(PLUGIN_DIR, fname)
            name = fname[:-3]
            spec = importlib.util.spec_from_file_location(name, path)
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)
            plugins.append(mod)
    return plugins

def run_plugins(df, context=None, sampling_info=None):
    """
    Menjalankan semua plugin yang memiliki fungsi analyze(df, context, sampling_info).
    df: DataFrame (idealnya sample_df hasil sampling.py)
    context: dict opsional untuk info tambahan
    sampling_info: dict opsional hasil sampling.py untuk audit/representativitas
    """
    results = {}
    for plugin in discover_plugins():
        analyze_fn = getattr(plugin, "analyze", None)
        if callable(analyze_fn):
            try:
                # Plugin signature: analyze(df, context=None, sampling_info=None)
                # Backward compatible: plugin lama tanpa sampling_info tetap jalan
                try:
                    result = analyze_fn(df, context=context, sampling_info=sampling_info)
                except TypeError:
                    # Fallback ke signature lama
                    result = analyze_fn(df, context)
                results[plugin.__name__] = result
            except Exception as e:
                results[plugin.__name__] = {"error": str(e)}
    return results

##############################
# Example usage in EDA pipeline:
# from plugin_registry import run_plugins
# plugin_results = run_plugins(sample_df, context={"file": file_path}, sampling_info=sampling_info)
##############################

# --- Unit Test ---
def test_plugin_registry():
    import pandas as pd
    # Dummy DataFrame
    df = pd.DataFrame({
        "a": [1, 2, 3],
        "b": ["x", "y", "z"]
    })
    # Dummy context & sampling_info
    context = {"file": "dummy.parquet"}
    sampling_info = {"n_sample": 3, "method": "test"}
    # Should not fail even if plugins/ empty
    results = run_plugins(df, context, sampling_info)
    print("Plugin registry results:", results)
    assert isinstance(results, dict)
    print("plugin_registry.py tests OK")

if __name__ == "__main__":
    test_plugin_registry()

